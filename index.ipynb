{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Regularization-and-Optimization-of-Neural-Networks---Lab\" data-toc-modified-id=\"Regularization-and-Optimization-of-Neural-Networks---Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Regularization and Optimization of Neural Networks - Lab</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Import-the-libraries\" data-toc-modified-id=\"Import-the-libraries-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Import the libraries</a></span></li><li><span><a href=\"#Load-the-Data\" data-toc-modified-id=\"Load-the-Data-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Load the Data</a></span></li><li><span><a href=\"#Preprocessing-Overview\" data-toc-modified-id=\"Preprocessing-Overview-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Preprocessing Overview</a></span></li><li><span><a href=\"#Preprocessing:-Generate-a-Random-Sample\" data-toc-modified-id=\"Preprocessing:-Generate-a-Random-Sample-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Preprocessing: Generate a Random Sample</a></span></li><li><span><a href=\"#Preprocessing:-One-hot-Encoding-of-the-Complaints\" data-toc-modified-id=\"Preprocessing:-One-hot-Encoding-of-the-Complaints-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Preprocessing: One-hot Encoding of the Complaints</a></span></li><li><span><a href=\"#Preprocessing:-Encoding-the-Products\" data-toc-modified-id=\"Preprocessing:-Encoding-the-Products-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Preprocessing: Encoding the Products</a></span></li><li><span><a href=\"#Train-test-Split\" data-toc-modified-id=\"Train-test-Split-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Train-test Split</a></span></li><li><span><a href=\"#Running-the-model-using-a-validation-set.\" data-toc-modified-id=\"Running-the-model-using-a-validation-set.-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Running the model using a validation set.</a></span></li><li><span><a href=\"#Creating-the-Validation-Set\" data-toc-modified-id=\"Creating-the-Validation-Set-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Creating the Validation Set</a></span></li><li><span><a href=\"#Creating-the-Model\" data-toc-modified-id=\"Creating-the-Model-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>Creating the Model</a></span></li><li><span><a href=\"#Compiling-the-Model\" data-toc-modified-id=\"Compiling-the-Model-1.13\"><span class=\"toc-item-num\">1.13&nbsp;&nbsp;</span>Compiling the Model</a></span></li><li><span><a href=\"#Part-2:-Code-Along\" data-toc-modified-id=\"Part-2:-Code-Along-1.14\"><span class=\"toc-item-num\">1.14&nbsp;&nbsp;</span>Part 2: Code Along</a></span></li><li><span><a href=\"#Training-the-Model\" data-toc-modified-id=\"Training-the-Model-1.15\"><span class=\"toc-item-num\">1.15&nbsp;&nbsp;</span>Training the Model</a></span></li><li><span><a href=\"#Retrieving-Performance-Results:-the-history-dictionary\" data-toc-modified-id=\"Retrieving-Performance-Results:-the-history-dictionary-1.16\"><span class=\"toc-item-num\">1.16&nbsp;&nbsp;</span>Retrieving Performance Results: the <code>history</code> dictionary</a></span></li><li><span><a href=\"#Plotting-the-Results\" data-toc-modified-id=\"Plotting-the-Results-1.17\"><span class=\"toc-item-num\">1.17&nbsp;&nbsp;</span>Plotting the Results</a></span></li><li><span><a href=\"#Early-Stopping\" data-toc-modified-id=\"Early-Stopping-1.18\"><span class=\"toc-item-num\">1.18&nbsp;&nbsp;</span>Early Stopping</a></span></li><li><span><a href=\"#L2-Regularization\" data-toc-modified-id=\"L2-Regularization-1.19\"><span class=\"toc-item-num\">1.19&nbsp;&nbsp;</span>L2 Regularization</a></span></li><li><span><a href=\"#L1-Regularization\" data-toc-modified-id=\"L1-Regularization-1.20\"><span class=\"toc-item-num\">1.20&nbsp;&nbsp;</span>L1 Regularization</a></span></li><li><span><a href=\"#Dropout-Regularization\" data-toc-modified-id=\"Dropout-Regularization-1.21\"><span class=\"toc-item-num\">1.21&nbsp;&nbsp;</span>Dropout Regularization</a></span></li><li><span><a href=\"#Bigger-Data?\" data-toc-modified-id=\"Bigger-Data?-1.22\"><span class=\"toc-item-num\">1.22&nbsp;&nbsp;</span>Bigger Data?</a></span></li><li><span><a href=\"#Additional-Resources\" data-toc-modified-id=\"Additional-Resources-1.23\"><span class=\"toc-item-num\">1.23&nbsp;&nbsp;</span>Additional Resources</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-1.24\"><span class=\"toc-item-num\">1.24&nbsp;&nbsp;</span>Summary</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import random\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "space = print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 2 columns):\n",
      "Product                         60000 non-null object\n",
      "Consumer complaint narrative    60000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 937.6+ KB\n",
      "None\n",
      "             Product                       Consumer complaint narrative\n",
      "count          60000                                              60000\n",
      "unique             7                                              59724\n",
      "top     Student loan  I am filing this complaint because Experian ha...\n",
      "freq           11404                                                 26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "print(df.info())\n",
    "space, space\n",
    "print(df.describe())\n",
    "space, space\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df['Product']\n",
    "complaints = df['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "one_hot = tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer product labels to num values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_category = le.transform(product)\n",
    "\n",
    "# transform integer values into a matrix of binary flags\n",
    "product_one_hot = to_categorical(product_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot, product_one_hot, test_size=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 22:01:58.761739 4749919680 deprecation_wrapper.py:119] From /Users/paulw/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0829 22:01:58.767151 4749919680 deprecation_wrapper.py:119] From /Users/paulw/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 22:04:23.569839 4749919680 deprecation_wrapper.py:119] From /Users/paulw/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0829 22:04:23.590847 4749919680 deprecation_wrapper.py:119] From /Users/paulw/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', \\\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 22:11:15.517931 4749919680 deprecation.py:323] From /Users/paulw/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0829 22:11:15.569899 4749919680 deprecation_wrapper.py:119] From /Users/paulw/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7800 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7800/7800 [==============================] - 0s 53us/step - loss: 1.9558 - acc: 0.1515 - val_loss: 1.9426 - val_acc: 0.1790\n",
      "Epoch 2/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.9343 - acc: 0.1744 - val_loss: 1.9234 - val_acc: 0.2160\n",
      "Epoch 3/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.9175 - acc: 0.1923 - val_loss: 1.9062 - val_acc: 0.2260\n",
      "Epoch 4/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.9008 - acc: 0.2138 - val_loss: 1.8883 - val_acc: 0.2410\n",
      "Epoch 5/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.8832 - acc: 0.2374 - val_loss: 1.8700 - val_acc: 0.2600\n",
      "Epoch 6/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.8636 - acc: 0.2612 - val_loss: 1.8486 - val_acc: 0.2870\n",
      "Epoch 7/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.8408 - acc: 0.2824 - val_loss: 1.8233 - val_acc: 0.3240\n",
      "Epoch 8/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.8136 - acc: 0.3073 - val_loss: 1.7930 - val_acc: 0.3510\n",
      "Epoch 9/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.7813 - acc: 0.3337 - val_loss: 1.7572 - val_acc: 0.3720\n",
      "Epoch 10/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.7437 - acc: 0.3621 - val_loss: 1.7150 - val_acc: 0.3910\n",
      "Epoch 11/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.7012 - acc: 0.3832 - val_loss: 1.6700 - val_acc: 0.4090\n",
      "Epoch 12/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.6551 - acc: 0.4062 - val_loss: 1.6224 - val_acc: 0.4120\n",
      "Epoch 13/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.6058 - acc: 0.4245 - val_loss: 1.5712 - val_acc: 0.4360\n",
      "Epoch 14/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5548 - acc: 0.4538 - val_loss: 1.5199 - val_acc: 0.4610\n",
      "Epoch 15/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5030 - acc: 0.4797 - val_loss: 1.4688 - val_acc: 0.5040\n",
      "Epoch 16/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4515 - acc: 0.5083 - val_loss: 1.4184 - val_acc: 0.5250\n",
      "Epoch 17/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4009 - acc: 0.5369 - val_loss: 1.3689 - val_acc: 0.5580\n",
      "Epoch 18/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.3510 - acc: 0.5596 - val_loss: 1.3214 - val_acc: 0.5870\n",
      "Epoch 19/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.3021 - acc: 0.5871 - val_loss: 1.2726 - val_acc: 0.6000\n",
      "Epoch 20/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.2535 - acc: 0.6014 - val_loss: 1.2270 - val_acc: 0.6220\n",
      "Epoch 21/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.2069 - acc: 0.6210 - val_loss: 1.1835 - val_acc: 0.6340\n",
      "Epoch 22/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.1634 - acc: 0.6353 - val_loss: 1.1425 - val_acc: 0.6360\n",
      "Epoch 23/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.1218 - acc: 0.6472 - val_loss: 1.1038 - val_acc: 0.6510\n",
      "Epoch 24/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.0835 - acc: 0.6601 - val_loss: 1.0690 - val_acc: 0.6650\n",
      "Epoch 25/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.0477 - acc: 0.6737 - val_loss: 1.0371 - val_acc: 0.6720\n",
      "Epoch 26/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.0142 - acc: 0.6797 - val_loss: 1.0069 - val_acc: 0.6810\n",
      "Epoch 27/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9830 - acc: 0.6912 - val_loss: 0.9767 - val_acc: 0.6880\n",
      "Epoch 28/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.9540 - acc: 0.6959 - val_loss: 0.9524 - val_acc: 0.6940\n",
      "Epoch 29/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.9270 - acc: 0.7038 - val_loss: 0.9289 - val_acc: 0.6950\n",
      "Epoch 30/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9020 - acc: 0.7081 - val_loss: 0.9056 - val_acc: 0.7020\n",
      "Epoch 31/120\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.8794 - acc: 0.715 - 0s 23us/step - loss: 0.8787 - acc: 0.7153 - val_loss: 0.8874 - val_acc: 0.6970\n",
      "Epoch 32/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8576 - acc: 0.7192 - val_loss: 0.8686 - val_acc: 0.7020\n",
      "Epoch 33/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8372 - acc: 0.7237 - val_loss: 0.8494 - val_acc: 0.7000\n",
      "Epoch 34/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8185 - acc: 0.7279 - val_loss: 0.8325 - val_acc: 0.7100\n",
      "Epoch 35/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8005 - acc: 0.7336 - val_loss: 0.8180 - val_acc: 0.7110\n",
      "Epoch 36/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.7844 - acc: 0.7345 - val_loss: 0.8034 - val_acc: 0.7150\n",
      "Epoch 37/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.7687 - acc: 0.7414 - val_loss: 0.7886 - val_acc: 0.7220\n",
      "Epoch 38/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.7542 - acc: 0.7433 - val_loss: 0.7784 - val_acc: 0.7240\n",
      "Epoch 39/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.7406 - acc: 0.7460 - val_loss: 0.7675 - val_acc: 0.7210\n",
      "Epoch 40/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.7276 - acc: 0.7499 - val_loss: 0.7573 - val_acc: 0.7250\n",
      "Epoch 41/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.7157 - acc: 0.7523 - val_loss: 0.7462 - val_acc: 0.7280\n",
      "Epoch 42/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.7039 - acc: 0.7556 - val_loss: 0.7375 - val_acc: 0.7310\n",
      "Epoch 43/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.6928 - acc: 0.7600 - val_loss: 0.7305 - val_acc: 0.7270\n",
      "Epoch 44/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.6825 - acc: 0.7622 - val_loss: 0.7200 - val_acc: 0.7350\n",
      "Epoch 45/120\n",
      "7800/7800 [==============================] - 0s 22us/step - loss: 0.6722 - acc: 0.7635 - val_loss: 0.7144 - val_acc: 0.7370\n",
      "Epoch 46/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.6631 - acc: 0.7656 - val_loss: 0.7062 - val_acc: 0.7370\n",
      "Epoch 47/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.6539 - acc: 0.7681 - val_loss: 0.6985 - val_acc: 0.7370\n",
      "Epoch 48/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.6455 - acc: 0.7703 - val_loss: 0.6938 - val_acc: 0.7400\n",
      "Epoch 49/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.6371 - acc: 0.7745 - val_loss: 0.6916 - val_acc: 0.7440\n",
      "Epoch 50/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.6290 - acc: 0.7782 - val_loss: 0.6857 - val_acc: 0.7460\n",
      "Epoch 51/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.6218 - acc: 0.7786 - val_loss: 0.6792 - val_acc: 0.7460\n",
      "Epoch 52/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.6143 - acc: 0.7824 - val_loss: 0.6726 - val_acc: 0.7390\n",
      "Epoch 53/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.6070 - acc: 0.7842 - val_loss: 0.6679 - val_acc: 0.7460\n",
      "Epoch 54/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.6002 - acc: 0.7877 - val_loss: 0.6642 - val_acc: 0.7460\n",
      "Epoch 55/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5939 - acc: 0.7892 - val_loss: 0.6601 - val_acc: 0.7380\n",
      "Epoch 56/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5870 - acc: 0.7914 - val_loss: 0.6571 - val_acc: 0.7390\n",
      "Epoch 57/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5813 - acc: 0.7940 - val_loss: 0.6538 - val_acc: 0.7440\n",
      "Epoch 58/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5751 - acc: 0.7958 - val_loss: 0.6485 - val_acc: 0.7470\n",
      "Epoch 59/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5689 - acc: 0.7983 - val_loss: 0.6441 - val_acc: 0.7450\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5635 - acc: 0.7987 - val_loss: 0.6413 - val_acc: 0.7440\n",
      "Epoch 61/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5578 - acc: 0.8001 - val_loss: 0.6376 - val_acc: 0.7510\n",
      "Epoch 62/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5529 - acc: 0.8041 - val_loss: 0.6375 - val_acc: 0.7510\n",
      "Epoch 63/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5476 - acc: 0.8056 - val_loss: 0.6349 - val_acc: 0.7510\n",
      "Epoch 64/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5423 - acc: 0.8064 - val_loss: 0.6367 - val_acc: 0.7530\n",
      "Epoch 65/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5373 - acc: 0.8103 - val_loss: 0.6286 - val_acc: 0.7540\n",
      "Epoch 66/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5324 - acc: 0.8100 - val_loss: 0.6282 - val_acc: 0.7540\n",
      "Epoch 67/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5278 - acc: 0.8131 - val_loss: 0.6228 - val_acc: 0.7530\n",
      "Epoch 68/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5231 - acc: 0.8123 - val_loss: 0.6220 - val_acc: 0.7570\n",
      "Epoch 69/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5187 - acc: 0.8144 - val_loss: 0.6194 - val_acc: 0.7560\n",
      "Epoch 70/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5140 - acc: 0.8169 - val_loss: 0.6192 - val_acc: 0.7570\n",
      "Epoch 71/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5097 - acc: 0.8177 - val_loss: 0.6173 - val_acc: 0.7580\n",
      "Epoch 72/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5055 - acc: 0.8204 - val_loss: 0.6157 - val_acc: 0.7590\n",
      "Epoch 73/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5011 - acc: 0.8201 - val_loss: 0.6145 - val_acc: 0.7540\n",
      "Epoch 74/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4970 - acc: 0.8226 - val_loss: 0.6114 - val_acc: 0.7580\n",
      "Epoch 75/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4930 - acc: 0.8238 - val_loss: 0.6107 - val_acc: 0.7600\n",
      "Epoch 76/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4890 - acc: 0.8237 - val_loss: 0.6085 - val_acc: 0.7600\n",
      "Epoch 77/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4847 - acc: 0.8265 - val_loss: 0.6069 - val_acc: 0.7560\n",
      "Epoch 78/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4813 - acc: 0.8295 - val_loss: 0.6078 - val_acc: 0.7590\n",
      "Epoch 79/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4775 - acc: 0.8318 - val_loss: 0.6072 - val_acc: 0.7580\n",
      "Epoch 80/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4735 - acc: 0.8331 - val_loss: 0.6051 - val_acc: 0.7600\n",
      "Epoch 81/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4702 - acc: 0.8324 - val_loss: 0.6042 - val_acc: 0.7600\n",
      "Epoch 82/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4662 - acc: 0.8359 - val_loss: 0.6058 - val_acc: 0.7620\n",
      "Epoch 83/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4627 - acc: 0.8363 - val_loss: 0.6012 - val_acc: 0.7600\n",
      "Epoch 84/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4588 - acc: 0.8386 - val_loss: 0.6016 - val_acc: 0.7630\n",
      "Epoch 85/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4553 - acc: 0.8383 - val_loss: 0.6019 - val_acc: 0.7560\n",
      "Epoch 86/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4522 - acc: 0.8415 - val_loss: 0.5988 - val_acc: 0.7510\n",
      "Epoch 87/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4484 - acc: 0.8419 - val_loss: 0.5984 - val_acc: 0.7600\n",
      "Epoch 88/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4455 - acc: 0.8437 - val_loss: 0.5977 - val_acc: 0.7630\n",
      "Epoch 89/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4418 - acc: 0.8456 - val_loss: 0.5968 - val_acc: 0.7670\n",
      "Epoch 90/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4388 - acc: 0.8460 - val_loss: 0.5961 - val_acc: 0.7590\n",
      "Epoch 91/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.4354 - acc: 0.8477 - val_loss: 0.5954 - val_acc: 0.7640\n",
      "Epoch 92/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4325 - acc: 0.8492 - val_loss: 0.5963 - val_acc: 0.7620\n",
      "Epoch 93/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.8503 - val_loss: 0.5964 - val_acc: 0.7660\n",
      "Epoch 94/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4257 - acc: 0.8508 - val_loss: 0.5957 - val_acc: 0.7700\n",
      "Epoch 95/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4228 - acc: 0.8537 - val_loss: 0.5946 - val_acc: 0.7630\n",
      "Epoch 96/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4201 - acc: 0.8544 - val_loss: 0.5951 - val_acc: 0.7650\n",
      "Epoch 97/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4167 - acc: 0.8563 - val_loss: 0.5944 - val_acc: 0.7660\n",
      "Epoch 98/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.4139 - acc: 0.8587 - val_loss: 0.5933 - val_acc: 0.7670\n",
      "Epoch 99/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4104 - acc: 0.8583 - val_loss: 0.5922 - val_acc: 0.7670\n",
      "Epoch 100/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4079 - acc: 0.8585 - val_loss: 0.5943 - val_acc: 0.7710\n",
      "Epoch 101/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4051 - acc: 0.8619 - val_loss: 0.5907 - val_acc: 0.7690\n",
      "Epoch 102/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4026 - acc: 0.8618 - val_loss: 0.5907 - val_acc: 0.7740\n",
      "Epoch 103/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.3993 - acc: 0.8633 - val_loss: 0.5909 - val_acc: 0.7680\n",
      "Epoch 104/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3962 - acc: 0.8646 - val_loss: 0.5908 - val_acc: 0.7730\n",
      "Epoch 105/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3939 - acc: 0.8654 - val_loss: 0.5923 - val_acc: 0.7730\n",
      "Epoch 106/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3909 - acc: 0.8673 - val_loss: 0.5912 - val_acc: 0.7740\n",
      "Epoch 107/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3883 - acc: 0.8676 - val_loss: 0.5906 - val_acc: 0.7730\n",
      "Epoch 108/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3853 - acc: 0.8699 - val_loss: 0.5925 - val_acc: 0.7710\n",
      "Epoch 109/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3829 - acc: 0.8709 - val_loss: 0.5903 - val_acc: 0.7750\n",
      "Epoch 110/120\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.3801 - acc: 0.8717 - val_loss: 0.5915 - val_acc: 0.7770\n",
      "Epoch 111/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3775 - acc: 0.8704 - val_loss: 0.5922 - val_acc: 0.7770\n",
      "Epoch 112/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3749 - acc: 0.8733 - val_loss: 0.5918 - val_acc: 0.7780\n",
      "Epoch 113/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3722 - acc: 0.8744 - val_loss: 0.5913 - val_acc: 0.7770\n",
      "Epoch 114/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3699 - acc: 0.8745 - val_loss: 0.5918 - val_acc: 0.7760\n",
      "Epoch 115/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3675 - acc: 0.8778 - val_loss: 0.5904 - val_acc: 0.7790\n",
      "Epoch 116/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3647 - acc: 0.8795 - val_loss: 0.5905 - val_acc: 0.7720\n",
      "Epoch 117/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3626 - acc: 0.8785 - val_loss: 0.5916 - val_acc: 0.7840\n",
      "Epoch 118/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3602 - acc: 0.8800 - val_loss: 0.5926 - val_acc: 0.7810\n",
      "Epoch 119/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3576 - acc: 0.8808 - val_loss: 0.5936 - val_acc: 0.7780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.3551 - acc: 0.8814 - val_loss: 0.5927 - val_acc: 0.7740\n"
     ]
    }
   ],
   "source": [
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 16us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 21us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.35260544101397195, 0.8837179487179487]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7030640443166097, 0.74]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7800 samples, validate on 1000 samples\n",
      "Epoch 1/80\n",
      "7800/7800 [==============================] - 0s 44us/step - loss: 1.9774 - acc: 0.1242 - val_loss: 1.9499 - val_acc: 0.1540\n",
      "Epoch 2/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.9365 - acc: 0.1899 - val_loss: 1.9238 - val_acc: 0.2000\n",
      "Epoch 3/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.9152 - acc: 0.2190 - val_loss: 1.9054 - val_acc: 0.2220\n",
      "Epoch 4/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.8971 - acc: 0.2364 - val_loss: 1.8871 - val_acc: 0.2340\n",
      "Epoch 5/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.8785 - acc: 0.2440 - val_loss: 1.8689 - val_acc: 0.2520\n",
      "Epoch 6/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.8586 - acc: 0.2568 - val_loss: 1.8480 - val_acc: 0.2690\n",
      "Epoch 7/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.8361 - acc: 0.2735 - val_loss: 1.8243 - val_acc: 0.2840\n",
      "Epoch 8/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.8109 - acc: 0.2862 - val_loss: 1.7978 - val_acc: 0.3050\n",
      "Epoch 9/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.7823 - acc: 0.3063 - val_loss: 1.7682 - val_acc: 0.3290\n",
      "Epoch 10/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.7498 - acc: 0.3354 - val_loss: 1.7340 - val_acc: 0.3540\n",
      "Epoch 11/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.7122 - acc: 0.3704 - val_loss: 1.6958 - val_acc: 0.3810\n",
      "Epoch 12/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.6702 - acc: 0.4060 - val_loss: 1.6535 - val_acc: 0.3990\n",
      "Epoch 13/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.6252 - acc: 0.4377 - val_loss: 1.6087 - val_acc: 0.4350\n",
      "Epoch 14/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.5779 - acc: 0.4635 - val_loss: 1.5626 - val_acc: 0.4820\n",
      "Epoch 15/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5296 - acc: 0.4932 - val_loss: 1.5140 - val_acc: 0.4870\n",
      "Epoch 16/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.4807 - acc: 0.5146 - val_loss: 1.4669 - val_acc: 0.5210\n",
      "Epoch 17/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4317 - acc: 0.5379 - val_loss: 1.4202 - val_acc: 0.5500\n",
      "Epoch 18/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.3833 - acc: 0.5586 - val_loss: 1.3749 - val_acc: 0.5760\n",
      "Epoch 19/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.3354 - acc: 0.5853 - val_loss: 1.3296 - val_acc: 0.5810\n",
      "Epoch 20/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 1.2888 - acc: 0.6035 - val_loss: 1.2861 - val_acc: 0.6090\n",
      "Epoch 21/80\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2436 - acc: 0.6221 - val_loss: 1.2426 - val_acc: 0.6270\n",
      "Epoch 22/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.1998 - acc: 0.6373 - val_loss: 1.2009 - val_acc: 0.6400\n",
      "Epoch 23/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.1573 - acc: 0.6523 - val_loss: 1.1614 - val_acc: 0.6450\n",
      "Epoch 24/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.1166 - acc: 0.6637 - val_loss: 1.1224 - val_acc: 0.6490\n",
      "Epoch 25/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.0770 - acc: 0.6755 - val_loss: 1.0857 - val_acc: 0.6610\n",
      "Epoch 26/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.0395 - acc: 0.6856 - val_loss: 1.0518 - val_acc: 0.6740\n",
      "Epoch 27/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.0049 - acc: 0.6972 - val_loss: 1.0182 - val_acc: 0.6850\n",
      "Epoch 28/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.9721 - acc: 0.7041 - val_loss: 0.9888 - val_acc: 0.6920\n",
      "Epoch 29/80\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9417 - acc: 0.7110 - val_loss: 0.9598 - val_acc: 0.7030\n",
      "Epoch 30/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9130 - acc: 0.7199 - val_loss: 0.9342 - val_acc: 0.7050\n",
      "Epoch 31/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8866 - acc: 0.7247 - val_loss: 0.9121 - val_acc: 0.7140\n",
      "Epoch 32/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8628 - acc: 0.7287 - val_loss: 0.8882 - val_acc: 0.7180\n",
      "Epoch 33/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8401 - acc: 0.7332 - val_loss: 0.8681 - val_acc: 0.7210\n",
      "Epoch 34/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8194 - acc: 0.7377 - val_loss: 0.8489 - val_acc: 0.7240\n",
      "Epoch 35/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8003 - acc: 0.7409 - val_loss: 0.8322 - val_acc: 0.7320\n",
      "Epoch 36/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.7825 - acc: 0.7454 - val_loss: 0.8156 - val_acc: 0.7270\n",
      "Epoch 37/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.7658 - acc: 0.7487 - val_loss: 0.8024 - val_acc: 0.7330\n",
      "Epoch 38/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.7507 - acc: 0.7503 - val_loss: 0.7888 - val_acc: 0.7350\n",
      "Epoch 39/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.7365 - acc: 0.7533 - val_loss: 0.7749 - val_acc: 0.7340\n",
      "Epoch 40/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.7229 - acc: 0.7558 - val_loss: 0.7638 - val_acc: 0.7390\n",
      "Epoch 41/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.7105 - acc: 0.7587 - val_loss: 0.7534 - val_acc: 0.7370\n",
      "Epoch 42/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.6984 - acc: 0.7636 - val_loss: 0.7434 - val_acc: 0.7390\n",
      "Epoch 43/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.6873 - acc: 0.7659 - val_loss: 0.7343 - val_acc: 0.7400\n",
      "Epoch 44/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.6770 - acc: 0.7681 - val_loss: 0.7267 - val_acc: 0.7430\n",
      "Epoch 45/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.6673 - acc: 0.7733 - val_loss: 0.7182 - val_acc: 0.7430\n",
      "Epoch 46/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.6579 - acc: 0.7744 - val_loss: 0.7114 - val_acc: 0.7480\n",
      "Epoch 47/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.6488 - acc: 0.7774 - val_loss: 0.7059 - val_acc: 0.7470\n",
      "Epoch 48/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.6407 - acc: 0.7774 - val_loss: 0.7000 - val_acc: 0.7480\n",
      "Epoch 49/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.6326 - acc: 0.7801 - val_loss: 0.6904 - val_acc: 0.7540\n",
      "Epoch 50/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.6244 - acc: 0.7829 - val_loss: 0.6877 - val_acc: 0.7530\n",
      "Epoch 51/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.6174 - acc: 0.7840 - val_loss: 0.6805 - val_acc: 0.7540\n",
      "Epoch 52/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.6103 - acc: 0.7863 - val_loss: 0.6751 - val_acc: 0.7560\n",
      "Epoch 53/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.6033 - acc: 0.7900 - val_loss: 0.6717 - val_acc: 0.7530\n",
      "Epoch 54/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5971 - acc: 0.7914 - val_loss: 0.6656 - val_acc: 0.7590\n",
      "Epoch 55/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5904 - acc: 0.7954 - val_loss: 0.6618 - val_acc: 0.7620\n",
      "Epoch 56/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5845 - acc: 0.7954 - val_loss: 0.6589 - val_acc: 0.7640\n",
      "Epoch 57/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5787 - acc: 0.7959 - val_loss: 0.6541 - val_acc: 0.7580\n",
      "Epoch 58/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5724 - acc: 0.7996 - val_loss: 0.6520 - val_acc: 0.7650\n",
      "Epoch 59/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5671 - acc: 0.8013 - val_loss: 0.6477 - val_acc: 0.7680\n",
      "Epoch 60/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5620 - acc: 0.8014 - val_loss: 0.6441 - val_acc: 0.7650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5561 - acc: 0.8046 - val_loss: 0.6409 - val_acc: 0.7690\n",
      "Epoch 62/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5509 - acc: 0.8055 - val_loss: 0.6383 - val_acc: 0.7640\n",
      "Epoch 63/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5462 - acc: 0.8063 - val_loss: 0.6353 - val_acc: 0.7700\n",
      "Epoch 64/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5413 - acc: 0.8109 - val_loss: 0.6322 - val_acc: 0.7670\n",
      "Epoch 65/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5364 - acc: 0.8086 - val_loss: 0.6307 - val_acc: 0.7730\n",
      "Epoch 66/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5319 - acc: 0.8126 - val_loss: 0.6293 - val_acc: 0.7740\n",
      "Epoch 67/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5271 - acc: 0.8133 - val_loss: 0.6275 - val_acc: 0.7750\n",
      "Epoch 68/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5225 - acc: 0.8156 - val_loss: 0.6227 - val_acc: 0.7780\n",
      "Epoch 69/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5180 - acc: 0.8174 - val_loss: 0.6220 - val_acc: 0.7790\n",
      "Epoch 70/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.5143 - acc: 0.8205 - val_loss: 0.6192 - val_acc: 0.7800\n",
      "Epoch 71/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5094 - acc: 0.8227 - val_loss: 0.6177 - val_acc: 0.7800\n",
      "Epoch 72/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5054 - acc: 0.8240 - val_loss: 0.6157 - val_acc: 0.7820\n",
      "Epoch 73/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.5013 - acc: 0.8254 - val_loss: 0.6136 - val_acc: 0.7760\n",
      "Epoch 74/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.4972 - acc: 0.8271 - val_loss: 0.6118 - val_acc: 0.7840\n",
      "Epoch 75/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.4934 - acc: 0.8258 - val_loss: 0.6111 - val_acc: 0.7830\n",
      "Epoch 76/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.4892 - acc: 0.8300 - val_loss: 0.6105 - val_acc: 0.7780\n",
      "Epoch 77/80\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.4856 - acc: 0.8321 - val_loss: 0.6074 - val_acc: 0.7840\n",
      "Epoch 78/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.4819 - acc: 0.8341 - val_loss: 0.6111 - val_acc: 0.7780\n",
      "Epoch 79/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.4780 - acc: 0.8340 - val_loss: 0.6057 - val_acc: 0.7780\n",
      "Epoch 80/80\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.4736 - acc: 0.8382 - val_loss: 0.6070 - val_acc: 0.7820\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=80,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 30us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5689828497727712, 0.8097333333651224]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7319343857765198, 0.7146666668256124]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7800 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7800/7800 [==============================] - 0s 62us/step - loss: 2.5962 - acc: 0.1713 - val_loss: 2.5793 - val_acc: 0.2010\n",
      "Epoch 2/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 2.5679 - acc: 0.2091 - val_loss: 2.5524 - val_acc: 0.2450\n",
      "Epoch 3/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.5407 - acc: 0.2391 - val_loss: 2.5221 - val_acc: 0.2720\n",
      "Epoch 4/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.5096 - acc: 0.2635 - val_loss: 2.4880 - val_acc: 0.2820\n",
      "Epoch 5/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.4747 - acc: 0.2858 - val_loss: 2.4507 - val_acc: 0.3180\n",
      "Epoch 6/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.4365 - acc: 0.3086 - val_loss: 2.4117 - val_acc: 0.3480\n",
      "Epoch 7/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.3949 - acc: 0.3342 - val_loss: 2.3692 - val_acc: 0.3670\n",
      "Epoch 8/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.3496 - acc: 0.3556 - val_loss: 2.3238 - val_acc: 0.3860\n",
      "Epoch 9/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.3016 - acc: 0.3878 - val_loss: 2.2758 - val_acc: 0.4180\n",
      "Epoch 10/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.2517 - acc: 0.4192 - val_loss: 2.2260 - val_acc: 0.4410\n",
      "Epoch 11/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.2004 - acc: 0.4532 - val_loss: 2.1773 - val_acc: 0.4720\n",
      "Epoch 12/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.1487 - acc: 0.4878 - val_loss: 2.1275 - val_acc: 0.5080\n",
      "Epoch 13/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.0970 - acc: 0.5210 - val_loss: 2.0787 - val_acc: 0.5320\n",
      "Epoch 14/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.0462 - acc: 0.5515 - val_loss: 2.0296 - val_acc: 0.5620\n",
      "Epoch 15/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.9963 - acc: 0.5765 - val_loss: 1.9830 - val_acc: 0.5880\n",
      "Epoch 16/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.9476 - acc: 0.5996 - val_loss: 1.9378 - val_acc: 0.5970\n",
      "Epoch 17/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.9011 - acc: 0.6153 - val_loss: 1.8938 - val_acc: 0.6220\n",
      "Epoch 18/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.8559 - acc: 0.6338 - val_loss: 1.8523 - val_acc: 0.6280\n",
      "Epoch 19/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.8123 - acc: 0.6526 - val_loss: 1.8115 - val_acc: 0.6440\n",
      "Epoch 20/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.7710 - acc: 0.6614 - val_loss: 1.7739 - val_acc: 0.6470\n",
      "Epoch 21/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.7312 - acc: 0.6738 - val_loss: 1.7379 - val_acc: 0.6580\n",
      "Epoch 22/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.6936 - acc: 0.6850 - val_loss: 1.7026 - val_acc: 0.6660\n",
      "Epoch 23/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6581 - acc: 0.6926 - val_loss: 1.6697 - val_acc: 0.6700\n",
      "Epoch 24/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6245 - acc: 0.6986 - val_loss: 1.6393 - val_acc: 0.6740\n",
      "Epoch 25/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5933 - acc: 0.7050 - val_loss: 1.6102 - val_acc: 0.6910\n",
      "Epoch 26/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.5631 - acc: 0.7129 - val_loss: 1.5844 - val_acc: 0.6930\n",
      "Epoch 27/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5356 - acc: 0.7194 - val_loss: 1.5596 - val_acc: 0.7010\n",
      "Epoch 28/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.5091 - acc: 0.7241 - val_loss: 1.5343 - val_acc: 0.7050\n",
      "Epoch 29/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4844 - acc: 0.7306 - val_loss: 1.5110 - val_acc: 0.7070\n",
      "Epoch 30/120\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.4612 - acc: 0.7345 - val_loss: 1.4903 - val_acc: 0.7130\n",
      "Epoch 31/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4391 - acc: 0.7412 - val_loss: 1.4703 - val_acc: 0.7140\n",
      "Epoch 32/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4191 - acc: 0.7427 - val_loss: 1.4523 - val_acc: 0.7190\n",
      "Epoch 33/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.3995 - acc: 0.7477 - val_loss: 1.4339 - val_acc: 0.7180\n",
      "Epoch 34/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3812 - acc: 0.7503 - val_loss: 1.4183 - val_acc: 0.7220\n",
      "Epoch 35/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3642 - acc: 0.7535 - val_loss: 1.4018 - val_acc: 0.7260\n",
      "Epoch 36/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.3479 - acc: 0.7592 - val_loss: 1.3881 - val_acc: 0.7380\n",
      "Epoch 37/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.3324 - acc: 0.7636 - val_loss: 1.3733 - val_acc: 0.7320\n",
      "Epoch 38/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.3177 - acc: 0.7629 - val_loss: 1.3617 - val_acc: 0.7300\n",
      "Epoch 39/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.3040 - acc: 0.7647 - val_loss: 1.3486 - val_acc: 0.7280\n",
      "Epoch 40/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2904 - acc: 0.7692 - val_loss: 1.3387 - val_acc: 0.7270\n",
      "Epoch 41/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.2779 - acc: 0.7731 - val_loss: 1.3265 - val_acc: 0.7390\n",
      "Epoch 42/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.2658 - acc: 0.7763 - val_loss: 1.3192 - val_acc: 0.7420\n",
      "Epoch 43/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.2543 - acc: 0.7776 - val_loss: 1.3072 - val_acc: 0.7370\n",
      "Epoch 44/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2433 - acc: 0.7801 - val_loss: 1.2971 - val_acc: 0.7350\n",
      "Epoch 45/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.2328 - acc: 0.7818 - val_loss: 1.2906 - val_acc: 0.7370\n",
      "Epoch 46/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.2225 - acc: 0.7844 - val_loss: 1.2808 - val_acc: 0.7430\n",
      "Epoch 47/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.2126 - acc: 0.7864 - val_loss: 1.2738 - val_acc: 0.7460\n",
      "Epoch 48/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.2032 - acc: 0.7867 - val_loss: 1.2647 - val_acc: 0.7420\n",
      "Epoch 49/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1938 - acc: 0.7914 - val_loss: 1.2576 - val_acc: 0.7430\n",
      "Epoch 50/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1848 - acc: 0.7914 - val_loss: 1.2517 - val_acc: 0.7490\n",
      "Epoch 51/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1765 - acc: 0.7938 - val_loss: 1.2455 - val_acc: 0.7530\n",
      "Epoch 52/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1677 - acc: 0.7995 - val_loss: 1.2403 - val_acc: 0.7550\n",
      "Epoch 53/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1600 - acc: 0.7990 - val_loss: 1.2328 - val_acc: 0.7560\n",
      "Epoch 54/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1519 - acc: 0.8004 - val_loss: 1.2264 - val_acc: 0.7520\n",
      "Epoch 55/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.1442 - acc: 0.7988 - val_loss: 1.2197 - val_acc: 0.7550\n",
      "Epoch 56/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1366 - acc: 0.8032 - val_loss: 1.2143 - val_acc: 0.7560\n",
      "Epoch 57/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.1292 - acc: 0.8050 - val_loss: 1.2091 - val_acc: 0.7560\n",
      "Epoch 58/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.1220 - acc: 0.8073 - val_loss: 1.2057 - val_acc: 0.7540\n",
      "Epoch 59/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.1148 - acc: 0.8087 - val_loss: 1.1988 - val_acc: 0.7550\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1079 - acc: 0.8112 - val_loss: 1.1934 - val_acc: 0.7550\n",
      "Epoch 61/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1013 - acc: 0.8123 - val_loss: 1.1900 - val_acc: 0.7610\n",
      "Epoch 62/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.0945 - acc: 0.8163 - val_loss: 1.1852 - val_acc: 0.7510\n",
      "Epoch 63/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0886 - acc: 0.8142 - val_loss: 1.1802 - val_acc: 0.7590\n",
      "Epoch 64/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.0818 - acc: 0.8183 - val_loss: 1.1757 - val_acc: 0.7610\n",
      "Epoch 65/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0749 - acc: 0.8187 - val_loss: 1.1723 - val_acc: 0.7600\n",
      "Epoch 66/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0695 - acc: 0.8208 - val_loss: 1.1688 - val_acc: 0.7660\n",
      "Epoch 67/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.0628 - acc: 0.8217 - val_loss: 1.1655 - val_acc: 0.7650\n",
      "Epoch 68/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0571 - acc: 0.8244 - val_loss: 1.1590 - val_acc: 0.7620\n",
      "Epoch 69/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0512 - acc: 0.8265 - val_loss: 1.1572 - val_acc: 0.7630\n",
      "Epoch 70/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.0453 - acc: 0.8276 - val_loss: 1.1512 - val_acc: 0.7650\n",
      "Epoch 71/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.0400 - acc: 0.8281 - val_loss: 1.1477 - val_acc: 0.7630\n",
      "Epoch 72/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0339 - acc: 0.8277 - val_loss: 1.1442 - val_acc: 0.7660\n",
      "Epoch 73/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0287 - acc: 0.8326 - val_loss: 1.1415 - val_acc: 0.7650\n",
      "Epoch 74/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0236 - acc: 0.8319 - val_loss: 1.1384 - val_acc: 0.7700\n",
      "Epoch 75/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.0172 - acc: 0.8338 - val_loss: 1.1351 - val_acc: 0.7670\n",
      "Epoch 76/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0123 - acc: 0.8344 - val_loss: 1.1310 - val_acc: 0.7670\n",
      "Epoch 77/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.0071 - acc: 0.8350 - val_loss: 1.1297 - val_acc: 0.7630\n",
      "Epoch 78/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0017 - acc: 0.8392 - val_loss: 1.1237 - val_acc: 0.7650\n",
      "Epoch 79/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9969 - acc: 0.8388 - val_loss: 1.1245 - val_acc: 0.7650\n",
      "Epoch 80/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9918 - acc: 0.8394 - val_loss: 1.1210 - val_acc: 0.7690\n",
      "Epoch 81/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9867 - acc: 0.8414 - val_loss: 1.1168 - val_acc: 0.7670\n",
      "Epoch 82/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9818 - acc: 0.8422 - val_loss: 1.1133 - val_acc: 0.7760\n",
      "Epoch 83/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9771 - acc: 0.8426 - val_loss: 1.1103 - val_acc: 0.7750\n",
      "Epoch 84/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9723 - acc: 0.8440 - val_loss: 1.1085 - val_acc: 0.7690\n",
      "Epoch 85/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9677 - acc: 0.8468 - val_loss: 1.1049 - val_acc: 0.7760\n",
      "Epoch 86/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9629 - acc: 0.8471 - val_loss: 1.1012 - val_acc: 0.7650\n",
      "Epoch 87/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9582 - acc: 0.8474 - val_loss: 1.0991 - val_acc: 0.7750\n",
      "Epoch 88/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9533 - acc: 0.8500 - val_loss: 1.0976 - val_acc: 0.7820\n",
      "Epoch 89/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9491 - acc: 0.8490 - val_loss: 1.0926 - val_acc: 0.7700\n",
      "Epoch 90/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9444 - acc: 0.8506 - val_loss: 1.0917 - val_acc: 0.7760\n",
      "Epoch 91/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9399 - acc: 0.8508 - val_loss: 1.0917 - val_acc: 0.7790\n",
      "Epoch 92/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9357 - acc: 0.8529 - val_loss: 1.0853 - val_acc: 0.7750\n",
      "Epoch 93/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9312 - acc: 0.8540 - val_loss: 1.0850 - val_acc: 0.7790\n",
      "Epoch 94/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9267 - acc: 0.8560 - val_loss: 1.0821 - val_acc: 0.7730\n",
      "Epoch 95/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9229 - acc: 0.8562 - val_loss: 1.0804 - val_acc: 0.7800\n",
      "Epoch 96/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9187 - acc: 0.8582 - val_loss: 1.0774 - val_acc: 0.7810\n",
      "Epoch 97/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9144 - acc: 0.8572 - val_loss: 1.0745 - val_acc: 0.7730\n",
      "Epoch 98/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9102 - acc: 0.8595 - val_loss: 1.0722 - val_acc: 0.7760\n",
      "Epoch 99/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9062 - acc: 0.8619 - val_loss: 1.0696 - val_acc: 0.7800\n",
      "Epoch 100/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.9021 - acc: 0.8622 - val_loss: 1.0705 - val_acc: 0.7770\n",
      "Epoch 101/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8982 - acc: 0.8635 - val_loss: 1.0644 - val_acc: 0.7800\n",
      "Epoch 102/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8937 - acc: 0.8654 - val_loss: 1.0649 - val_acc: 0.7770\n",
      "Epoch 103/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8902 - acc: 0.8659 - val_loss: 1.0620 - val_acc: 0.7750\n",
      "Epoch 104/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8860 - acc: 0.8685 - val_loss: 1.0596 - val_acc: 0.7730\n",
      "Epoch 105/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8824 - acc: 0.8668 - val_loss: 1.0589 - val_acc: 0.7750\n",
      "Epoch 106/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8783 - acc: 0.8688 - val_loss: 1.0560 - val_acc: 0.7790\n",
      "Epoch 107/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8745 - acc: 0.8709 - val_loss: 1.0576 - val_acc: 0.7790\n",
      "Epoch 108/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8711 - acc: 0.8703 - val_loss: 1.0532 - val_acc: 0.7730\n",
      "Epoch 109/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8668 - acc: 0.8713 - val_loss: 1.0481 - val_acc: 0.7800\n",
      "Epoch 110/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8632 - acc: 0.8731 - val_loss: 1.0470 - val_acc: 0.7750\n",
      "Epoch 111/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8599 - acc: 0.8728 - val_loss: 1.0464 - val_acc: 0.7790\n",
      "Epoch 112/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8561 - acc: 0.8741 - val_loss: 1.0430 - val_acc: 0.7800\n",
      "Epoch 113/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8525 - acc: 0.8744 - val_loss: 1.0396 - val_acc: 0.7810\n",
      "Epoch 114/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8492 - acc: 0.8755 - val_loss: 1.0406 - val_acc: 0.7830\n",
      "Epoch 115/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8455 - acc: 0.8781 - val_loss: 1.0367 - val_acc: 0.7780\n",
      "Epoch 116/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8415 - acc: 0.8778 - val_loss: 1.0348 - val_acc: 0.7810\n",
      "Epoch 117/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8385 - acc: 0.8787 - val_loss: 1.0350 - val_acc: 0.7840\n",
      "Epoch 118/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8349 - acc: 0.8800 - val_loss: 1.0327 - val_acc: 0.7780\n",
      "Epoch 119/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8318 - acc: 0.8805 - val_loss: 1.0321 - val_acc: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8284 - acc: 0.8815 - val_loss: 1.0272 - val_acc: 0.7800\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7800 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7800/7800 [==============================] - 1s 71us/step - loss: 15.9479 - acc: 0.1592 - val_loss: 15.5279 - val_acc: 0.1570\n",
      "Epoch 2/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 15.1623 - acc: 0.1722 - val_loss: 14.7589 - val_acc: 0.1820\n",
      "Epoch 3/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 14.4046 - acc: 0.1945 - val_loss: 14.0128 - val_acc: 0.2200\n",
      "Epoch 4/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 13.6688 - acc: 0.2242 - val_loss: 13.2876 - val_acc: 0.2520\n",
      "Epoch 5/120\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 12.9543 - acc: 0.2585 - val_loss: 12.5841 - val_acc: 0.2820\n",
      "Epoch 6/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 12.2605 - acc: 0.2890 - val_loss: 11.8998 - val_acc: 0.3290\n",
      "Epoch 7/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 11.5868 - acc: 0.3246 - val_loss: 11.2363 - val_acc: 0.3650\n",
      "Epoch 8/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 10.9335 - acc: 0.3627 - val_loss: 10.5939 - val_acc: 0.4200\n",
      "Epoch 9/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 10.3001 - acc: 0.4019 - val_loss: 9.9715 - val_acc: 0.4430\n",
      "Epoch 10/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 9.6875 - acc: 0.4187 - val_loss: 9.3708 - val_acc: 0.4640\n",
      "Epoch 11/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 9.0961 - acc: 0.4405 - val_loss: 8.7918 - val_acc: 0.4830\n",
      "Epoch 12/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 8.5275 - acc: 0.4603 - val_loss: 8.2367 - val_acc: 0.4890\n",
      "Epoch 13/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 7.9835 - acc: 0.4760 - val_loss: 7.7068 - val_acc: 0.5110\n",
      "Epoch 14/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 7.4642 - acc: 0.4919 - val_loss: 7.2006 - val_acc: 0.5150\n",
      "Epoch 15/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 6.9699 - acc: 0.4981 - val_loss: 6.7190 - val_acc: 0.5260\n",
      "Epoch 16/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 6.5004 - acc: 0.5091 - val_loss: 6.2638 - val_acc: 0.5410\n",
      "Epoch 17/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 6.0563 - acc: 0.5182 - val_loss: 5.8329 - val_acc: 0.5480\n",
      "Epoch 18/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 5.6376 - acc: 0.5335 - val_loss: 5.4269 - val_acc: 0.5500\n",
      "Epoch 19/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 5.2442 - acc: 0.5471 - val_loss: 5.0475 - val_acc: 0.5700\n",
      "Epoch 20/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 4.8762 - acc: 0.5622 - val_loss: 4.6915 - val_acc: 0.5660\n",
      "Epoch 21/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 4.5326 - acc: 0.5678 - val_loss: 4.3601 - val_acc: 0.5820\n",
      "Epoch 22/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 4.2132 - acc: 0.5772 - val_loss: 4.0538 - val_acc: 0.5980\n",
      "Epoch 23/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 3.9179 - acc: 0.5874 - val_loss: 3.7706 - val_acc: 0.6040\n",
      "Epoch 24/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 3.6462 - acc: 0.5995 - val_loss: 3.5110 - val_acc: 0.6090\n",
      "Epoch 25/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 3.3982 - acc: 0.6032 - val_loss: 3.2759 - val_acc: 0.6180\n",
      "Epoch 26/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 3.1738 - acc: 0.6112 - val_loss: 3.0627 - val_acc: 0.6220\n",
      "Epoch 27/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.9722 - acc: 0.6185 - val_loss: 2.8725 - val_acc: 0.6230\n",
      "Epoch 28/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.7931 - acc: 0.6219 - val_loss: 2.7066 - val_acc: 0.6310\n",
      "Epoch 29/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.6363 - acc: 0.6290 - val_loss: 2.5594 - val_acc: 0.6370\n",
      "Epoch 30/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 2.5006 - acc: 0.6355 - val_loss: 2.4350 - val_acc: 0.6320\n",
      "Epoch 31/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 2.3863 - acc: 0.6381 - val_loss: 2.3306 - val_acc: 0.6410\n",
      "Epoch 32/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.2922 - acc: 0.6438 - val_loss: 2.2463 - val_acc: 0.6400\n",
      "Epoch 33/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 2.2177 - acc: 0.6404 - val_loss: 2.1828 - val_acc: 0.6490\n",
      "Epoch 34/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 2.1606 - acc: 0.6463 - val_loss: 2.1336 - val_acc: 0.6550\n",
      "Epoch 35/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.1186 - acc: 0.6531 - val_loss: 2.0959 - val_acc: 0.6520\n",
      "Epoch 36/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 2.0873 - acc: 0.6551 - val_loss: 2.0696 - val_acc: 0.6530\n",
      "Epoch 37/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.0616 - acc: 0.6583 - val_loss: 2.0469 - val_acc: 0.6620\n",
      "Epoch 38/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 2.0396 - acc: 0.6592 - val_loss: 2.0260 - val_acc: 0.6640\n",
      "Epoch 39/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 2.0191 - acc: 0.6654 - val_loss: 2.0043 - val_acc: 0.6710\n",
      "Epoch 40/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.9993 - acc: 0.6688 - val_loss: 1.9874 - val_acc: 0.6710\n",
      "Epoch 41/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.9812 - acc: 0.6704 - val_loss: 1.9676 - val_acc: 0.6750\n",
      "Epoch 42/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.9637 - acc: 0.6722 - val_loss: 1.9539 - val_acc: 0.6760\n",
      "Epoch 43/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.9471 - acc: 0.6778 - val_loss: 1.9348 - val_acc: 0.6760\n",
      "Epoch 44/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.9311 - acc: 0.6759 - val_loss: 1.9180 - val_acc: 0.6780\n",
      "Epoch 45/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.9149 - acc: 0.6797 - val_loss: 1.9038 - val_acc: 0.6790\n",
      "Epoch 46/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.8995 - acc: 0.6827 - val_loss: 1.8875 - val_acc: 0.6790\n",
      "Epoch 47/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.8847 - acc: 0.6842 - val_loss: 1.8731 - val_acc: 0.6790\n",
      "Epoch 48/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.8700 - acc: 0.6856 - val_loss: 1.8587 - val_acc: 0.6790\n",
      "Epoch 49/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.8557 - acc: 0.6871 - val_loss: 1.8443 - val_acc: 0.6820\n",
      "Epoch 50/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.8414 - acc: 0.6906 - val_loss: 1.8307 - val_acc: 0.6790\n",
      "Epoch 51/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.8277 - acc: 0.6912 - val_loss: 1.8198 - val_acc: 0.6900\n",
      "Epoch 52/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.8144 - acc: 0.6918 - val_loss: 1.8043 - val_acc: 0.6880\n",
      "Epoch 53/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.8014 - acc: 0.6924 - val_loss: 1.7913 - val_acc: 0.7010\n",
      "Epoch 54/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.7885 - acc: 0.6955 - val_loss: 1.7788 - val_acc: 0.6930\n",
      "Epoch 55/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.7761 - acc: 0.6959 - val_loss: 1.7666 - val_acc: 0.7020\n",
      "Epoch 56/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.7632 - acc: 0.6982 - val_loss: 1.7541 - val_acc: 0.7060\n",
      "Epoch 57/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.7510 - acc: 0.6988 - val_loss: 1.7423 - val_acc: 0.7010\n",
      "Epoch 58/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.7394 - acc: 0.6988 - val_loss: 1.7310 - val_acc: 0.7050\n",
      "Epoch 59/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.7274 - acc: 0.7004 - val_loss: 1.7214 - val_acc: 0.7020\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.7160 - acc: 0.7014 - val_loss: 1.7073 - val_acc: 0.7110\n",
      "Epoch 61/120\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.7044 - acc: 0.7022 - val_loss: 1.6970 - val_acc: 0.7090\n",
      "Epoch 62/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6936 - acc: 0.7003 - val_loss: 1.6871 - val_acc: 0.7070\n",
      "Epoch 63/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6828 - acc: 0.7024 - val_loss: 1.6751 - val_acc: 0.7040\n",
      "Epoch 64/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6715 - acc: 0.7044 - val_loss: 1.6644 - val_acc: 0.7080\n",
      "Epoch 65/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6611 - acc: 0.7045 - val_loss: 1.6536 - val_acc: 0.7040\n",
      "Epoch 66/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6512 - acc: 0.7047 - val_loss: 1.6445 - val_acc: 0.7060\n",
      "Epoch 67/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6409 - acc: 0.7055 - val_loss: 1.6338 - val_acc: 0.7070\n",
      "Epoch 68/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6310 - acc: 0.7069 - val_loss: 1.6239 - val_acc: 0.7070\n",
      "Epoch 69/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6212 - acc: 0.7069 - val_loss: 1.6144 - val_acc: 0.7050\n",
      "Epoch 70/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6118 - acc: 0.7077 - val_loss: 1.6075 - val_acc: 0.6990\n",
      "Epoch 71/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6022 - acc: 0.7092 - val_loss: 1.5963 - val_acc: 0.7050\n",
      "Epoch 72/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.5929 - acc: 0.7087 - val_loss: 1.5857 - val_acc: 0.7070\n",
      "Epoch 73/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5833 - acc: 0.7119 - val_loss: 1.5775 - val_acc: 0.7080\n",
      "Epoch 74/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5743 - acc: 0.7109 - val_loss: 1.5676 - val_acc: 0.7100\n",
      "Epoch 75/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.5651 - acc: 0.7101 - val_loss: 1.5604 - val_acc: 0.7120\n",
      "Epoch 76/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.5567 - acc: 0.7113 - val_loss: 1.5512 - val_acc: 0.7090\n",
      "Epoch 77/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5482 - acc: 0.7131 - val_loss: 1.5457 - val_acc: 0.7060\n",
      "Epoch 78/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5396 - acc: 0.7129 - val_loss: 1.5342 - val_acc: 0.7160\n",
      "Epoch 79/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5316 - acc: 0.7127 - val_loss: 1.5256 - val_acc: 0.7110\n",
      "Epoch 80/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5234 - acc: 0.7124 - val_loss: 1.5197 - val_acc: 0.7150\n",
      "Epoch 81/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5160 - acc: 0.7140 - val_loss: 1.5117 - val_acc: 0.7100\n",
      "Epoch 82/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.5081 - acc: 0.7140 - val_loss: 1.5034 - val_acc: 0.7120\n",
      "Epoch 83/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.5001 - acc: 0.7147 - val_loss: 1.4963 - val_acc: 0.7110\n",
      "Epoch 84/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4927 - acc: 0.7154 - val_loss: 1.4893 - val_acc: 0.7120\n",
      "Epoch 85/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4850 - acc: 0.7155 - val_loss: 1.4835 - val_acc: 0.7090\n",
      "Epoch 86/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4780 - acc: 0.7153 - val_loss: 1.4733 - val_acc: 0.7160\n",
      "Epoch 87/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4706 - acc: 0.7160 - val_loss: 1.4674 - val_acc: 0.7110\n",
      "Epoch 88/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4636 - acc: 0.7163 - val_loss: 1.4598 - val_acc: 0.7110\n",
      "Epoch 89/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4564 - acc: 0.7172 - val_loss: 1.4533 - val_acc: 0.7210\n",
      "Epoch 90/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4495 - acc: 0.7178 - val_loss: 1.4477 - val_acc: 0.7140\n",
      "Epoch 91/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4429 - acc: 0.7177 - val_loss: 1.4401 - val_acc: 0.7170\n",
      "Epoch 92/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4360 - acc: 0.7190 - val_loss: 1.4327 - val_acc: 0.7190\n",
      "Epoch 93/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4298 - acc: 0.7195 - val_loss: 1.4260 - val_acc: 0.7190\n",
      "Epoch 94/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4229 - acc: 0.7204 - val_loss: 1.4198 - val_acc: 0.7120\n",
      "Epoch 95/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4159 - acc: 0.7208 - val_loss: 1.4131 - val_acc: 0.7170\n",
      "Epoch 96/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4099 - acc: 0.7205 - val_loss: 1.4070 - val_acc: 0.7160\n",
      "Epoch 97/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.4035 - acc: 0.7203 - val_loss: 1.4011 - val_acc: 0.7180\n",
      "Epoch 98/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.3980 - acc: 0.7223 - val_loss: 1.3946 - val_acc: 0.7170\n",
      "Epoch 99/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.3910 - acc: 0.7204 - val_loss: 1.3884 - val_acc: 0.7180\n",
      "Epoch 100/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.3849 - acc: 0.7215 - val_loss: 1.3834 - val_acc: 0.7180\n",
      "Epoch 101/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3786 - acc: 0.7224 - val_loss: 1.3777 - val_acc: 0.7190\n",
      "Epoch 102/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.3726 - acc: 0.7226 - val_loss: 1.3705 - val_acc: 0.7200\n",
      "Epoch 103/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3668 - acc: 0.7232 - val_loss: 1.3656 - val_acc: 0.7190\n",
      "Epoch 104/120\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 1.3608 - acc: 0.7233 - val_loss: 1.3609 - val_acc: 0.7280\n",
      "Epoch 105/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3552 - acc: 0.7231 - val_loss: 1.3537 - val_acc: 0.7220\n",
      "Epoch 106/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3494 - acc: 0.7241 - val_loss: 1.3517 - val_acc: 0.7190\n",
      "Epoch 107/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3440 - acc: 0.7245 - val_loss: 1.3419 - val_acc: 0.7230\n",
      "Epoch 108/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3386 - acc: 0.7249 - val_loss: 1.3400 - val_acc: 0.7200\n",
      "Epoch 109/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3328 - acc: 0.7258 - val_loss: 1.3327 - val_acc: 0.7220\n",
      "Epoch 110/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3274 - acc: 0.7251 - val_loss: 1.3279 - val_acc: 0.7240\n",
      "Epoch 111/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3222 - acc: 0.7260 - val_loss: 1.3225 - val_acc: 0.7210\n",
      "Epoch 112/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3172 - acc: 0.7263 - val_loss: 1.3161 - val_acc: 0.7210\n",
      "Epoch 113/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3121 - acc: 0.7260 - val_loss: 1.3139 - val_acc: 0.7250\n",
      "Epoch 114/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3065 - acc: 0.7264 - val_loss: 1.3068 - val_acc: 0.7260\n",
      "Epoch 115/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3015 - acc: 0.7285 - val_loss: 1.3012 - val_acc: 0.7250\n",
      "Epoch 116/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2962 - acc: 0.7273 - val_loss: 1.2981 - val_acc: 0.7220\n",
      "Epoch 117/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2914 - acc: 0.7268 - val_loss: 1.2942 - val_acc: 0.7260\n",
      "Epoch 118/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2866 - acc: 0.7273 - val_loss: 1.2865 - val_acc: 0.7250\n",
      "Epoch 119/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2819 - acc: 0.7295 - val_loss: 1.2820 - val_acc: 0.7220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2773 - acc: 0.7291 - val_loss: 1.2787 - val_acc: 0.7240\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7800 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7800/7800 [==============================] - 1s 74us/step - loss: 15.9703 - acc: 0.1795 - val_loss: 15.5464 - val_acc: 0.2120\n",
      "Epoch 2/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 15.1824 - acc: 0.2062 - val_loss: 14.7729 - val_acc: 0.2240\n",
      "Epoch 3/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 14.4180 - acc: 0.2233 - val_loss: 14.0201 - val_acc: 0.2440\n",
      "Epoch 4/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 13.6746 - acc: 0.2341 - val_loss: 13.2880 - val_acc: 0.2460\n",
      "Epoch 5/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 12.9513 - acc: 0.2469 - val_loss: 12.5760 - val_acc: 0.2590\n",
      "Epoch 6/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 12.2478 - acc: 0.2690 - val_loss: 11.8842 - val_acc: 0.2770\n",
      "Epoch 7/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 11.5645 - acc: 0.2973 - val_loss: 11.2125 - val_acc: 0.3200\n",
      "Epoch 8/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 10.9018 - acc: 0.3427 - val_loss: 10.5619 - val_acc: 0.3660\n",
      "Epoch 9/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 10.2609 - acc: 0.3837 - val_loss: 9.9342 - val_acc: 0.4000\n",
      "Epoch 10/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 9.6444 - acc: 0.4332 - val_loss: 9.3317 - val_acc: 0.4340\n",
      "Epoch 11/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 9.0527 - acc: 0.4699 - val_loss: 8.7543 - val_acc: 0.4840\n",
      "Epoch 12/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 8.4863 - acc: 0.5094 - val_loss: 8.2026 - val_acc: 0.4870\n",
      "Epoch 13/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 7.9453 - acc: 0.5332 - val_loss: 7.6747 - val_acc: 0.5470\n",
      "Epoch 14/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 7.4291 - acc: 0.5662 - val_loss: 7.1729 - val_acc: 0.5710\n",
      "Epoch 15/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 6.9385 - acc: 0.5906 - val_loss: 6.6967 - val_acc: 0.5990\n",
      "Epoch 16/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 6.4729 - acc: 0.6095 - val_loss: 6.2449 - val_acc: 0.6040\n",
      "Epoch 17/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 6.0325 - acc: 0.6271 - val_loss: 5.8185 - val_acc: 0.6230\n",
      "Epoch 18/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 5.6170 - acc: 0.6399 - val_loss: 5.4181 - val_acc: 0.6430\n",
      "Epoch 19/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 5.2263 - acc: 0.6549 - val_loss: 5.0396 - val_acc: 0.6470\n",
      "Epoch 20/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 4.8592 - acc: 0.6612 - val_loss: 4.6854 - val_acc: 0.6440\n",
      "Epoch 21/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 4.5166 - acc: 0.6668 - val_loss: 4.3567 - val_acc: 0.6470\n",
      "Epoch 22/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 4.1985 - acc: 0.6740 - val_loss: 4.0526 - val_acc: 0.6610\n",
      "Epoch 23/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 3.9050 - acc: 0.6806 - val_loss: 3.7711 - val_acc: 0.6620\n",
      "Epoch 24/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 3.6352 - acc: 0.6827 - val_loss: 3.5137 - val_acc: 0.6710\n",
      "Epoch 25/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 3.3894 - acc: 0.6849 - val_loss: 3.2808 - val_acc: 0.6810\n",
      "Epoch 26/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 3.1663 - acc: 0.6886 - val_loss: 3.0694 - val_acc: 0.6800\n",
      "Epoch 27/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 2.9664 - acc: 0.6882 - val_loss: 2.8815 - val_acc: 0.6860\n",
      "Epoch 28/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.7888 - acc: 0.6881 - val_loss: 2.7137 - val_acc: 0.6800\n",
      "Epoch 29/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 2.6329 - acc: 0.6883 - val_loss: 2.5693 - val_acc: 0.6820\n",
      "Epoch 30/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.4987 - acc: 0.6882 - val_loss: 2.4460 - val_acc: 0.6830\n",
      "Epoch 31/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.3853 - acc: 0.6887 - val_loss: 2.3420 - val_acc: 0.6880\n",
      "Epoch 32/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 2.2918 - acc: 0.6887 - val_loss: 2.2580 - val_acc: 0.6860\n",
      "Epoch 33/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 2.2165 - acc: 0.6882 - val_loss: 2.1929 - val_acc: 0.6860\n",
      "Epoch 34/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 2.1590 - acc: 0.6859 - val_loss: 2.1423 - val_acc: 0.6880\n",
      "Epoch 35/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 2.1154 - acc: 0.6863 - val_loss: 2.1055 - val_acc: 0.6890\n",
      "Epoch 36/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 2.0824 - acc: 0.6851 - val_loss: 2.0758 - val_acc: 0.6840\n",
      "Epoch 37/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 2.0557 - acc: 0.6856 - val_loss: 2.0518 - val_acc: 0.6830\n",
      "Epoch 38/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 2.0324 - acc: 0.6844 - val_loss: 2.0298 - val_acc: 0.6840\n",
      "Epoch 39/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 2.0112 - acc: 0.6871 - val_loss: 2.0105 - val_acc: 0.6760\n",
      "Epoch 40/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.9919 - acc: 0.6856 - val_loss: 1.9913 - val_acc: 0.6770\n",
      "Epoch 41/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.9732 - acc: 0.6846 - val_loss: 1.9728 - val_acc: 0.6760\n",
      "Epoch 42/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.9561 - acc: 0.6872 - val_loss: 1.9559 - val_acc: 0.6770\n",
      "Epoch 43/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.9393 - acc: 0.6876 - val_loss: 1.9394 - val_acc: 0.6770\n",
      "Epoch 44/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.9233 - acc: 0.6896 - val_loss: 1.9234 - val_acc: 0.6820\n",
      "Epoch 45/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.9085 - acc: 0.6910 - val_loss: 1.9096 - val_acc: 0.6760\n",
      "Epoch 46/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.8937 - acc: 0.6891 - val_loss: 1.8973 - val_acc: 0.6780\n",
      "Epoch 47/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.8798 - acc: 0.6910 - val_loss: 1.8831 - val_acc: 0.6800\n",
      "Epoch 48/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.8662 - acc: 0.6928 - val_loss: 1.8697 - val_acc: 0.6800\n",
      "Epoch 49/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.8532 - acc: 0.6919 - val_loss: 1.8575 - val_acc: 0.6780\n",
      "Epoch 50/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.8408 - acc: 0.6932 - val_loss: 1.8430 - val_acc: 0.6810\n",
      "Epoch 51/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.8284 - acc: 0.6924 - val_loss: 1.8311 - val_acc: 0.6800\n",
      "Epoch 52/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.8166 - acc: 0.6936 - val_loss: 1.8218 - val_acc: 0.6830\n",
      "Epoch 53/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.8051 - acc: 0.6936 - val_loss: 1.8094 - val_acc: 0.6770\n",
      "Epoch 54/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.7935 - acc: 0.6937 - val_loss: 1.7951 - val_acc: 0.6780\n",
      "Epoch 55/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.7826 - acc: 0.6937 - val_loss: 1.7854 - val_acc: 0.6770\n",
      "Epoch 56/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.7717 - acc: 0.6968 - val_loss: 1.7741 - val_acc: 0.6750\n",
      "Epoch 57/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.7608 - acc: 0.6950 - val_loss: 1.7620 - val_acc: 0.6870\n",
      "Epoch 58/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.7506 - acc: 0.6971 - val_loss: 1.7521 - val_acc: 0.6810\n",
      "Epoch 59/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.7405 - acc: 0.6965 - val_loss: 1.7483 - val_acc: 0.6810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.7308 - acc: 0.6962 - val_loss: 1.7329 - val_acc: 0.6860\n",
      "Epoch 61/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.7208 - acc: 0.6973 - val_loss: 1.7237 - val_acc: 0.6850\n",
      "Epoch 62/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.7116 - acc: 0.6995 - val_loss: 1.7143 - val_acc: 0.6780\n",
      "Epoch 63/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.7022 - acc: 0.6977 - val_loss: 1.7047 - val_acc: 0.6790\n",
      "Epoch 64/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6934 - acc: 0.6983 - val_loss: 1.6948 - val_acc: 0.6850\n",
      "Epoch 65/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.6839 - acc: 0.7000 - val_loss: 1.6869 - val_acc: 0.6860\n",
      "Epoch 66/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6756 - acc: 0.6988 - val_loss: 1.6788 - val_acc: 0.6880\n",
      "Epoch 67/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6669 - acc: 0.7000 - val_loss: 1.6700 - val_acc: 0.6890\n",
      "Epoch 68/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6581 - acc: 0.6999 - val_loss: 1.6663 - val_acc: 0.6880\n",
      "Epoch 69/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.6501 - acc: 0.7018 - val_loss: 1.6538 - val_acc: 0.6880\n",
      "Epoch 70/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6422 - acc: 0.7022 - val_loss: 1.6458 - val_acc: 0.6930\n",
      "Epoch 71/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.6336 - acc: 0.7035 - val_loss: 1.6385 - val_acc: 0.6950\n",
      "Epoch 72/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.6262 - acc: 0.7023 - val_loss: 1.6290 - val_acc: 0.6890\n",
      "Epoch 73/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.6185 - acc: 0.7033 - val_loss: 1.6237 - val_acc: 0.6880\n",
      "Epoch 74/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.6102 - acc: 0.7056 - val_loss: 1.6145 - val_acc: 0.6950\n",
      "Epoch 75/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.6029 - acc: 0.7046 - val_loss: 1.6075 - val_acc: 0.6980\n",
      "Epoch 76/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 1.5971 - acc: 0.703 - 0s 26us/step - loss: 1.5954 - acc: 0.7042 - val_loss: 1.6002 - val_acc: 0.6980\n",
      "Epoch 77/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.5880 - acc: 0.7055 - val_loss: 1.5942 - val_acc: 0.7010\n",
      "Epoch 78/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.5808 - acc: 0.7054 - val_loss: 1.5844 - val_acc: 0.6930\n",
      "Epoch 79/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.5737 - acc: 0.7071 - val_loss: 1.5802 - val_acc: 0.6970\n",
      "Epoch 80/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.5667 - acc: 0.7065 - val_loss: 1.5713 - val_acc: 0.6960\n",
      "Epoch 81/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.5594 - acc: 0.7062 - val_loss: 1.5654 - val_acc: 0.7020\n",
      "Epoch 82/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.5523 - acc: 0.7077 - val_loss: 1.5586 - val_acc: 0.6980\n",
      "Epoch 83/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.5453 - acc: 0.7071 - val_loss: 1.5499 - val_acc: 0.7050\n",
      "Epoch 84/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.5384 - acc: 0.7074 - val_loss: 1.5462 - val_acc: 0.7050\n",
      "Epoch 85/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.5316 - acc: 0.7072 - val_loss: 1.5391 - val_acc: 0.6980\n",
      "Epoch 86/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.5246 - acc: 0.7072 - val_loss: 1.5299 - val_acc: 0.7000\n",
      "Epoch 87/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.5189 - acc: 0.7086 - val_loss: 1.5249 - val_acc: 0.7030\n",
      "Epoch 88/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.5118 - acc: 0.7095 - val_loss: 1.5197 - val_acc: 0.7070\n",
      "Epoch 89/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.5054 - acc: 0.7087 - val_loss: 1.5113 - val_acc: 0.7030\n",
      "Epoch 90/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.4990 - acc: 0.7109 - val_loss: 1.5066 - val_acc: 0.7040\n",
      "Epoch 91/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.4923 - acc: 0.7113 - val_loss: 1.5002 - val_acc: 0.7050\n",
      "Epoch 92/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.4862 - acc: 0.7106 - val_loss: 1.4931 - val_acc: 0.7000\n",
      "Epoch 93/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4804 - acc: 0.7121 - val_loss: 1.4883 - val_acc: 0.7060\n",
      "Epoch 94/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4742 - acc: 0.7105 - val_loss: 1.4812 - val_acc: 0.7040\n",
      "Epoch 95/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4688 - acc: 0.7099 - val_loss: 1.4772 - val_acc: 0.7070\n",
      "Epoch 96/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4622 - acc: 0.7110 - val_loss: 1.4708 - val_acc: 0.7100\n",
      "Epoch 97/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4564 - acc: 0.7124 - val_loss: 1.4643 - val_acc: 0.7070\n",
      "Epoch 98/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4507 - acc: 0.7115 - val_loss: 1.4594 - val_acc: 0.7090\n",
      "Epoch 99/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.4452 - acc: 0.7123 - val_loss: 1.4524 - val_acc: 0.7090\n",
      "Epoch 100/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.4387 - acc: 0.7132 - val_loss: 1.4482 - val_acc: 0.7090\n",
      "Epoch 101/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4333 - acc: 0.7142 - val_loss: 1.4414 - val_acc: 0.7080\n",
      "Epoch 102/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4276 - acc: 0.7144 - val_loss: 1.4346 - val_acc: 0.7080\n",
      "Epoch 103/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.4222 - acc: 0.7142 - val_loss: 1.4312 - val_acc: 0.7120\n",
      "Epoch 104/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.4166 - acc: 0.7141 - val_loss: 1.4287 - val_acc: 0.7140\n",
      "Epoch 105/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.4111 - acc: 0.7147 - val_loss: 1.4202 - val_acc: 0.7120\n",
      "Epoch 106/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.4055 - acc: 0.7150 - val_loss: 1.4165 - val_acc: 0.7140\n",
      "Epoch 107/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.4004 - acc: 0.7153 - val_loss: 1.4080 - val_acc: 0.7110\n",
      "Epoch 108/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.3947 - acc: 0.7169 - val_loss: 1.4043 - val_acc: 0.7110\n",
      "Epoch 109/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.3899 - acc: 0.7149 - val_loss: 1.4010 - val_acc: 0.7120\n",
      "Epoch 110/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 1.3799 - acc: 0.718 - 0s 26us/step - loss: 1.3842 - acc: 0.7164 - val_loss: 1.3938 - val_acc: 0.7100\n",
      "Epoch 111/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3794 - acc: 0.7169 - val_loss: 1.3876 - val_acc: 0.7110\n",
      "Epoch 112/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3739 - acc: 0.7163 - val_loss: 1.3820 - val_acc: 0.7120\n",
      "Epoch 113/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3689 - acc: 0.7169 - val_loss: 1.3789 - val_acc: 0.7120\n",
      "Epoch 114/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.3636 - acc: 0.7172 - val_loss: 1.3783 - val_acc: 0.7150\n",
      "Epoch 115/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3590 - acc: 0.7167 - val_loss: 1.3684 - val_acc: 0.7140\n",
      "Epoch 116/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.3540 - acc: 0.7183 - val_loss: 1.3623 - val_acc: 0.7120\n",
      "Epoch 117/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3484 - acc: 0.7188 - val_loss: 1.3622 - val_acc: 0.7150\n",
      "Epoch 118/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.3444 - acc: 0.7194 - val_loss: 1.3613 - val_acc: 0.7110\n",
      "Epoch 119/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.3395 - acc: 0.7188 - val_loss: 1.3512 - val_acc: 0.7130\n",
      "Epoch 120/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3349 - acc: 0.7196 - val_loss: 1.3471 - val_acc: 0.7140\n",
      "Epoch 121/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.3302 - acc: 0.7188 - val_loss: 1.3427 - val_acc: 0.7160\n",
      "Epoch 122/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3255 - acc: 0.7206 - val_loss: 1.3374 - val_acc: 0.7170\n",
      "Epoch 123/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3206 - acc: 0.7210 - val_loss: 1.3347 - val_acc: 0.7170\n",
      "Epoch 124/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.3168 - acc: 0.7212 - val_loss: 1.3271 - val_acc: 0.7180\n",
      "Epoch 125/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.3119 - acc: 0.7203 - val_loss: 1.3265 - val_acc: 0.7170\n",
      "Epoch 126/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 1.3106 - acc: 0.721 - 0s 26us/step - loss: 1.3079 - acc: 0.7217 - val_loss: 1.3229 - val_acc: 0.7230\n",
      "Epoch 127/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.3034 - acc: 0.7218 - val_loss: 1.3154 - val_acc: 0.7190\n",
      "Epoch 128/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2994 - acc: 0.7222 - val_loss: 1.3106 - val_acc: 0.7180\n",
      "Epoch 129/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2955 - acc: 0.7222 - val_loss: 1.3049 - val_acc: 0.7190\n",
      "Epoch 130/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2910 - acc: 0.7221 - val_loss: 1.3031 - val_acc: 0.7110\n",
      "Epoch 131/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2871 - acc: 0.7224 - val_loss: 1.2986 - val_acc: 0.7170\n",
      "Epoch 132/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2833 - acc: 0.7222 - val_loss: 1.2976 - val_acc: 0.7140\n",
      "Epoch 133/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2790 - acc: 0.7235 - val_loss: 1.2895 - val_acc: 0.7180\n",
      "Epoch 134/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2754 - acc: 0.7237 - val_loss: 1.2891 - val_acc: 0.7250\n",
      "Epoch 135/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2713 - acc: 0.7246 - val_loss: 1.2866 - val_acc: 0.7230\n",
      "Epoch 136/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2680 - acc: 0.7250 - val_loss: 1.2811 - val_acc: 0.7230\n",
      "Epoch 137/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2633 - acc: 0.7246 - val_loss: 1.2763 - val_acc: 0.7240\n",
      "Epoch 138/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2600 - acc: 0.7254 - val_loss: 1.2728 - val_acc: 0.7210\n",
      "Epoch 139/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2560 - acc: 0.7240 - val_loss: 1.2716 - val_acc: 0.7210\n",
      "Epoch 140/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2528 - acc: 0.7255 - val_loss: 1.2650 - val_acc: 0.7200\n",
      "Epoch 141/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2493 - acc: 0.7262 - val_loss: 1.2611 - val_acc: 0.7220\n",
      "Epoch 142/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2457 - acc: 0.7267 - val_loss: 1.2577 - val_acc: 0.7210\n",
      "Epoch 143/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2419 - acc: 0.7269 - val_loss: 1.2569 - val_acc: 0.7270\n",
      "Epoch 144/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2386 - acc: 0.7274 - val_loss: 1.2530 - val_acc: 0.7270\n",
      "Epoch 145/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2349 - acc: 0.7283 - val_loss: 1.2531 - val_acc: 0.7260\n",
      "Epoch 146/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2319 - acc: 0.7283 - val_loss: 1.2509 - val_acc: 0.7250\n",
      "Epoch 147/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2291 - acc: 0.7267 - val_loss: 1.2428 - val_acc: 0.7270\n",
      "Epoch 148/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2252 - acc: 0.7285 - val_loss: 1.2424 - val_acc: 0.7250\n",
      "Epoch 149/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2222 - acc: 0.7279 - val_loss: 1.2375 - val_acc: 0.7280\n",
      "Epoch 150/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2194 - acc: 0.7290 - val_loss: 1.2332 - val_acc: 0.7230\n",
      "Epoch 151/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2161 - acc: 0.7301 - val_loss: 1.2333 - val_acc: 0.7240\n",
      "Epoch 152/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2126 - acc: 0.7310 - val_loss: 1.2339 - val_acc: 0.7210\n",
      "Epoch 153/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2098 - acc: 0.7290 - val_loss: 1.2255 - val_acc: 0.7250\n",
      "Epoch 154/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2068 - acc: 0.7313 - val_loss: 1.2212 - val_acc: 0.7250\n",
      "Epoch 155/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.2034 - acc: 0.7315 - val_loss: 1.2173 - val_acc: 0.7280\n",
      "Epoch 156/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.2002 - acc: 0.7323 - val_loss: 1.2257 - val_acc: 0.7210\n",
      "Epoch 157/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1982 - acc: 0.7308 - val_loss: 1.2140 - val_acc: 0.7270\n",
      "Epoch 158/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1950 - acc: 0.7322 - val_loss: 1.2124 - val_acc: 0.7210\n",
      "Epoch 159/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1923 - acc: 0.7326 - val_loss: 1.2091 - val_acc: 0.7240\n",
      "Epoch 160/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1892 - acc: 0.7328 - val_loss: 1.2068 - val_acc: 0.7220\n",
      "Epoch 161/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1868 - acc: 0.7324 - val_loss: 1.2044 - val_acc: 0.7240\n",
      "Epoch 162/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1844 - acc: 0.7342 - val_loss: 1.2009 - val_acc: 0.7260\n",
      "Epoch 163/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1818 - acc: 0.7326 - val_loss: 1.2005 - val_acc: 0.7290\n",
      "Epoch 164/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1790 - acc: 0.7342 - val_loss: 1.1935 - val_acc: 0.7270\n",
      "Epoch 165/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1766 - acc: 0.7341 - val_loss: 1.1934 - val_acc: 0.7260\n",
      "Epoch 166/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1736 - acc: 0.7351 - val_loss: 1.1919 - val_acc: 0.7310\n",
      "Epoch 167/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1715 - acc: 0.7350 - val_loss: 1.1938 - val_acc: 0.7280\n",
      "Epoch 168/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1686 - acc: 0.7364 - val_loss: 1.1865 - val_acc: 0.7260\n",
      "Epoch 169/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1669 - acc: 0.7344 - val_loss: 1.1895 - val_acc: 0.7310\n",
      "Epoch 170/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1645 - acc: 0.7368 - val_loss: 1.1796 - val_acc: 0.7260\n",
      "Epoch 171/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1623 - acc: 0.7359 - val_loss: 1.1780 - val_acc: 0.7280\n",
      "Epoch 172/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1604 - acc: 0.7349 - val_loss: 1.1763 - val_acc: 0.7320\n",
      "Epoch 173/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1577 - acc: 0.7369 - val_loss: 1.1772 - val_acc: 0.7220\n",
      "Epoch 174/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1556 - acc: 0.7376 - val_loss: 1.1822 - val_acc: 0.7270\n",
      "Epoch 175/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1533 - acc: 0.7386 - val_loss: 1.1711 - val_acc: 0.7280\n",
      "Epoch 176/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 1.1497 - acc: 0.737 - 0s 26us/step - loss: 1.1514 - acc: 0.7378 - val_loss: 1.1740 - val_acc: 0.7330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1497 - acc: 0.7383 - val_loss: 1.1667 - val_acc: 0.7290\n",
      "Epoch 178/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1478 - acc: 0.7373 - val_loss: 1.1651 - val_acc: 0.7350\n",
      "Epoch 179/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1456 - acc: 0.7385 - val_loss: 1.1622 - val_acc: 0.7320\n",
      "Epoch 180/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1432 - acc: 0.7382 - val_loss: 1.1616 - val_acc: 0.7290\n",
      "Epoch 181/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1421 - acc: 0.7392 - val_loss: 1.1597 - val_acc: 0.7280\n",
      "Epoch 182/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1397 - acc: 0.7395 - val_loss: 1.1587 - val_acc: 0.7320\n",
      "Epoch 183/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1379 - acc: 0.7406 - val_loss: 1.1557 - val_acc: 0.7270\n",
      "Epoch 184/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1359 - acc: 0.7379 - val_loss: 1.1542 - val_acc: 0.7310\n",
      "Epoch 185/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1338 - acc: 0.7404 - val_loss: 1.1522 - val_acc: 0.7300\n",
      "Epoch 186/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1322 - acc: 0.7414 - val_loss: 1.1546 - val_acc: 0.7310\n",
      "Epoch 187/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1309 - acc: 0.7401 - val_loss: 1.1519 - val_acc: 0.7380\n",
      "Epoch 188/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1291 - acc: 0.7405 - val_loss: 1.1470 - val_acc: 0.7340\n",
      "Epoch 189/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1268 - acc: 0.7436 - val_loss: 1.1461 - val_acc: 0.7350\n",
      "Epoch 190/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1252 - acc: 0.7406 - val_loss: 1.1442 - val_acc: 0.7270\n",
      "Epoch 191/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1240 - acc: 0.7409 - val_loss: 1.1440 - val_acc: 0.7280\n",
      "Epoch 192/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1224 - acc: 0.7424 - val_loss: 1.1414 - val_acc: 0.7340\n",
      "Epoch 193/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1201 - acc: 0.7412 - val_loss: 1.1418 - val_acc: 0.7350\n",
      "Epoch 194/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1183 - acc: 0.7412 - val_loss: 1.1416 - val_acc: 0.7370\n",
      "Epoch 195/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1169 - acc: 0.7437 - val_loss: 1.1381 - val_acc: 0.7360\n",
      "Epoch 196/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1152 - acc: 0.7438 - val_loss: 1.1348 - val_acc: 0.7360\n",
      "Epoch 197/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1141 - acc: 0.7414 - val_loss: 1.1336 - val_acc: 0.7350\n",
      "Epoch 198/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1127 - acc: 0.7445 - val_loss: 1.1310 - val_acc: 0.7320\n",
      "Epoch 199/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1106 - acc: 0.7435 - val_loss: 1.1317 - val_acc: 0.7320\n",
      "Epoch 200/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1094 - acc: 0.7442 - val_loss: 1.1295 - val_acc: 0.7330\n",
      "Epoch 201/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1071 - acc: 0.7453 - val_loss: 1.1336 - val_acc: 0.7380\n",
      "Epoch 202/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1055 - acc: 0.7442 - val_loss: 1.1272 - val_acc: 0.7390\n",
      "Epoch 203/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1041 - acc: 0.7437 - val_loss: 1.1248 - val_acc: 0.7360\n",
      "Epoch 204/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 1.1012 - acc: 0.746 - 0s 26us/step - loss: 1.1025 - acc: 0.7455 - val_loss: 1.1246 - val_acc: 0.7390\n",
      "Epoch 205/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.1006 - acc: 0.7450 - val_loss: 1.1281 - val_acc: 0.7400\n",
      "Epoch 206/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.1001 - acc: 0.7459 - val_loss: 1.1218 - val_acc: 0.7420\n",
      "Epoch 207/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0986 - acc: 0.7467 - val_loss: 1.1182 - val_acc: 0.7360\n",
      "Epoch 208/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0963 - acc: 0.7465 - val_loss: 1.1164 - val_acc: 0.7370\n",
      "Epoch 209/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0952 - acc: 0.7473 - val_loss: 1.1146 - val_acc: 0.7390\n",
      "Epoch 210/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0934 - acc: 0.7450 - val_loss: 1.1138 - val_acc: 0.7400\n",
      "Epoch 211/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0917 - acc: 0.7476 - val_loss: 1.1142 - val_acc: 0.7390\n",
      "Epoch 212/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0905 - acc: 0.7468 - val_loss: 1.1131 - val_acc: 0.7350\n",
      "Epoch 213/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0890 - acc: 0.7463 - val_loss: 1.1106 - val_acc: 0.7380\n",
      "Epoch 214/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0876 - acc: 0.7479 - val_loss: 1.1133 - val_acc: 0.7350\n",
      "Epoch 215/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0865 - acc: 0.7471 - val_loss: 1.1092 - val_acc: 0.7420\n",
      "Epoch 216/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0844 - acc: 0.7487 - val_loss: 1.1080 - val_acc: 0.7370\n",
      "Epoch 217/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0829 - acc: 0.7488 - val_loss: 1.1062 - val_acc: 0.7360\n",
      "Epoch 218/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0821 - acc: 0.7481 - val_loss: 1.1074 - val_acc: 0.7390\n",
      "Epoch 219/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0806 - acc: 0.7483 - val_loss: 1.1036 - val_acc: 0.7400\n",
      "Epoch 220/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0792 - acc: 0.7492 - val_loss: 1.1016 - val_acc: 0.7360\n",
      "Epoch 221/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0773 - acc: 0.7496 - val_loss: 1.1037 - val_acc: 0.7370\n",
      "Epoch 222/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0762 - acc: 0.7497 - val_loss: 1.1008 - val_acc: 0.7420\n",
      "Epoch 223/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0748 - acc: 0.7485 - val_loss: 1.0965 - val_acc: 0.7370\n",
      "Epoch 224/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0729 - acc: 0.7486 - val_loss: 1.0991 - val_acc: 0.7400\n",
      "Epoch 225/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0715 - acc: 0.7494 - val_loss: 1.0947 - val_acc: 0.7430\n",
      "Epoch 226/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0706 - acc: 0.7509 - val_loss: 1.0957 - val_acc: 0.7410\n",
      "Epoch 227/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0694 - acc: 0.7491 - val_loss: 1.0919 - val_acc: 0.7440\n",
      "Epoch 228/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0681 - acc: 0.7505 - val_loss: 1.0918 - val_acc: 0.7380\n",
      "Epoch 229/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0659 - acc: 0.7504 - val_loss: 1.0893 - val_acc: 0.7390\n",
      "Epoch 230/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0650 - acc: 0.7508 - val_loss: 1.0874 - val_acc: 0.7400\n",
      "Epoch 231/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0638 - acc: 0.7501 - val_loss: 1.0918 - val_acc: 0.7420\n",
      "Epoch 232/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0628 - acc: 0.7519 - val_loss: 1.0904 - val_acc: 0.7380\n",
      "Epoch 233/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0610 - acc: 0.7495 - val_loss: 1.0892 - val_acc: 0.7360\n",
      "Epoch 234/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0600 - acc: 0.7501 - val_loss: 1.0840 - val_acc: 0.7400\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0582 - acc: 0.7517 - val_loss: 1.0826 - val_acc: 0.7360\n",
      "Epoch 236/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0570 - acc: 0.7504 - val_loss: 1.0857 - val_acc: 0.7390\n",
      "Epoch 237/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0557 - acc: 0.7519 - val_loss: 1.0814 - val_acc: 0.7430\n",
      "Epoch 238/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0545 - acc: 0.7514 - val_loss: 1.0808 - val_acc: 0.7400\n",
      "Epoch 239/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0530 - acc: 0.7513 - val_loss: 1.0843 - val_acc: 0.7370\n",
      "Epoch 240/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0520 - acc: 0.7521 - val_loss: 1.0804 - val_acc: 0.7380\n",
      "Epoch 241/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0509 - acc: 0.7518 - val_loss: 1.0748 - val_acc: 0.7460\n",
      "Epoch 242/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 1.0519 - acc: 0.750 - 0s 26us/step - loss: 1.0490 - acc: 0.7531 - val_loss: 1.0749 - val_acc: 0.7450\n",
      "Epoch 243/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0481 - acc: 0.7523 - val_loss: 1.0743 - val_acc: 0.7470\n",
      "Epoch 244/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0472 - acc: 0.7540 - val_loss: 1.0713 - val_acc: 0.7470\n",
      "Epoch 245/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0461 - acc: 0.7544 - val_loss: 1.0706 - val_acc: 0.7420\n",
      "Epoch 246/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0445 - acc: 0.7537 - val_loss: 1.0722 - val_acc: 0.7390\n",
      "Epoch 247/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0430 - acc: 0.7553 - val_loss: 1.0767 - val_acc: 0.7390\n",
      "Epoch 248/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0425 - acc: 0.7527 - val_loss: 1.0706 - val_acc: 0.7410\n",
      "Epoch 249/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0409 - acc: 0.7535 - val_loss: 1.0655 - val_acc: 0.7480\n",
      "Epoch 250/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0398 - acc: 0.7532 - val_loss: 1.0660 - val_acc: 0.7420\n",
      "Epoch 251/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0387 - acc: 0.7536 - val_loss: 1.0709 - val_acc: 0.7390\n",
      "Epoch 252/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0373 - acc: 0.7521 - val_loss: 1.0715 - val_acc: 0.7370\n",
      "Epoch 253/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0365 - acc: 0.7538 - val_loss: 1.0635 - val_acc: 0.7470\n",
      "Epoch 254/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0358 - acc: 0.7555 - val_loss: 1.0634 - val_acc: 0.7460\n",
      "Epoch 255/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0343 - acc: 0.7544 - val_loss: 1.0628 - val_acc: 0.7430\n",
      "Epoch 256/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0332 - acc: 0.7567 - val_loss: 1.0596 - val_acc: 0.7490\n",
      "Epoch 257/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0322 - acc: 0.7555 - val_loss: 1.0602 - val_acc: 0.7460\n",
      "Epoch 258/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0309 - acc: 0.7558 - val_loss: 1.0572 - val_acc: 0.7420\n",
      "Epoch 259/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0301 - acc: 0.7553 - val_loss: 1.0599 - val_acc: 0.7480\n",
      "Epoch 260/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0294 - acc: 0.7563 - val_loss: 1.0557 - val_acc: 0.7460\n",
      "Epoch 261/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0281 - acc: 0.7558 - val_loss: 1.0538 - val_acc: 0.7440\n",
      "Epoch 262/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0274 - acc: 0.7554 - val_loss: 1.0564 - val_acc: 0.7450\n",
      "Epoch 263/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0260 - acc: 0.7573 - val_loss: 1.0568 - val_acc: 0.7440\n",
      "Epoch 264/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0249 - acc: 0.7568 - val_loss: 1.0518 - val_acc: 0.7470\n",
      "Epoch 265/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0241 - acc: 0.7564 - val_loss: 1.0513 - val_acc: 0.7500\n",
      "Epoch 266/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0232 - acc: 0.7574 - val_loss: 1.0529 - val_acc: 0.7430\n",
      "Epoch 267/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0215 - acc: 0.7550 - val_loss: 1.0496 - val_acc: 0.7530\n",
      "Epoch 268/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0209 - acc: 0.7564 - val_loss: 1.0534 - val_acc: 0.7450\n",
      "Epoch 269/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0203 - acc: 0.7579 - val_loss: 1.0487 - val_acc: 0.7500\n",
      "Epoch 270/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0186 - acc: 0.7553 - val_loss: 1.0473 - val_acc: 0.7450\n",
      "Epoch 271/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0174 - acc: 0.7551 - val_loss: 1.0480 - val_acc: 0.7490\n",
      "Epoch 272/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0165 - acc: 0.7583 - val_loss: 1.0470 - val_acc: 0.7470\n",
      "Epoch 273/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0156 - acc: 0.7562 - val_loss: 1.0453 - val_acc: 0.7490\n",
      "Epoch 274/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0150 - acc: 0.7576 - val_loss: 1.0439 - val_acc: 0.7490\n",
      "Epoch 275/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0144 - acc: 0.7595 - val_loss: 1.0439 - val_acc: 0.7450\n",
      "Epoch 276/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0127 - acc: 0.7582 - val_loss: 1.0429 - val_acc: 0.7480\n",
      "Epoch 277/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0120 - acc: 0.7581 - val_loss: 1.0422 - val_acc: 0.7490\n",
      "Epoch 278/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0105 - acc: 0.7590 - val_loss: 1.0409 - val_acc: 0.7470\n",
      "Epoch 279/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0101 - acc: 0.7583 - val_loss: 1.0408 - val_acc: 0.7550\n",
      "Epoch 280/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0092 - acc: 0.7601 - val_loss: 1.0481 - val_acc: 0.7500\n",
      "Epoch 281/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0087 - acc: 0.7579 - val_loss: 1.0385 - val_acc: 0.7490\n",
      "Epoch 282/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0075 - acc: 0.7568 - val_loss: 1.0398 - val_acc: 0.7480\n",
      "Epoch 283/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0065 - acc: 0.7574 - val_loss: 1.0397 - val_acc: 0.7450\n",
      "Epoch 284/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0060 - acc: 0.7599 - val_loss: 1.0409 - val_acc: 0.7410\n",
      "Epoch 285/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0046 - acc: 0.7585 - val_loss: 1.0383 - val_acc: 0.7430\n",
      "Epoch 286/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0038 - acc: 0.7583 - val_loss: 1.0334 - val_acc: 0.7530\n",
      "Epoch 287/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0027 - acc: 0.7601 - val_loss: 1.0354 - val_acc: 0.7490\n",
      "Epoch 288/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 1.0022 - acc: 0.7579 - val_loss: 1.0337 - val_acc: 0.7540\n",
      "Epoch 289/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0015 - acc: 0.7582 - val_loss: 1.0340 - val_acc: 0.7490\n",
      "Epoch 290/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 1.0002 - acc: 0.7591 - val_loss: 1.0303 - val_acc: 0.7520\n",
      "Epoch 291/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9996 - acc: 0.7610 - val_loss: 1.0313 - val_acc: 0.7540\n",
      "Epoch 292/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9985 - acc: 0.7605 - val_loss: 1.0307 - val_acc: 0.7540\n",
      "Epoch 293/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9979 - acc: 0.7599 - val_loss: 1.0297 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9977 - acc: 0.7609 - val_loss: 1.0303 - val_acc: 0.7550\n",
      "Epoch 295/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9962 - acc: 0.7624 - val_loss: 1.0269 - val_acc: 0.7550\n",
      "Epoch 296/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9958 - acc: 0.7600 - val_loss: 1.0263 - val_acc: 0.7560\n",
      "Epoch 297/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9950 - acc: 0.7608 - val_loss: 1.0274 - val_acc: 0.7550\n",
      "Epoch 298/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9942 - acc: 0.7588 - val_loss: 1.0264 - val_acc: 0.7580\n",
      "Epoch 299/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9932 - acc: 0.7610 - val_loss: 1.0242 - val_acc: 0.7520\n",
      "Epoch 300/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9931 - acc: 0.7608 - val_loss: 1.0235 - val_acc: 0.7510\n",
      "Epoch 301/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9911 - acc: 0.7619 - val_loss: 1.0325 - val_acc: 0.7450\n",
      "Epoch 302/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9907 - acc: 0.7606 - val_loss: 1.0332 - val_acc: 0.7440\n",
      "Epoch 303/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9902 - acc: 0.7619 - val_loss: 1.0248 - val_acc: 0.7570\n",
      "Epoch 304/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9891 - acc: 0.7626 - val_loss: 1.0227 - val_acc: 0.7540\n",
      "Epoch 305/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9882 - acc: 0.7647 - val_loss: 1.0245 - val_acc: 0.7530\n",
      "Epoch 306/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9880 - acc: 0.7612 - val_loss: 1.0215 - val_acc: 0.7600\n",
      "Epoch 307/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9872 - acc: 0.7629 - val_loss: 1.0256 - val_acc: 0.7540\n",
      "Epoch 308/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9860 - acc: 0.7640 - val_loss: 1.0259 - val_acc: 0.7440\n",
      "Epoch 309/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9850 - acc: 0.7619 - val_loss: 1.0273 - val_acc: 0.7440\n",
      "Epoch 310/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9850 - acc: 0.7637 - val_loss: 1.0195 - val_acc: 0.7560\n",
      "Epoch 311/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9830 - acc: 0.7632 - val_loss: 1.0264 - val_acc: 0.7470\n",
      "Epoch 312/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9828 - acc: 0.7624 - val_loss: 1.0199 - val_acc: 0.7510\n",
      "Epoch 313/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9827 - acc: 0.7621 - val_loss: 1.0210 - val_acc: 0.7490\n",
      "Epoch 314/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9817 - acc: 0.7622 - val_loss: 1.0163 - val_acc: 0.7580\n",
      "Epoch 315/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9809 - acc: 0.7649 - val_loss: 1.0202 - val_acc: 0.7550\n",
      "Epoch 316/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9804 - acc: 0.7641 - val_loss: 1.0171 - val_acc: 0.7500\n",
      "Epoch 317/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9793 - acc: 0.7655 - val_loss: 1.0173 - val_acc: 0.7530\n",
      "Epoch 318/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9785 - acc: 0.7644 - val_loss: 1.0188 - val_acc: 0.7500\n",
      "Epoch 319/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9780 - acc: 0.7629 - val_loss: 1.0200 - val_acc: 0.7490\n",
      "Epoch 320/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9781 - acc: 0.7650 - val_loss: 1.0147 - val_acc: 0.7530\n",
      "Epoch 321/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9764 - acc: 0.7651 - val_loss: 1.0151 - val_acc: 0.7490\n",
      "Epoch 322/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9756 - acc: 0.7642 - val_loss: 1.0142 - val_acc: 0.7560\n",
      "Epoch 323/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9750 - acc: 0.7646 - val_loss: 1.0113 - val_acc: 0.7570\n",
      "Epoch 324/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9749 - acc: 0.7637 - val_loss: 1.0115 - val_acc: 0.7560\n",
      "Epoch 325/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9737 - acc: 0.7654 - val_loss: 1.0118 - val_acc: 0.7480\n",
      "Epoch 326/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9731 - acc: 0.7665 - val_loss: 1.0117 - val_acc: 0.7550\n",
      "Epoch 327/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9723 - acc: 0.7654 - val_loss: 1.0136 - val_acc: 0.7480\n",
      "Epoch 328/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9707 - acc: 0.7654 - val_loss: 1.0090 - val_acc: 0.7610\n",
      "Epoch 329/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9711 - acc: 0.7647 - val_loss: 1.0077 - val_acc: 0.7590\n",
      "Epoch 330/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9702 - acc: 0.7655 - val_loss: 1.0120 - val_acc: 0.7520\n",
      "Epoch 331/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9703 - acc: 0.7671 - val_loss: 1.0176 - val_acc: 0.7450\n",
      "Epoch 332/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9691 - acc: 0.7662 - val_loss: 1.0099 - val_acc: 0.7500\n",
      "Epoch 333/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9687 - acc: 0.7668 - val_loss: 1.0113 - val_acc: 0.7490\n",
      "Epoch 334/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.9690 - acc: 0.766 - 0s 26us/step - loss: 0.9668 - acc: 0.7674 - val_loss: 1.0056 - val_acc: 0.7600\n",
      "Epoch 335/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9661 - acc: 0.7674 - val_loss: 1.0068 - val_acc: 0.7530\n",
      "Epoch 336/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9654 - acc: 0.7667 - val_loss: 1.0047 - val_acc: 0.7600\n",
      "Epoch 337/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9653 - acc: 0.7662 - val_loss: 1.0208 - val_acc: 0.7390\n",
      "Epoch 338/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9658 - acc: 0.7667 - val_loss: 1.0076 - val_acc: 0.7490\n",
      "Epoch 339/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9646 - acc: 0.7669 - val_loss: 1.0062 - val_acc: 0.7510\n",
      "Epoch 340/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9632 - acc: 0.7674 - val_loss: 1.0082 - val_acc: 0.7530\n",
      "Epoch 341/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9625 - acc: 0.7691 - val_loss: 1.0051 - val_acc: 0.7550\n",
      "Epoch 342/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9622 - acc: 0.7665 - val_loss: 1.0024 - val_acc: 0.7610\n",
      "Epoch 343/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9618 - acc: 0.7687 - val_loss: 1.0045 - val_acc: 0.7500\n",
      "Epoch 344/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9608 - acc: 0.7681 - val_loss: 1.0006 - val_acc: 0.7630\n",
      "Epoch 345/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9602 - acc: 0.7682 - val_loss: 1.0004 - val_acc: 0.7540\n",
      "Epoch 346/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9590 - acc: 0.7688 - val_loss: 0.9996 - val_acc: 0.7570\n",
      "Epoch 347/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9592 - acc: 0.7686 - val_loss: 0.9995 - val_acc: 0.7610\n",
      "Epoch 348/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9574 - acc: 0.7682 - val_loss: 1.0002 - val_acc: 0.7580\n",
      "Epoch 349/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9581 - acc: 0.7687 - val_loss: 0.9991 - val_acc: 0.7600\n",
      "Epoch 350/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9567 - acc: 0.7682 - val_loss: 1.0026 - val_acc: 0.7630\n",
      "Epoch 351/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9567 - acc: 0.7706 - val_loss: 1.0031 - val_acc: 0.7480\n",
      "Epoch 352/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9569 - acc: 0.7664 - val_loss: 0.9999 - val_acc: 0.7570\n",
      "Epoch 353/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9550 - acc: 0.7694 - val_loss: 0.9970 - val_acc: 0.7560\n",
      "Epoch 354/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9547 - acc: 0.7688 - val_loss: 0.9957 - val_acc: 0.7620\n",
      "Epoch 355/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9539 - acc: 0.7708 - val_loss: 0.9983 - val_acc: 0.7570\n",
      "Epoch 356/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9534 - acc: 0.7679 - val_loss: 0.9990 - val_acc: 0.7510\n",
      "Epoch 357/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9530 - acc: 0.7688 - val_loss: 0.9957 - val_acc: 0.7600\n",
      "Epoch 358/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9521 - acc: 0.7678 - val_loss: 0.9947 - val_acc: 0.7570\n",
      "Epoch 359/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9519 - acc: 0.7692 - val_loss: 0.9992 - val_acc: 0.7500\n",
      "Epoch 360/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9512 - acc: 0.7691 - val_loss: 0.9934 - val_acc: 0.7580\n",
      "Epoch 361/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9499 - acc: 0.7700 - val_loss: 0.9984 - val_acc: 0.7590\n",
      "Epoch 362/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9499 - acc: 0.7714 - val_loss: 0.9929 - val_acc: 0.7580\n",
      "Epoch 363/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9491 - acc: 0.7692 - val_loss: 0.9937 - val_acc: 0.7560\n",
      "Epoch 364/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9484 - acc: 0.7695 - val_loss: 0.9939 - val_acc: 0.7510\n",
      "Epoch 365/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9480 - acc: 0.7701 - val_loss: 0.9952 - val_acc: 0.7520\n",
      "Epoch 366/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9472 - acc: 0.7692 - val_loss: 0.9895 - val_acc: 0.7600\n",
      "Epoch 367/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9472 - acc: 0.7703 - val_loss: 0.9918 - val_acc: 0.7500\n",
      "Epoch 368/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9465 - acc: 0.7692 - val_loss: 0.9895 - val_acc: 0.7610\n",
      "Epoch 369/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9447 - acc: 0.7699 - val_loss: 0.9900 - val_acc: 0.7610\n",
      "Epoch 370/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9447 - acc: 0.7679 - val_loss: 0.9894 - val_acc: 0.7640\n",
      "Epoch 371/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9447 - acc: 0.7703 - val_loss: 0.9888 - val_acc: 0.7550\n",
      "Epoch 372/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9429 - acc: 0.7705 - val_loss: 0.9893 - val_acc: 0.7600\n",
      "Epoch 373/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9429 - acc: 0.7718 - val_loss: 0.9890 - val_acc: 0.7640\n",
      "Epoch 374/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9430 - acc: 0.7710 - val_loss: 0.9880 - val_acc: 0.7590\n",
      "Epoch 375/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9416 - acc: 0.7713 - val_loss: 0.9993 - val_acc: 0.7390\n",
      "Epoch 376/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9422 - acc: 0.7706 - val_loss: 0.9884 - val_acc: 0.7590\n",
      "Epoch 377/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9414 - acc: 0.7701 - val_loss: 0.9854 - val_acc: 0.7630\n",
      "Epoch 378/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9411 - acc: 0.7738 - val_loss: 0.9938 - val_acc: 0.7500\n",
      "Epoch 379/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9413 - acc: 0.7722 - val_loss: 0.9853 - val_acc: 0.7590\n",
      "Epoch 380/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9397 - acc: 0.7704 - val_loss: 0.9883 - val_acc: 0.7540\n",
      "Epoch 381/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9386 - acc: 0.7733 - val_loss: 0.9842 - val_acc: 0.7590\n",
      "Epoch 382/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9383 - acc: 0.7731 - val_loss: 0.9828 - val_acc: 0.7590\n",
      "Epoch 383/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9376 - acc: 0.7724 - val_loss: 0.9842 - val_acc: 0.7620\n",
      "Epoch 384/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9374 - acc: 0.7729 - val_loss: 0.9856 - val_acc: 0.7560\n",
      "Epoch 385/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9366 - acc: 0.7747 - val_loss: 0.9861 - val_acc: 0.7490\n",
      "Epoch 386/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9361 - acc: 0.7721 - val_loss: 0.9809 - val_acc: 0.7630\n",
      "Epoch 387/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9352 - acc: 0.7719 - val_loss: 0.9825 - val_acc: 0.7630\n",
      "Epoch 388/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9345 - acc: 0.7713 - val_loss: 0.9849 - val_acc: 0.7560\n",
      "Epoch 389/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9354 - acc: 0.7714 - val_loss: 0.9806 - val_acc: 0.7570\n",
      "Epoch 390/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9339 - acc: 0.7736 - val_loss: 0.9806 - val_acc: 0.7630\n",
      "Epoch 391/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9332 - acc: 0.7740 - val_loss: 0.9921 - val_acc: 0.7490\n",
      "Epoch 392/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9334 - acc: 0.7741 - val_loss: 0.9870 - val_acc: 0.7470\n",
      "Epoch 393/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9333 - acc: 0.7738 - val_loss: 0.9816 - val_acc: 0.7560\n",
      "Epoch 394/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9323 - acc: 0.7726 - val_loss: 0.9783 - val_acc: 0.7560\n",
      "Epoch 395/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9303 - acc: 0.7723 - val_loss: 0.9843 - val_acc: 0.7500\n",
      "Epoch 396/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9307 - acc: 0.7737 - val_loss: 0.9779 - val_acc: 0.7640\n",
      "Epoch 397/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9297 - acc: 0.7758 - val_loss: 0.9777 - val_acc: 0.7610\n",
      "Epoch 398/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9301 - acc: 0.7724 - val_loss: 0.9808 - val_acc: 0.7550\n",
      "Epoch 399/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9289 - acc: 0.7737 - val_loss: 0.9793 - val_acc: 0.7570\n",
      "Epoch 400/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9282 - acc: 0.7735 - val_loss: 0.9782 - val_acc: 0.7530\n",
      "Epoch 401/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9276 - acc: 0.7754 - val_loss: 0.9756 - val_acc: 0.7550\n",
      "Epoch 402/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9278 - acc: 0.7731 - val_loss: 0.9792 - val_acc: 0.7510\n",
      "Epoch 403/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9271 - acc: 0.7729 - val_loss: 0.9854 - val_acc: 0.7540\n",
      "Epoch 404/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9274 - acc: 0.7724 - val_loss: 0.9799 - val_acc: 0.7490\n",
      "Epoch 405/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9258 - acc: 0.7732 - val_loss: 0.9781 - val_acc: 0.7540\n",
      "Epoch 406/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9256 - acc: 0.7760 - val_loss: 0.9747 - val_acc: 0.7540\n",
      "Epoch 407/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9249 - acc: 0.7772 - val_loss: 0.9733 - val_acc: 0.7610\n",
      "Epoch 408/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.9261 - acc: 0.775 - 0s 26us/step - loss: 0.9242 - acc: 0.7746 - val_loss: 0.9739 - val_acc: 0.7570\n",
      "Epoch 409/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9237 - acc: 0.7768 - val_loss: 0.9789 - val_acc: 0.7510\n",
      "Epoch 410/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9235 - acc: 0.7729 - val_loss: 0.9743 - val_acc: 0.7570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 411/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9238 - acc: 0.7755 - val_loss: 0.9763 - val_acc: 0.7540\n",
      "Epoch 412/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9220 - acc: 0.7742 - val_loss: 0.9710 - val_acc: 0.7610\n",
      "Epoch 413/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9225 - acc: 0.7763 - val_loss: 0.9724 - val_acc: 0.7620\n",
      "Epoch 414/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9221 - acc: 0.7759 - val_loss: 0.9741 - val_acc: 0.7600\n",
      "Epoch 415/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9212 - acc: 0.7759 - val_loss: 0.9713 - val_acc: 0.7600\n",
      "Epoch 416/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9205 - acc: 0.7767 - val_loss: 0.9698 - val_acc: 0.7610\n",
      "Epoch 417/1000\n",
      "7800/7800 [==============================] - 0s 28us/step - loss: 0.9205 - acc: 0.7765 - val_loss: 0.9745 - val_acc: 0.7530\n",
      "Epoch 418/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9192 - acc: 0.7740 - val_loss: 0.9693 - val_acc: 0.7560\n",
      "Epoch 419/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.9179 - acc: 0.7760 - val_loss: 0.9687 - val_acc: 0.7580\n",
      "Epoch 420/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9187 - acc: 0.7765 - val_loss: 0.9760 - val_acc: 0.7530\n",
      "Epoch 421/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9178 - acc: 0.7771 - val_loss: 0.9715 - val_acc: 0.7530\n",
      "Epoch 422/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9186 - acc: 0.7760 - val_loss: 0.9767 - val_acc: 0.7500\n",
      "Epoch 423/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9176 - acc: 0.7751 - val_loss: 0.9710 - val_acc: 0.7560\n",
      "Epoch 424/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9169 - acc: 0.7756 - val_loss: 0.9706 - val_acc: 0.7500\n",
      "Epoch 425/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9161 - acc: 0.7747 - val_loss: 0.9713 - val_acc: 0.7490\n",
      "Epoch 426/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9166 - acc: 0.7749 - val_loss: 0.9685 - val_acc: 0.7530\n",
      "Epoch 427/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9153 - acc: 0.7781 - val_loss: 0.9678 - val_acc: 0.7600\n",
      "Epoch 428/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9155 - acc: 0.7765 - val_loss: 0.9680 - val_acc: 0.7530\n",
      "Epoch 429/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9143 - acc: 0.7762 - val_loss: 0.9705 - val_acc: 0.7530\n",
      "Epoch 430/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9140 - acc: 0.7746 - val_loss: 0.9660 - val_acc: 0.7580\n",
      "Epoch 431/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9140 - acc: 0.7763 - val_loss: 0.9646 - val_acc: 0.7630\n",
      "Epoch 432/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.9128 - acc: 0.7755 - val_loss: 0.9637 - val_acc: 0.7600\n",
      "Epoch 433/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9116 - acc: 0.7755 - val_loss: 0.9666 - val_acc: 0.7570\n",
      "Epoch 434/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9135 - acc: 0.7777 - val_loss: 0.9663 - val_acc: 0.7550\n",
      "Epoch 435/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9122 - acc: 0.7764 - val_loss: 0.9653 - val_acc: 0.7580\n",
      "Epoch 436/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9113 - acc: 0.7755 - val_loss: 0.9635 - val_acc: 0.7610\n",
      "Epoch 437/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9110 - acc: 0.7799 - val_loss: 0.9659 - val_acc: 0.7550\n",
      "Epoch 438/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9100 - acc: 0.7777 - val_loss: 0.9631 - val_acc: 0.7580\n",
      "Epoch 439/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9101 - acc: 0.7782 - val_loss: 0.9681 - val_acc: 0.7480\n",
      "Epoch 440/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9093 - acc: 0.7776 - val_loss: 0.9636 - val_acc: 0.7560\n",
      "Epoch 441/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9091 - acc: 0.7772 - val_loss: 0.9665 - val_acc: 0.7580\n",
      "Epoch 442/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9087 - acc: 0.7782 - val_loss: 0.9803 - val_acc: 0.7540\n",
      "Epoch 443/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9085 - acc: 0.7774 - val_loss: 0.9684 - val_acc: 0.7500\n",
      "Epoch 444/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9068 - acc: 0.7796 - val_loss: 0.9615 - val_acc: 0.7600\n",
      "Epoch 445/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9078 - acc: 0.7778 - val_loss: 0.9653 - val_acc: 0.7550\n",
      "Epoch 446/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.9027 - acc: 0.780 - 0s 26us/step - loss: 0.9065 - acc: 0.7797 - val_loss: 0.9651 - val_acc: 0.7510\n",
      "Epoch 447/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9060 - acc: 0.7776 - val_loss: 0.9658 - val_acc: 0.7460\n",
      "Epoch 448/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.9069 - acc: 0.778 - 0s 26us/step - loss: 0.9053 - acc: 0.7787 - val_loss: 0.9620 - val_acc: 0.7570\n",
      "Epoch 449/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9056 - acc: 0.7774 - val_loss: 0.9669 - val_acc: 0.7530\n",
      "Epoch 450/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9043 - acc: 0.7791 - val_loss: 0.9673 - val_acc: 0.7490\n",
      "Epoch 451/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9040 - acc: 0.7804 - val_loss: 0.9583 - val_acc: 0.7630\n",
      "Epoch 452/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9043 - acc: 0.7782 - val_loss: 0.9579 - val_acc: 0.7530\n",
      "Epoch 453/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9031 - acc: 0.7776 - val_loss: 0.9630 - val_acc: 0.7570\n",
      "Epoch 454/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9041 - acc: 0.7795 - val_loss: 0.9624 - val_acc: 0.7590\n",
      "Epoch 455/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9028 - acc: 0.7781 - val_loss: 0.9598 - val_acc: 0.7570\n",
      "Epoch 456/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9019 - acc: 0.7795 - val_loss: 0.9592 - val_acc: 0.7570\n",
      "Epoch 457/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9010 - acc: 0.7800 - val_loss: 0.9588 - val_acc: 0.7520\n",
      "Epoch 458/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.8988 - acc: 0.779 - 0s 26us/step - loss: 0.9010 - acc: 0.7777 - val_loss: 0.9775 - val_acc: 0.7440\n",
      "Epoch 459/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9008 - acc: 0.7795 - val_loss: 0.9612 - val_acc: 0.7570\n",
      "Epoch 460/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9000 - acc: 0.7795 - val_loss: 0.9608 - val_acc: 0.7520\n",
      "Epoch 461/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.9008 - acc: 0.7791 - val_loss: 0.9567 - val_acc: 0.7560\n",
      "Epoch 462/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.9005 - acc: 0.7801 - val_loss: 0.9570 - val_acc: 0.7590\n",
      "Epoch 463/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8992 - acc: 0.7790 - val_loss: 0.9563 - val_acc: 0.7540\n",
      "Epoch 464/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8986 - acc: 0.7787 - val_loss: 0.9611 - val_acc: 0.7510\n",
      "Epoch 465/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8993 - acc: 0.7787 - val_loss: 0.9579 - val_acc: 0.7530A: 0s - loss: 0.9038 - acc: 0.77\n",
      "Epoch 466/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8980 - acc: 0.7792 - val_loss: 0.9578 - val_acc: 0.7570\n",
      "Epoch 467/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8966 - acc: 0.7810 - val_loss: 0.9589 - val_acc: 0.7490\n",
      "Epoch 468/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8975 - acc: 0.7805 - val_loss: 0.9546 - val_acc: 0.7560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8966 - acc: 0.7791 - val_loss: 0.9539 - val_acc: 0.7580\n",
      "Epoch 470/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8956 - acc: 0.7808 - val_loss: 0.9601 - val_acc: 0.7460\n",
      "Epoch 471/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8959 - acc: 0.7801 - val_loss: 0.9531 - val_acc: 0.7580\n",
      "Epoch 472/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8949 - acc: 0.7818 - val_loss: 0.9535 - val_acc: 0.7580\n",
      "Epoch 473/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8946 - acc: 0.7799 - val_loss: 0.9565 - val_acc: 0.7510\n",
      "Epoch 474/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8945 - acc: 0.7797 - val_loss: 0.9630 - val_acc: 0.7500\n",
      "Epoch 475/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8944 - acc: 0.7818 - val_loss: 0.9509 - val_acc: 0.7630\n",
      "Epoch 476/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8932 - acc: 0.7810 - val_loss: 0.9538 - val_acc: 0.7570\n",
      "Epoch 477/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8932 - acc: 0.7797 - val_loss: 0.9532 - val_acc: 0.7600\n",
      "Epoch 478/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8933 - acc: 0.7800 - val_loss: 0.9485 - val_acc: 0.7590\n",
      "Epoch 479/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8919 - acc: 0.7801 - val_loss: 0.9548 - val_acc: 0.7530\n",
      "Epoch 480/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8921 - acc: 0.7822 - val_loss: 0.9535 - val_acc: 0.7530\n",
      "Epoch 481/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8921 - acc: 0.7806 - val_loss: 0.9510 - val_acc: 0.7550\n",
      "Epoch 482/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8921 - acc: 0.7823 - val_loss: 0.9528 - val_acc: 0.7560\n",
      "Epoch 483/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8902 - acc: 0.7824 - val_loss: 0.9492 - val_acc: 0.7530\n",
      "Epoch 484/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8903 - acc: 0.7790 - val_loss: 0.9492 - val_acc: 0.7580\n",
      "Epoch 485/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8904 - acc: 0.7813 - val_loss: 0.9543 - val_acc: 0.7570\n",
      "Epoch 486/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8909 - acc: 0.7810 - val_loss: 0.9474 - val_acc: 0.7620\n",
      "Epoch 487/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8889 - acc: 0.7795 - val_loss: 0.9539 - val_acc: 0.7450\n",
      "Epoch 488/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8888 - acc: 0.7809 - val_loss: 0.9485 - val_acc: 0.7600\n",
      "Epoch 489/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8899 - acc: 0.7806 - val_loss: 0.9523 - val_acc: 0.7470\n",
      "Epoch 490/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8885 - acc: 0.7812 - val_loss: 0.9488 - val_acc: 0.7600\n",
      "Epoch 491/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8880 - acc: 0.7818 - val_loss: 0.9485 - val_acc: 0.7560\n",
      "Epoch 492/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8873 - acc: 0.7828 - val_loss: 0.9483 - val_acc: 0.7560\n",
      "Epoch 493/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8881 - acc: 0.7828 - val_loss: 0.9463 - val_acc: 0.7630\n",
      "Epoch 494/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8867 - acc: 0.7815 - val_loss: 0.9450 - val_acc: 0.7650\n",
      "Epoch 495/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8866 - acc: 0.7826 - val_loss: 0.9475 - val_acc: 0.7580\n",
      "Epoch 496/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8866 - acc: 0.7813 - val_loss: 0.9455 - val_acc: 0.7540\n",
      "Epoch 497/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8860 - acc: 0.7819 - val_loss: 0.9460 - val_acc: 0.7610\n",
      "Epoch 498/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8858 - acc: 0.7803 - val_loss: 0.9521 - val_acc: 0.7510\n",
      "Epoch 499/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8847 - acc: 0.7826 - val_loss: 0.9449 - val_acc: 0.7580\n",
      "Epoch 500/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8847 - acc: 0.7801 - val_loss: 0.9478 - val_acc: 0.7580\n",
      "Epoch 501/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8858 - acc: 0.7810 - val_loss: 0.9452 - val_acc: 0.7580\n",
      "Epoch 502/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8847 - acc: 0.7835 - val_loss: 0.9574 - val_acc: 0.7520\n",
      "Epoch 503/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.8870 - acc: 0.780 - 0s 26us/step - loss: 0.8837 - acc: 0.7822 - val_loss: 0.9463 - val_acc: 0.7560\n",
      "Epoch 504/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8836 - acc: 0.7813 - val_loss: 0.9450 - val_acc: 0.7550\n",
      "Epoch 505/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8849 - acc: 0.7808 - val_loss: 0.9504 - val_acc: 0.7530\n",
      "Epoch 506/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8829 - acc: 0.7833 - val_loss: 0.9419 - val_acc: 0.7630\n",
      "Epoch 507/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8820 - acc: 0.7814 - val_loss: 0.9472 - val_acc: 0.7560\n",
      "Epoch 508/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8831 - acc: 0.7812 - val_loss: 0.9488 - val_acc: 0.7520\n",
      "Epoch 509/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8828 - acc: 0.7833 - val_loss: 0.9422 - val_acc: 0.7610\n",
      "Epoch 510/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.8783 - acc: 0.783 - 0s 26us/step - loss: 0.8820 - acc: 0.7822 - val_loss: 0.9424 - val_acc: 0.7620\n",
      "Epoch 511/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8810 - acc: 0.7831 - val_loss: 0.9407 - val_acc: 0.7580\n",
      "Epoch 512/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8813 - acc: 0.7815 - val_loss: 0.9411 - val_acc: 0.7550\n",
      "Epoch 513/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8802 - acc: 0.7827 - val_loss: 0.9423 - val_acc: 0.7560\n",
      "Epoch 514/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.8807 - acc: 0.7814 - val_loss: 0.9412 - val_acc: 0.7570\n",
      "Epoch 515/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8784 - acc: 0.7822 - val_loss: 0.9413 - val_acc: 0.7600\n",
      "Epoch 516/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8796 - acc: 0.7838 - val_loss: 0.9420 - val_acc: 0.7590\n",
      "Epoch 517/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8803 - acc: 0.7818 - val_loss: 0.9415 - val_acc: 0.7580\n",
      "Epoch 518/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8782 - acc: 0.7805 - val_loss: 0.9571 - val_acc: 0.7430\n",
      "Epoch 519/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8793 - acc: 0.7813 - val_loss: 0.9396 - val_acc: 0.7600\n",
      "Epoch 520/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8797 - acc: 0.7819 - val_loss: 0.9407 - val_acc: 0.7600\n",
      "Epoch 521/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8794 - acc: 0.7821 - val_loss: 0.9414 - val_acc: 0.7620\n",
      "Epoch 522/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8776 - acc: 0.7827 - val_loss: 0.9479 - val_acc: 0.7470\n",
      "Epoch 523/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8766 - acc: 0.7810 - val_loss: 0.9397 - val_acc: 0.7560\n",
      "Epoch 524/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8776 - acc: 0.7836 - val_loss: 0.9486 - val_acc: 0.7590\n",
      "Epoch 525/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8773 - acc: 0.7827 - val_loss: 0.9451 - val_acc: 0.7570\n",
      "Epoch 526/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8767 - acc: 0.7818 - val_loss: 0.9414 - val_acc: 0.7540\n",
      "Epoch 527/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8755 - acc: 0.7837 - val_loss: 0.9416 - val_acc: 0.7600\n",
      "Epoch 528/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8766 - acc: 0.7849 - val_loss: 0.9427 - val_acc: 0.7590\n",
      "Epoch 529/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.8752 - acc: 0.784 - 0s 26us/step - loss: 0.8752 - acc: 0.7841 - val_loss: 0.9411 - val_acc: 0.7510\n",
      "Epoch 530/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8741 - acc: 0.7850 - val_loss: 0.9376 - val_acc: 0.7600\n",
      "Epoch 531/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8751 - acc: 0.7833 - val_loss: 0.9437 - val_acc: 0.7530\n",
      "Epoch 532/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8749 - acc: 0.7826 - val_loss: 0.9413 - val_acc: 0.7580\n",
      "Epoch 533/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8737 - acc: 0.7845 - val_loss: 0.9456 - val_acc: 0.7510\n",
      "Epoch 534/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8739 - acc: 0.7838 - val_loss: 0.9373 - val_acc: 0.7620\n",
      "Epoch 535/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8739 - acc: 0.7828 - val_loss: 0.9391 - val_acc: 0.7620\n",
      "Epoch 536/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8726 - acc: 0.7831 - val_loss: 0.9423 - val_acc: 0.7560\n",
      "Epoch 537/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8729 - acc: 0.7829 - val_loss: 0.9498 - val_acc: 0.7580\n",
      "Epoch 538/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8721 - acc: 0.7831 - val_loss: 0.9420 - val_acc: 0.7560\n",
      "Epoch 539/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8712 - acc: 0.7858 - val_loss: 0.9376 - val_acc: 0.7570\n",
      "Epoch 540/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8713 - acc: 0.7827 - val_loss: 0.9500 - val_acc: 0.7510\n",
      "Epoch 541/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8715 - acc: 0.7841 - val_loss: 0.9383 - val_acc: 0.7590\n",
      "Epoch 542/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8713 - acc: 0.7845 - val_loss: 0.9390 - val_acc: 0.7540\n",
      "Epoch 543/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8717 - acc: 0.7831 - val_loss: 0.9365 - val_acc: 0.7600\n",
      "Epoch 544/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8714 - acc: 0.7844 - val_loss: 0.9343 - val_acc: 0.7630\n",
      "Epoch 545/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8702 - acc: 0.7865 - val_loss: 0.9390 - val_acc: 0.7570\n",
      "Epoch 546/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8693 - acc: 0.7853 - val_loss: 0.9357 - val_acc: 0.7620\n",
      "Epoch 547/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8700 - acc: 0.7824 - val_loss: 0.9517 - val_acc: 0.7460\n",
      "Epoch 548/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8691 - acc: 0.7859 - val_loss: 0.9364 - val_acc: 0.7530\n",
      "Epoch 549/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8689 - acc: 0.7850 - val_loss: 0.9350 - val_acc: 0.7630\n",
      "Epoch 550/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8687 - acc: 0.7855 - val_loss: 0.9379 - val_acc: 0.7590\n",
      "Epoch 551/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8686 - acc: 0.7837 - val_loss: 0.9342 - val_acc: 0.7610\n",
      "Epoch 552/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8685 - acc: 0.7828 - val_loss: 0.9330 - val_acc: 0.7620\n",
      "Epoch 553/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8686 - acc: 0.7831 - val_loss: 0.9385 - val_acc: 0.7570\n",
      "Epoch 554/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8666 - acc: 0.7853 - val_loss: 0.9315 - val_acc: 0.7620\n",
      "Epoch 555/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8681 - acc: 0.7849 - val_loss: 0.9309 - val_acc: 0.7610\n",
      "Epoch 556/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8670 - acc: 0.7860 - val_loss: 0.9378 - val_acc: 0.7590\n",
      "Epoch 557/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8683 - acc: 0.7856 - val_loss: 0.9430 - val_acc: 0.7520\n",
      "Epoch 558/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8670 - acc: 0.7853 - val_loss: 0.9393 - val_acc: 0.7570\n",
      "Epoch 559/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8658 - acc: 0.7854 - val_loss: 0.9306 - val_acc: 0.7620\n",
      "Epoch 560/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8663 - acc: 0.7836 - val_loss: 0.9313 - val_acc: 0.7640\n",
      "Epoch 561/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8663 - acc: 0.7860 - val_loss: 0.9377 - val_acc: 0.7550\n",
      "Epoch 562/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8665 - acc: 0.7860 - val_loss: 0.9312 - val_acc: 0.7640\n",
      "Epoch 563/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8647 - acc: 0.7853 - val_loss: 0.9320 - val_acc: 0.7600\n",
      "Epoch 564/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8638 - acc: 0.7882 - val_loss: 0.9347 - val_acc: 0.7560\n",
      "Epoch 565/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8637 - acc: 0.7867 - val_loss: 0.9345 - val_acc: 0.7500\n",
      "Epoch 566/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.8662 - acc: 0.786 - ETA: 0s - loss: 0.8633 - acc: 0.788 - 0s 26us/step - loss: 0.8637 - acc: 0.7881 - val_loss: 0.9298 - val_acc: 0.7630\n",
      "Epoch 567/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8641 - acc: 0.7850 - val_loss: 0.9309 - val_acc: 0.7600\n",
      "Epoch 568/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8633 - acc: 0.7867 - val_loss: 0.9357 - val_acc: 0.7540\n",
      "Epoch 569/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8633 - acc: 0.7882 - val_loss: 0.9315 - val_acc: 0.7580\n",
      "Epoch 570/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8620 - acc: 0.7874 - val_loss: 0.9374 - val_acc: 0.7550\n",
      "Epoch 571/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8632 - acc: 0.7863 - val_loss: 0.9326 - val_acc: 0.7560\n",
      "Epoch 572/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8647 - acc: 0.7862 - val_loss: 0.9364 - val_acc: 0.7560\n",
      "Epoch 573/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8628 - acc: 0.7872 - val_loss: 0.9293 - val_acc: 0.7610\n",
      "Epoch 574/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8634 - acc: 0.7863 - val_loss: 0.9304 - val_acc: 0.7620\n",
      "Epoch 575/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8617 - acc: 0.7864 - val_loss: 0.9435 - val_acc: 0.7450\n",
      "Epoch 576/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8601 - acc: 0.7882 - val_loss: 0.9305 - val_acc: 0.7640\n",
      "Epoch 577/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8609 - acc: 0.7859 - val_loss: 0.9287 - val_acc: 0.7600\n",
      "Epoch 578/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8595 - acc: 0.7894 - val_loss: 0.9325 - val_acc: 0.7550\n",
      "Epoch 579/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8602 - acc: 0.7860 - val_loss: 0.9284 - val_acc: 0.7620\n",
      "Epoch 580/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8598 - acc: 0.7855 - val_loss: 0.9282 - val_acc: 0.7650\n",
      "Epoch 581/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.8601 - acc: 0.7886 - val_loss: 0.9431 - val_acc: 0.7540\n",
      "Epoch 582/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8602 - acc: 0.7864 - val_loss: 0.9314 - val_acc: 0.7460\n",
      "Epoch 583/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8594 - acc: 0.7881 - val_loss: 0.9313 - val_acc: 0.7500\n",
      "Epoch 584/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8583 - acc: 0.7873 - val_loss: 0.9282 - val_acc: 0.7570\n",
      "Epoch 585/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8581 - acc: 0.7879 - val_loss: 0.9433 - val_acc: 0.7500\n",
      "Epoch 586/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8581 - acc: 0.7854 - val_loss: 0.9295 - val_acc: 0.7560\n",
      "Epoch 587/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8581 - acc: 0.7883 - val_loss: 0.9287 - val_acc: 0.7590\n",
      "Epoch 588/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8584 - acc: 0.7846 - val_loss: 0.9358 - val_acc: 0.7480\n",
      "Epoch 589/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8577 - acc: 0.7869 - val_loss: 0.9296 - val_acc: 0.7590\n",
      "Epoch 590/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8577 - acc: 0.7882 - val_loss: 0.9274 - val_acc: 0.7560\n",
      "Epoch 591/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8579 - acc: 0.7882 - val_loss: 0.9242 - val_acc: 0.7600\n",
      "Epoch 592/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8571 - acc: 0.7865 - val_loss: 0.9315 - val_acc: 0.7610\n",
      "Epoch 593/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8580 - acc: 0.7900 - val_loss: 0.9311 - val_acc: 0.7530\n",
      "Epoch 594/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8558 - acc: 0.7877 - val_loss: 0.9299 - val_acc: 0.7490\n",
      "Epoch 595/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8550 - acc: 0.7863 - val_loss: 0.9282 - val_acc: 0.7580\n",
      "Epoch 596/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8557 - acc: 0.7885 - val_loss: 0.9297 - val_acc: 0.7600\n",
      "Epoch 597/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8538 - acc: 0.7871 - val_loss: 0.9258 - val_acc: 0.7650\n",
      "Epoch 598/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8536 - acc: 0.7874 - val_loss: 0.9327 - val_acc: 0.7530\n",
      "Epoch 599/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8544 - acc: 0.7831 - val_loss: 0.9238 - val_acc: 0.7640\n",
      "Epoch 600/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8539 - acc: 0.7885 - val_loss: 0.9252 - val_acc: 0.7620\n",
      "Epoch 601/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8542 - acc: 0.7879 - val_loss: 0.9305 - val_acc: 0.7540\n",
      "Epoch 602/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8540 - acc: 0.7859 - val_loss: 0.9335 - val_acc: 0.7530\n",
      "Epoch 603/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8534 - acc: 0.7873 - val_loss: 0.9223 - val_acc: 0.7670\n",
      "Epoch 604/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8540 - acc: 0.7872 - val_loss: 0.9262 - val_acc: 0.7500\n",
      "Epoch 605/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8524 - acc: 0.7891 - val_loss: 0.9259 - val_acc: 0.7610\n",
      "Epoch 606/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8531 - acc: 0.7877 - val_loss: 0.9363 - val_acc: 0.7460\n",
      "Epoch 607/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8526 - acc: 0.7877 - val_loss: 0.9231 - val_acc: 0.7620\n",
      "Epoch 608/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8522 - acc: 0.7878 - val_loss: 0.9209 - val_acc: 0.7650\n",
      "Epoch 609/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8508 - acc: 0.7882 - val_loss: 0.9226 - val_acc: 0.7630\n",
      "Epoch 610/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8523 - acc: 0.7872 - val_loss: 0.9241 - val_acc: 0.7600\n",
      "Epoch 611/1000\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8512 - acc: 0.7874 - val_loss: 0.9227 - val_acc: 0.7610\n",
      "Epoch 612/1000\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8507 - acc: 0.7899 - val_loss: 0.9288 - val_acc: 0.7580\n",
      "Epoch 613/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8512 - acc: 0.7895 - val_loss: 0.9283 - val_acc: 0.7630\n",
      "Epoch 614/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8506 - acc: 0.7895 - val_loss: 0.9250 - val_acc: 0.7590\n",
      "Epoch 615/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8517 - acc: 0.7865 - val_loss: 0.9225 - val_acc: 0.7660\n",
      "Epoch 616/1000\n",
      "7800/7800 [==============================] - 0s 22us/step - loss: 0.8497 - acc: 0.7887 - val_loss: 0.9226 - val_acc: 0.7600\n",
      "Epoch 617/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8494 - acc: 0.7892 - val_loss: 0.9323 - val_acc: 0.7530\n",
      "Epoch 618/1000\n",
      "7800/7800 [==============================] - 0s 22us/step - loss: 0.8492 - acc: 0.7895 - val_loss: 0.9395 - val_acc: 0.7510\n",
      "Epoch 619/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8488 - acc: 0.7899 - val_loss: 0.9261 - val_acc: 0.7610\n",
      "Epoch 620/1000\n",
      "7800/7800 [==============================] - 0s 22us/step - loss: 0.8491 - acc: 0.7899 - val_loss: 0.9227 - val_acc: 0.7550\n",
      "Epoch 621/1000\n",
      "7800/7800 [==============================] - 0s 22us/step - loss: 0.8492 - acc: 0.7887 - val_loss: 0.9236 - val_acc: 0.7570\n",
      "Epoch 622/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8500 - acc: 0.7888 - val_loss: 0.9221 - val_acc: 0.7590\n",
      "Epoch 623/1000\n",
      "7800/7800 [==============================] - 0s 22us/step - loss: 0.8484 - acc: 0.7871 - val_loss: 0.9208 - val_acc: 0.7690\n",
      "Epoch 624/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8484 - acc: 0.7892 - val_loss: 0.9373 - val_acc: 0.7500\n",
      "Epoch 625/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8488 - acc: 0.7874 - val_loss: 0.9210 - val_acc: 0.7590\n",
      "Epoch 626/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8464 - acc: 0.7883 - val_loss: 0.9297 - val_acc: 0.7490\n",
      "Epoch 627/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8464 - acc: 0.7910 - val_loss: 0.9230 - val_acc: 0.7630\n",
      "Epoch 628/1000\n",
      "7800/7800 [==============================] - 0s 22us/step - loss: 0.8487 - acc: 0.7896 - val_loss: 0.9218 - val_acc: 0.7600\n",
      "Epoch 629/1000\n",
      "7800/7800 [==============================] - 0s 22us/step - loss: 0.8469 - acc: 0.7897 - val_loss: 0.9243 - val_acc: 0.7550\n",
      "Epoch 630/1000\n",
      "7800/7800 [==============================] - 0s 22us/step - loss: 0.8465 - acc: 0.7894 - val_loss: 0.9179 - val_acc: 0.7650\n",
      "Epoch 631/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8449 - acc: 0.7929 - val_loss: 0.9221 - val_acc: 0.7530\n",
      "Epoch 632/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8456 - acc: 0.7883 - val_loss: 0.9397 - val_acc: 0.7510\n",
      "Epoch 633/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8456 - acc: 0.7909 - val_loss: 0.9167 - val_acc: 0.7700\n",
      "Epoch 634/1000\n",
      "7800/7800 [==============================] - 0s 22us/step - loss: 0.8445 - acc: 0.7909 - val_loss: 0.9194 - val_acc: 0.7540\n",
      "Epoch 635/1000\n",
      "7800/7800 [==============================] - 0s 23us/step - loss: 0.8447 - acc: 0.7903 - val_loss: 0.9273 - val_acc: 0.7510\n",
      "Epoch 636/1000\n",
      "7800/7800 [==============================] - 0s 24us/step - loss: 0.8448 - acc: 0.7904 - val_loss: 0.9237 - val_acc: 0.7510\n",
      "Epoch 637/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8456 - acc: 0.7892 - val_loss: 0.9202 - val_acc: 0.7620\n",
      "Epoch 638/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8449 - acc: 0.7921 - val_loss: 0.9306 - val_acc: 0.7540\n",
      "Epoch 639/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8452 - acc: 0.7913 - val_loss: 0.9238 - val_acc: 0.7580\n",
      "Epoch 640/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8459 - acc: 0.7886 - val_loss: 0.9168 - val_acc: 0.7640\n",
      "Epoch 641/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8433 - acc: 0.7931 - val_loss: 0.9283 - val_acc: 0.7560\n",
      "Epoch 642/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8432 - acc: 0.7912 - val_loss: 0.9152 - val_acc: 0.7690\n",
      "Epoch 643/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8429 - acc: 0.7905 - val_loss: 0.9173 - val_acc: 0.7660\n",
      "Epoch 644/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8431 - acc: 0.7897 - val_loss: 0.9172 - val_acc: 0.7620\n",
      "Epoch 645/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8427 - acc: 0.7897 - val_loss: 0.9164 - val_acc: 0.7600\n",
      "Epoch 646/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8418 - acc: 0.7913 - val_loss: 0.9273 - val_acc: 0.7510\n",
      "Epoch 647/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8443 - acc: 0.7908 - val_loss: 0.9172 - val_acc: 0.7590\n",
      "Epoch 648/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8435 - acc: 0.7922 - val_loss: 0.9180 - val_acc: 0.7510\n",
      "Epoch 649/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8422 - acc: 0.7909 - val_loss: 0.9235 - val_acc: 0.7510\n",
      "Epoch 650/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8425 - acc: 0.7904 - val_loss: 0.9346 - val_acc: 0.7450\n",
      "Epoch 651/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8434 - acc: 0.7885 - val_loss: 0.9183 - val_acc: 0.7620\n",
      "Epoch 652/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8411 - acc: 0.7926 - val_loss: 0.9185 - val_acc: 0.7580\n",
      "Epoch 653/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8409 - acc: 0.7895 - val_loss: 0.9195 - val_acc: 0.7640\n",
      "Epoch 654/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8394 - acc: 0.7904 - val_loss: 0.9188 - val_acc: 0.7590\n",
      "Epoch 655/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8420 - acc: 0.7921 - val_loss: 0.9158 - val_acc: 0.7600\n",
      "Epoch 656/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8388 - acc: 0.7918 - val_loss: 0.9160 - val_acc: 0.7560\n",
      "Epoch 657/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8402 - acc: 0.7885 - val_loss: 0.9222 - val_acc: 0.7520\n",
      "Epoch 658/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8406 - acc: 0.7899 - val_loss: 0.9144 - val_acc: 0.7610\n",
      "Epoch 659/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8398 - acc: 0.7915 - val_loss: 0.9168 - val_acc: 0.7570\n",
      "Epoch 660/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8396 - acc: 0.7900 - val_loss: 0.9151 - val_acc: 0.7560\n",
      "Epoch 661/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8394 - acc: 0.7937 - val_loss: 0.9177 - val_acc: 0.7540\n",
      "Epoch 662/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8376 - acc: 0.7928 - val_loss: 0.9200 - val_acc: 0.7550\n",
      "Epoch 663/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8390 - acc: 0.7886 - val_loss: 0.9166 - val_acc: 0.7530\n",
      "Epoch 664/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8389 - acc: 0.7932 - val_loss: 0.9139 - val_acc: 0.7600\n",
      "Epoch 665/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8379 - acc: 0.7927 - val_loss: 0.9214 - val_acc: 0.7530\n",
      "Epoch 666/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8369 - acc: 0.7923 - val_loss: 0.9190 - val_acc: 0.7540\n",
      "Epoch 667/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8368 - acc: 0.7921 - val_loss: 0.9160 - val_acc: 0.7640\n",
      "Epoch 668/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8376 - acc: 0.7937 - val_loss: 0.9322 - val_acc: 0.7510\n",
      "Epoch 669/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8366 - acc: 0.7906 - val_loss: 0.9116 - val_acc: 0.7650\n",
      "Epoch 670/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8364 - acc: 0.7910 - val_loss: 0.9182 - val_acc: 0.7590\n",
      "Epoch 671/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8371 - acc: 0.7905 - val_loss: 0.9149 - val_acc: 0.7610\n",
      "Epoch 672/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8352 - acc: 0.7909 - val_loss: 0.9094 - val_acc: 0.7600\n",
      "Epoch 673/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8358 - acc: 0.7897 - val_loss: 0.9196 - val_acc: 0.7570\n",
      "Epoch 674/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8362 - acc: 0.7900 - val_loss: 0.9124 - val_acc: 0.7650\n",
      "Epoch 675/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8380 - acc: 0.7912 - val_loss: 0.9216 - val_acc: 0.7480\n",
      "Epoch 676/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8346 - acc: 0.7929 - val_loss: 0.9121 - val_acc: 0.7650\n",
      "Epoch 677/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8339 - acc: 0.7921 - val_loss: 0.9133 - val_acc: 0.7580\n",
      "Epoch 678/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8335 - acc: 0.7935 - val_loss: 0.9152 - val_acc: 0.7580\n",
      "Epoch 679/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8343 - acc: 0.7903 - val_loss: 0.9154 - val_acc: 0.7650\n",
      "Epoch 680/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8343 - acc: 0.7914 - val_loss: 0.9104 - val_acc: 0.7670\n",
      "Epoch 681/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8370 - acc: 0.7906 - val_loss: 0.9088 - val_acc: 0.7650\n",
      "Epoch 682/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8345 - acc: 0.7904 - val_loss: 0.9231 - val_acc: 0.7620\n",
      "Epoch 683/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8361 - acc: 0.7922 - val_loss: 0.9163 - val_acc: 0.7600\n",
      "Epoch 684/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8345 - acc: 0.7921 - val_loss: 0.9198 - val_acc: 0.7510\n",
      "Epoch 685/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8324 - acc: 0.7927 - val_loss: 0.9085 - val_acc: 0.7600\n",
      "Epoch 686/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8327 - acc: 0.7931 - val_loss: 0.9083 - val_acc: 0.7630\n",
      "Epoch 687/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8323 - acc: 0.7928 - val_loss: 0.9121 - val_acc: 0.7610\n",
      "Epoch 688/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8349 - acc: 0.7910 - val_loss: 0.9084 - val_acc: 0.7590\n",
      "Epoch 689/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8313 - acc: 0.7929 - val_loss: 0.9150 - val_acc: 0.7600\n",
      "Epoch 690/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8321 - acc: 0.7926 - val_loss: 0.9183 - val_acc: 0.7520\n",
      "Epoch 691/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8314 - acc: 0.7919 - val_loss: 0.9145 - val_acc: 0.7560\n",
      "Epoch 692/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8337 - acc: 0.7913 - val_loss: 0.9119 - val_acc: 0.7620\n",
      "Epoch 693/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8313 - acc: 0.7932 - val_loss: 0.9182 - val_acc: 0.7540\n",
      "Epoch 694/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8317 - acc: 0.7922 - val_loss: 0.9098 - val_acc: 0.7540\n",
      "Epoch 695/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8316 - acc: 0.7937 - val_loss: 0.9085 - val_acc: 0.7670\n",
      "Epoch 696/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8319 - acc: 0.7922 - val_loss: 0.9182 - val_acc: 0.7580\n",
      "Epoch 697/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8306 - acc: 0.7947 - val_loss: 0.9242 - val_acc: 0.7530\n",
      "Epoch 698/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8314 - acc: 0.7935 - val_loss: 0.9101 - val_acc: 0.7560\n",
      "Epoch 699/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8285 - acc: 0.7938 - val_loss: 0.9345 - val_acc: 0.7510\n",
      "Epoch 700/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8325 - acc: 0.7935 - val_loss: 0.9057 - val_acc: 0.7590\n",
      "Epoch 701/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8303 - acc: 0.7929 - val_loss: 0.9222 - val_acc: 0.7570\n",
      "Epoch 702/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.8294 - acc: 0.792 - 0s 26us/step - loss: 0.8298 - acc: 0.7922 - val_loss: 0.9129 - val_acc: 0.7600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 703/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.8279 - acc: 0.7939- ETA: 0s - loss: 0.8283 - acc: 0.7 - 0s 26us/step - loss: 0.8279 - acc: 0.7947 - val_loss: 0.9122 - val_acc: 0.7510\n",
      "Epoch 704/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8275 - acc: 0.7951 - val_loss: 0.9087 - val_acc: 0.7580\n",
      "Epoch 705/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8279 - acc: 0.7951 - val_loss: 0.9144 - val_acc: 0.7580\n",
      "Epoch 706/1000\n",
      "7800/7800 [==============================] - 0s 28us/step - loss: 0.8301 - acc: 0.7917 - val_loss: 0.9100 - val_acc: 0.7570\n",
      "Epoch 707/1000\n",
      "7800/7800 [==============================] - 0s 30us/step - loss: 0.8275 - acc: 0.7946 - val_loss: 0.9076 - val_acc: 0.7580\n",
      "Epoch 708/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8276 - acc: 0.7922 - val_loss: 0.9062 - val_acc: 0.7580\n",
      "Epoch 709/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8289 - acc: 0.7938 - val_loss: 0.9241 - val_acc: 0.7650\n",
      "Epoch 710/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8291 - acc: 0.7944 - val_loss: 0.9093 - val_acc: 0.7580\n",
      "Epoch 711/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8278 - acc: 0.7938 - val_loss: 0.9115 - val_acc: 0.7590\n",
      "Epoch 712/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8275 - acc: 0.7936 - val_loss: 0.9133 - val_acc: 0.7570\n",
      "Epoch 713/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8276 - acc: 0.7942 - val_loss: 0.9046 - val_acc: 0.7690\n",
      "Epoch 714/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8260 - acc: 0.7932 - val_loss: 0.9053 - val_acc: 0.7600\n",
      "Epoch 715/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8252 - acc: 0.7947 - val_loss: 0.9134 - val_acc: 0.7520\n",
      "Epoch 716/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8275 - acc: 0.7938 - val_loss: 0.9086 - val_acc: 0.7640\n",
      "Epoch 717/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8261 - acc: 0.7959 - val_loss: 0.9043 - val_acc: 0.7650\n",
      "Epoch 718/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8251 - acc: 0.7956 - val_loss: 0.9175 - val_acc: 0.7510\n",
      "Epoch 719/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8259 - acc: 0.7931 - val_loss: 0.9120 - val_acc: 0.7620\n",
      "Epoch 720/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8263 - acc: 0.7927 - val_loss: 0.9224 - val_acc: 0.7630\n",
      "Epoch 721/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8256 - acc: 0.7936 - val_loss: 0.9052 - val_acc: 0.7590\n",
      "Epoch 722/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8260 - acc: 0.7953 - val_loss: 0.9145 - val_acc: 0.7540\n",
      "Epoch 723/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8259 - acc: 0.7953 - val_loss: 0.9053 - val_acc: 0.7590\n",
      "Epoch 724/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8244 - acc: 0.7947 - val_loss: 0.9048 - val_acc: 0.7630\n",
      "Epoch 725/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8250 - acc: 0.7942 - val_loss: 0.9093 - val_acc: 0.7560\n",
      "Epoch 726/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8251 - acc: 0.7941 - val_loss: 0.9085 - val_acc: 0.7560\n",
      "Epoch 727/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8249 - acc: 0.7951 - val_loss: 0.9165 - val_acc: 0.7530\n",
      "Epoch 728/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8261 - acc: 0.7950 - val_loss: 0.9189 - val_acc: 0.7580\n",
      "Epoch 729/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8242 - acc: 0.7956 - val_loss: 0.9081 - val_acc: 0.7620\n",
      "Epoch 730/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8235 - acc: 0.7971 - val_loss: 0.9264 - val_acc: 0.7520\n",
      "Epoch 731/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8234 - acc: 0.7967 - val_loss: 0.9158 - val_acc: 0.7560\n",
      "Epoch 732/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.8242 - acc: 0.7955 - val_loss: 0.9057 - val_acc: 0.7640\n",
      "Epoch 733/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8237 - acc: 0.7933 - val_loss: 0.9077 - val_acc: 0.7570\n",
      "Epoch 734/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8233 - acc: 0.7963 - val_loss: 0.9104 - val_acc: 0.7640\n",
      "Epoch 735/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8229 - acc: 0.7949 - val_loss: 0.9192 - val_acc: 0.7600\n",
      "Epoch 736/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8231 - acc: 0.7929 - val_loss: 0.9054 - val_acc: 0.7590\n",
      "Epoch 737/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8230 - acc: 0.7935 - val_loss: 0.9039 - val_acc: 0.7650\n",
      "Epoch 738/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8230 - acc: 0.7937 - val_loss: 0.9116 - val_acc: 0.7610\n",
      "Epoch 739/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8213 - acc: 0.7976 - val_loss: 0.9166 - val_acc: 0.7590\n",
      "Epoch 740/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8226 - acc: 0.7960 - val_loss: 0.9049 - val_acc: 0.7670\n",
      "Epoch 741/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8220 - acc: 0.7971 - val_loss: 0.9177 - val_acc: 0.7570\n",
      "Epoch 742/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8235 - acc: 0.7940 - val_loss: 0.9066 - val_acc: 0.7580\n",
      "Epoch 743/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8239 - acc: 0.7922 - val_loss: 0.9016 - val_acc: 0.7640\n",
      "Epoch 744/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8208 - acc: 0.7976 - val_loss: 0.9061 - val_acc: 0.7580\n",
      "Epoch 745/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8195 - acc: 0.7965 - val_loss: 0.9011 - val_acc: 0.7680\n",
      "Epoch 746/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8194 - acc: 0.7990 - val_loss: 0.9241 - val_acc: 0.7560\n",
      "Epoch 747/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8207 - acc: 0.7964 - val_loss: 0.9012 - val_acc: 0.7690\n",
      "Epoch 748/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8248 - acc: 0.7929 - val_loss: 0.9304 - val_acc: 0.7520\n",
      "Epoch 749/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8223 - acc: 0.7940 - val_loss: 0.9034 - val_acc: 0.7650\n",
      "Epoch 750/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8214 - acc: 0.7944 - val_loss: 0.9064 - val_acc: 0.7570\n",
      "Epoch 751/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8198 - acc: 0.7942 - val_loss: 0.9097 - val_acc: 0.7590\n",
      "Epoch 752/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8204 - acc: 0.7973 - val_loss: 0.9067 - val_acc: 0.7620\n",
      "Epoch 753/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8203 - acc: 0.7963 - val_loss: 0.9104 - val_acc: 0.7640\n",
      "Epoch 754/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8196 - acc: 0.7965 - val_loss: 0.9149 - val_acc: 0.7600\n",
      "Epoch 755/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8208 - acc: 0.7936 - val_loss: 0.9155 - val_acc: 0.7570\n",
      "Epoch 756/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8210 - acc: 0.7968 - val_loss: 0.9237 - val_acc: 0.7510\n",
      "Epoch 757/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8186 - acc: 0.7962 - val_loss: 0.9012 - val_acc: 0.7600\n",
      "Epoch 758/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8186 - acc: 0.7987 - val_loss: 0.9028 - val_acc: 0.7650\n",
      "Epoch 759/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.8242 - acc: 0.789 - 0s 26us/step - loss: 0.8235 - acc: 0.7901 - val_loss: 0.9006 - val_acc: 0.7660\n",
      "Epoch 760/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8186 - acc: 0.7972 - val_loss: 0.9005 - val_acc: 0.7640\n",
      "Epoch 761/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8194 - acc: 0.7960 - val_loss: 0.9072 - val_acc: 0.7580\n",
      "Epoch 762/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8200 - acc: 0.7949 - val_loss: 0.9063 - val_acc: 0.7630\n",
      "Epoch 763/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8188 - acc: 0.7977 - val_loss: 0.8980 - val_acc: 0.7640\n",
      "Epoch 764/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8185 - acc: 0.7995 - val_loss: 0.8995 - val_acc: 0.7660\n",
      "Epoch 765/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8160 - acc: 0.7972 - val_loss: 0.9095 - val_acc: 0.7570\n",
      "Epoch 766/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8186 - acc: 0.7946 - val_loss: 0.9038 - val_acc: 0.7660\n",
      "Epoch 767/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8168 - acc: 0.7982 - val_loss: 0.9013 - val_acc: 0.7690\n",
      "Epoch 768/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8188 - acc: 0.7964 - val_loss: 0.9188 - val_acc: 0.7610\n",
      "Epoch 769/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8182 - acc: 0.7967 - val_loss: 0.9177 - val_acc: 0.7550\n",
      "Epoch 770/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8193 - acc: 0.7976 - val_loss: 0.9119 - val_acc: 0.7630\n",
      "Epoch 771/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8181 - acc: 0.7941 - val_loss: 0.9063 - val_acc: 0.7600\n",
      "Epoch 772/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8167 - acc: 0.7962 - val_loss: 0.9024 - val_acc: 0.7650\n",
      "Epoch 773/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8145 - acc: 0.7995 - val_loss: 0.9215 - val_acc: 0.7530\n",
      "Epoch 774/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8181 - acc: 0.7986 - val_loss: 0.8997 - val_acc: 0.7630\n",
      "Epoch 775/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8189 - acc: 0.7944 - val_loss: 0.9070 - val_acc: 0.7580\n",
      "Epoch 776/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8156 - acc: 0.7976 - val_loss: 0.9035 - val_acc: 0.7640\n",
      "Epoch 777/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8169 - acc: 0.7996 - val_loss: 0.9079 - val_acc: 0.7600\n",
      "Epoch 778/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8167 - acc: 0.7974 - val_loss: 0.8996 - val_acc: 0.7610\n",
      "Epoch 779/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8162 - acc: 0.7990 - val_loss: 0.9118 - val_acc: 0.7630\n",
      "Epoch 780/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8179 - acc: 0.7974 - val_loss: 0.9238 - val_acc: 0.7550\n",
      "Epoch 781/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8178 - acc: 0.7978 - val_loss: 0.9143 - val_acc: 0.7610\n",
      "Epoch 782/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8156 - acc: 0.7971 - val_loss: 0.9005 - val_acc: 0.7630\n",
      "Epoch 783/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8142 - acc: 0.7973 - val_loss: 0.9168 - val_acc: 0.7540\n",
      "Epoch 784/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8144 - acc: 0.7976 - val_loss: 0.9080 - val_acc: 0.7620\n",
      "Epoch 785/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8158 - acc: 0.7982 - val_loss: 0.9037 - val_acc: 0.7600\n",
      "Epoch 786/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8136 - acc: 0.7990 - val_loss: 0.9020 - val_acc: 0.7640\n",
      "Epoch 787/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8153 - acc: 0.7979 - val_loss: 0.9114 - val_acc: 0.7540\n",
      "Epoch 788/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8164 - acc: 0.7965 - val_loss: 0.9160 - val_acc: 0.7560\n",
      "Epoch 789/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8158 - acc: 0.7956 - val_loss: 0.9388 - val_acc: 0.7470\n",
      "Epoch 790/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8135 - acc: 0.7951 - val_loss: 0.9153 - val_acc: 0.7530\n",
      "Epoch 791/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8133 - acc: 0.7978 - val_loss: 0.9013 - val_acc: 0.7660\n",
      "Epoch 792/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8133 - acc: 0.7979 - val_loss: 0.9015 - val_acc: 0.7610\n",
      "Epoch 793/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8146 - acc: 0.7997 - val_loss: 0.9126 - val_acc: 0.7610\n",
      "Epoch 794/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8134 - acc: 0.8006 - val_loss: 0.9002 - val_acc: 0.7620\n",
      "Epoch 795/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8136 - acc: 0.7967 - val_loss: 0.9062 - val_acc: 0.7620\n",
      "Epoch 796/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8099 - acc: 0.7968 - val_loss: 0.9108 - val_acc: 0.7570\n",
      "Epoch 797/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8148 - acc: 0.7968 - val_loss: 0.9200 - val_acc: 0.7530\n",
      "Epoch 798/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8117 - acc: 0.7979 - val_loss: 0.9098 - val_acc: 0.7590\n",
      "Epoch 799/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8109 - acc: 0.7991 - val_loss: 0.9106 - val_acc: 0.7610\n",
      "Epoch 800/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8127 - acc: 0.7969 - val_loss: 0.8987 - val_acc: 0.7660\n",
      "Epoch 801/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.8136 - acc: 0.7968 - val_loss: 0.9008 - val_acc: 0.7620\n",
      "Epoch 802/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8123 - acc: 0.7977 - val_loss: 0.8960 - val_acc: 0.7670\n",
      "Epoch 803/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8108 - acc: 0.7990 - val_loss: 0.9083 - val_acc: 0.7650\n",
      "Epoch 804/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8134 - acc: 0.7973 - val_loss: 0.9023 - val_acc: 0.7610\n",
      "Epoch 805/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8110 - acc: 0.8014 - val_loss: 0.9113 - val_acc: 0.7570\n",
      "Epoch 806/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8117 - acc: 0.7983 - val_loss: 0.9041 - val_acc: 0.7610\n",
      "Epoch 807/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8108 - acc: 0.7988 - val_loss: 0.8964 - val_acc: 0.7650\n",
      "Epoch 808/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8113 - acc: 0.7983 - val_loss: 0.9030 - val_acc: 0.7610\n",
      "Epoch 809/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8119 - acc: 0.7992 - val_loss: 0.8999 - val_acc: 0.7600\n",
      "Epoch 810/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8130 - acc: 0.7956 - val_loss: 0.9091 - val_acc: 0.7630\n",
      "Epoch 811/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8094 - acc: 0.7995 - val_loss: 0.9114 - val_acc: 0.7580\n",
      "Epoch 812/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8108 - acc: 0.7962 - val_loss: 0.8956 - val_acc: 0.7620\n",
      "Epoch 813/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8098 - acc: 0.7982 - val_loss: 0.9127 - val_acc: 0.7530\n",
      "Epoch 814/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8113 - acc: 0.7990 - val_loss: 0.8996 - val_acc: 0.7680\n",
      "Epoch 815/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8103 - acc: 0.7978 - val_loss: 0.9073 - val_acc: 0.7600\n",
      "Epoch 816/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8103 - acc: 0.7969 - val_loss: 0.9010 - val_acc: 0.7680\n",
      "Epoch 817/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8119 - acc: 0.7969 - val_loss: 0.9053 - val_acc: 0.7650\n",
      "Epoch 818/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8091 - acc: 0.8003 - val_loss: 0.9196 - val_acc: 0.7560\n",
      "Epoch 819/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8083 - acc: 0.8013 - val_loss: 0.9037 - val_acc: 0.7600\n",
      "Epoch 820/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8089 - acc: 0.7986 - val_loss: 0.9018 - val_acc: 0.7640\n",
      "Epoch 821/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8091 - acc: 0.7979 - val_loss: 0.9066 - val_acc: 0.7570\n",
      "Epoch 822/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8072 - acc: 0.7990 - val_loss: 0.9006 - val_acc: 0.7630\n",
      "Epoch 823/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8067 - acc: 0.8015 - val_loss: 0.8974 - val_acc: 0.7640\n",
      "Epoch 824/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8092 - acc: 0.8006 - val_loss: 0.8974 - val_acc: 0.7660\n",
      "Epoch 825/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8083 - acc: 0.8009 - val_loss: 0.8980 - val_acc: 0.7660\n",
      "Epoch 826/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8080 - acc: 0.8000 - val_loss: 0.9019 - val_acc: 0.7580\n",
      "Epoch 827/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8079 - acc: 0.7999 - val_loss: 0.9096 - val_acc: 0.7600\n",
      "Epoch 828/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8083 - acc: 0.8004 - val_loss: 0.8975 - val_acc: 0.7660\n",
      "Epoch 829/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8062 - acc: 0.8015 - val_loss: 0.8949 - val_acc: 0.7660\n",
      "Epoch 830/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8091 - acc: 0.7999 - val_loss: 0.9103 - val_acc: 0.7580\n",
      "Epoch 831/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8101 - acc: 0.7990 - val_loss: 0.9165 - val_acc: 0.7560\n",
      "Epoch 832/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8112 - acc: 0.7978 - val_loss: 0.8967 - val_acc: 0.7620\n",
      "Epoch 833/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8064 - acc: 0.7995 - val_loss: 0.8953 - val_acc: 0.7690\n",
      "Epoch 834/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8072 - acc: 0.7991 - val_loss: 0.8991 - val_acc: 0.7640\n",
      "Epoch 835/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8050 - acc: 0.8037 - val_loss: 0.9011 - val_acc: 0.7650\n",
      "Epoch 836/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8064 - acc: 0.8012 - val_loss: 0.9183 - val_acc: 0.7580\n",
      "Epoch 837/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8064 - acc: 0.8006 - val_loss: 0.8918 - val_acc: 0.7670\n",
      "Epoch 838/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8074 - acc: 0.7976 - val_loss: 0.8964 - val_acc: 0.7660\n",
      "Epoch 839/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8075 - acc: 0.7982 - val_loss: 0.8950 - val_acc: 0.7680\n",
      "Epoch 840/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8050 - acc: 0.8003 - val_loss: 0.9058 - val_acc: 0.7600\n",
      "Epoch 841/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8082 - acc: 0.8003 - val_loss: 0.9019 - val_acc: 0.7680\n",
      "Epoch 842/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8055 - acc: 0.8003 - val_loss: 0.9044 - val_acc: 0.7690\n",
      "Epoch 843/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8052 - acc: 0.8000 - val_loss: 0.8941 - val_acc: 0.7660\n",
      "Epoch 844/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8061 - acc: 0.8001 - val_loss: 0.9115 - val_acc: 0.7570\n",
      "Epoch 845/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8083 - acc: 0.7988 - val_loss: 0.9140 - val_acc: 0.7550\n",
      "Epoch 846/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8065 - acc: 0.8006 - val_loss: 0.9052 - val_acc: 0.7580\n",
      "Epoch 847/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8072 - acc: 0.8005 - val_loss: 0.9017 - val_acc: 0.7650\n",
      "Epoch 848/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8045 - acc: 0.8017 - val_loss: 0.8983 - val_acc: 0.7580\n",
      "Epoch 849/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8031 - acc: 0.8014 - val_loss: 0.8988 - val_acc: 0.7650\n",
      "Epoch 850/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8022 - acc: 0.8029 - val_loss: 0.9061 - val_acc: 0.7610\n",
      "Epoch 851/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8059 - acc: 0.7994 - val_loss: 0.8955 - val_acc: 0.7710\n",
      "Epoch 852/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8069 - acc: 0.8003 - val_loss: 0.8965 - val_acc: 0.7640\n",
      "Epoch 853/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8037 - acc: 0.8010 - val_loss: 0.8948 - val_acc: 0.7670\n",
      "Epoch 854/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8046 - acc: 0.8006 - val_loss: 0.8906 - val_acc: 0.7660\n",
      "Epoch 855/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8065 - acc: 0.7987 - val_loss: 0.9040 - val_acc: 0.7600\n",
      "Epoch 856/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8034 - acc: 0.8008 - val_loss: 0.9043 - val_acc: 0.7630\n",
      "Epoch 857/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8021 - acc: 0.8045 - val_loss: 0.9004 - val_acc: 0.7640\n",
      "Epoch 858/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8026 - acc: 0.8010 - val_loss: 0.9005 - val_acc: 0.7650\n",
      "Epoch 859/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8030 - acc: 0.8023 - val_loss: 0.8926 - val_acc: 0.7640\n",
      "Epoch 860/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8056 - acc: 0.7969 - val_loss: 0.9020 - val_acc: 0.7630\n",
      "Epoch 861/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8062 - acc: 0.8019 - val_loss: 0.9085 - val_acc: 0.7630\n",
      "Epoch 862/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8045 - acc: 0.8015 - val_loss: 0.8950 - val_acc: 0.7650\n",
      "Epoch 863/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8029 - acc: 0.8056 - val_loss: 0.9112 - val_acc: 0.7640\n",
      "Epoch 864/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8046 - acc: 0.8000 - val_loss: 0.8953 - val_acc: 0.7690\n",
      "Epoch 865/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8023 - acc: 0.8031 - val_loss: 0.8955 - val_acc: 0.7670\n",
      "Epoch 866/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8035 - acc: 0.7985 - val_loss: 0.9013 - val_acc: 0.7600\n",
      "Epoch 867/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8039 - acc: 0.8028 - val_loss: 0.8982 - val_acc: 0.7660\n",
      "Epoch 868/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8023 - acc: 0.8035 - val_loss: 0.9487 - val_acc: 0.7460\n",
      "Epoch 869/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8038 - acc: 0.7986 - val_loss: 0.9256 - val_acc: 0.7510\n",
      "Epoch 870/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8004 - acc: 0.8035 - val_loss: 0.8901 - val_acc: 0.7700\n",
      "Epoch 871/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8005 - acc: 0.8012 - val_loss: 0.9408 - val_acc: 0.7450\n",
      "Epoch 872/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8030 - acc: 0.8021 - val_loss: 0.9134 - val_acc: 0.7570\n",
      "Epoch 873/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8026 - acc: 0.8015 - val_loss: 0.8972 - val_acc: 0.7560\n",
      "Epoch 874/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8078 - acc: 0.8009 - val_loss: 0.9003 - val_acc: 0.7590\n",
      "Epoch 875/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8067 - acc: 0.8001 - val_loss: 0.8914 - val_acc: 0.7680\n",
      "Epoch 876/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8017 - acc: 0.8042 - val_loss: 0.9110 - val_acc: 0.7620\n",
      "Epoch 877/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8052 - acc: 0.8003 - val_loss: 0.9239 - val_acc: 0.7550\n",
      "Epoch 878/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8030 - acc: 0.8003 - val_loss: 0.8912 - val_acc: 0.7660\n",
      "Epoch 879/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - ETA: 0s - loss: 0.8031 - acc: 0.802 - 0s 26us/step - loss: 0.8002 - acc: 0.8029 - val_loss: 0.8963 - val_acc: 0.7650\n",
      "Epoch 880/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8011 - acc: 0.8026 - val_loss: 0.8995 - val_acc: 0.7600\n",
      "Epoch 881/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7987 - acc: 0.8009 - val_loss: 0.8978 - val_acc: 0.7690\n",
      "Epoch 882/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8012 - acc: 0.8008 - val_loss: 0.9130 - val_acc: 0.7570\n",
      "Epoch 883/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8026 - acc: 0.7979 - val_loss: 0.9144 - val_acc: 0.7520\n",
      "Epoch 884/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8012 - acc: 0.8023 - val_loss: 0.9024 - val_acc: 0.7610\n",
      "Epoch 885/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8051 - acc: 0.8006 - val_loss: 0.9169 - val_acc: 0.7550\n",
      "Epoch 886/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8055 - acc: 0.8015 - val_loss: 0.8938 - val_acc: 0.7660\n",
      "Epoch 887/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7996 - acc: 0.8027 - val_loss: 0.8913 - val_acc: 0.7700\n",
      "Epoch 888/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7992 - acc: 0.8028 - val_loss: 1.0080 - val_acc: 0.7190\n",
      "Epoch 889/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8041 - acc: 0.8013 - val_loss: 0.8969 - val_acc: 0.7680\n",
      "Epoch 890/1000\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 0.8016 - acc: 0.803 - 0s 26us/step - loss: 0.8001 - acc: 0.8018 - val_loss: 0.8987 - val_acc: 0.7640\n",
      "Epoch 891/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7993 - acc: 0.8035 - val_loss: 0.9258 - val_acc: 0.7620\n",
      "Epoch 892/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7997 - acc: 0.8036 - val_loss: 0.8920 - val_acc: 0.7710\n",
      "Epoch 893/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8024 - acc: 0.8021 - val_loss: 0.8958 - val_acc: 0.7630\n",
      "Epoch 894/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7986 - acc: 0.8038 - val_loss: 0.9026 - val_acc: 0.7620\n",
      "Epoch 895/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7979 - acc: 0.8051 - val_loss: 0.9098 - val_acc: 0.7600\n",
      "Epoch 896/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8026 - acc: 0.8023 - val_loss: 0.9336 - val_acc: 0.7540\n",
      "Epoch 897/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8002 - acc: 0.8018 - val_loss: 0.8928 - val_acc: 0.7620\n",
      "Epoch 898/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7989 - acc: 0.8032 - val_loss: 0.9386 - val_acc: 0.7390\n",
      "Epoch 899/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7975 - acc: 0.8033 - val_loss: 0.8987 - val_acc: 0.7620\n",
      "Epoch 900/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7969 - acc: 0.8031 - val_loss: 0.9094 - val_acc: 0.7600\n",
      "Epoch 901/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7981 - acc: 0.8031 - val_loss: 0.8966 - val_acc: 0.7650\n",
      "Epoch 902/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7988 - acc: 0.8028 - val_loss: 0.8956 - val_acc: 0.7670\n",
      "Epoch 903/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7973 - acc: 0.8067 - val_loss: 0.8925 - val_acc: 0.7670\n",
      "Epoch 904/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7979 - acc: 0.8006 - val_loss: 0.8951 - val_acc: 0.7630\n",
      "Epoch 905/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7958 - acc: 0.8059 - val_loss: 0.8945 - val_acc: 0.7660\n",
      "Epoch 906/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7972 - acc: 0.8029 - val_loss: 0.8897 - val_acc: 0.7640\n",
      "Epoch 907/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8011 - acc: 0.8021 - val_loss: 0.8995 - val_acc: 0.7620\n",
      "Epoch 908/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8012 - acc: 0.8027 - val_loss: 0.8919 - val_acc: 0.7720\n",
      "Epoch 909/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7983 - acc: 0.8036 - val_loss: 0.8937 - val_acc: 0.7620\n",
      "Epoch 910/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7973 - acc: 0.8028 - val_loss: 0.9040 - val_acc: 0.7670\n",
      "Epoch 911/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7969 - acc: 0.8033 - val_loss: 0.8932 - val_acc: 0.7680\n",
      "Epoch 912/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7995 - acc: 0.8021 - val_loss: 0.8964 - val_acc: 0.7670\n",
      "Epoch 913/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7979 - acc: 0.8036 - val_loss: 0.8906 - val_acc: 0.7690\n",
      "Epoch 914/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7951 - acc: 0.8037 - val_loss: 0.8906 - val_acc: 0.7700\n",
      "Epoch 915/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7982 - acc: 0.8042 - val_loss: 0.8959 - val_acc: 0.7640\n",
      "Epoch 916/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7977 - acc: 0.8051 - val_loss: 0.9133 - val_acc: 0.7550\n",
      "Epoch 917/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7983 - acc: 0.8023 - val_loss: 0.8997 - val_acc: 0.7680\n",
      "Epoch 918/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8009 - acc: 0.8026 - val_loss: 0.8991 - val_acc: 0.7630\n",
      "Epoch 919/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7969 - acc: 0.8049 - val_loss: 0.9080 - val_acc: 0.7590\n",
      "Epoch 920/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7946 - acc: 0.8058 - val_loss: 0.9019 - val_acc: 0.7690\n",
      "Epoch 921/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7937 - acc: 0.8056 - val_loss: 0.8951 - val_acc: 0.7630\n",
      "Epoch 922/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.8032 - acc: 0.8013 - val_loss: 0.8882 - val_acc: 0.7670\n",
      "Epoch 923/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7997 - acc: 0.8042 - val_loss: 0.8991 - val_acc: 0.7650\n",
      "Epoch 924/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7938 - acc: 0.8073 - val_loss: 0.8980 - val_acc: 0.7640\n",
      "Epoch 925/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7981 - acc: 0.8022 - val_loss: 0.8965 - val_acc: 0.7670\n",
      "Epoch 926/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7948 - acc: 0.8069 - val_loss: 0.8952 - val_acc: 0.7640\n",
      "Epoch 927/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7953 - acc: 0.8047 - val_loss: 0.9051 - val_acc: 0.7620\n",
      "Epoch 928/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7950 - acc: 0.8042 - val_loss: 0.8971 - val_acc: 0.7670\n",
      "Epoch 929/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7944 - acc: 0.8036 - val_loss: 0.8984 - val_acc: 0.7640\n",
      "Epoch 930/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7990 - acc: 0.8021 - val_loss: 0.8959 - val_acc: 0.7680\n",
      "Epoch 931/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7946 - acc: 0.8049 - val_loss: 0.8995 - val_acc: 0.7660\n",
      "Epoch 932/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7992 - acc: 0.8038 - val_loss: 0.8904 - val_acc: 0.7730\n",
      "Epoch 933/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7993 - acc: 0.8010 - val_loss: 0.9170 - val_acc: 0.7580\n",
      "Epoch 934/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7968 - acc: 0.8037 - val_loss: 0.8932 - val_acc: 0.7640\n",
      "Epoch 935/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7950 - acc: 0.8027 - val_loss: 0.9008 - val_acc: 0.7690\n",
      "Epoch 936/1000\n",
      "7800/7800 [==============================] - 0s 28us/step - loss: 0.7922 - acc: 0.8095 - val_loss: 0.9157 - val_acc: 0.7580\n",
      "Epoch 937/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7937 - acc: 0.8031 - val_loss: 0.8909 - val_acc: 0.7640\n",
      "Epoch 938/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7942 - acc: 0.8038 - val_loss: 0.8952 - val_acc: 0.7730\n",
      "Epoch 939/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7930 - acc: 0.8063 - val_loss: 0.8948 - val_acc: 0.7700\n",
      "Epoch 940/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7928 - acc: 0.8071 - val_loss: 0.9005 - val_acc: 0.7670\n",
      "Epoch 941/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7935 - acc: 0.8028 - val_loss: 0.8898 - val_acc: 0.7700\n",
      "Epoch 942/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7977 - acc: 0.8054 - val_loss: 0.9023 - val_acc: 0.7580\n",
      "Epoch 943/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7937 - acc: 0.8078 - val_loss: 0.9060 - val_acc: 0.7650\n",
      "Epoch 944/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7925 - acc: 0.8060 - val_loss: 0.9013 - val_acc: 0.7640\n",
      "Epoch 945/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7957 - acc: 0.8033 - val_loss: 0.8958 - val_acc: 0.7650\n",
      "Epoch 946/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7948 - acc: 0.8049 - val_loss: 0.9143 - val_acc: 0.7650\n",
      "Epoch 947/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7968 - acc: 0.8058 - val_loss: 0.9066 - val_acc: 0.7610\n",
      "Epoch 948/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7956 - acc: 0.8041 - val_loss: 0.9231 - val_acc: 0.7560\n",
      "Epoch 949/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7947 - acc: 0.8069 - val_loss: 0.9074 - val_acc: 0.7600\n",
      "Epoch 950/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7922 - acc: 0.8051 - val_loss: 0.9044 - val_acc: 0.7590\n",
      "Epoch 951/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7964 - acc: 0.8073 - val_loss: 0.8948 - val_acc: 0.7680\n",
      "Epoch 952/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7963 - acc: 0.8040 - val_loss: 0.9027 - val_acc: 0.7590\n",
      "Epoch 953/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7975 - acc: 0.8058 - val_loss: 0.8944 - val_acc: 0.7620\n",
      "Epoch 954/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7903 - acc: 0.8060 - val_loss: 0.8941 - val_acc: 0.7660\n",
      "Epoch 955/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7921 - acc: 0.8051 - val_loss: 0.8983 - val_acc: 0.7650\n",
      "Epoch 956/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7937 - acc: 0.8079 - val_loss: 0.8893 - val_acc: 0.7690\n",
      "Epoch 957/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7956 - acc: 0.8050 - val_loss: 0.9165 - val_acc: 0.7550\n",
      "Epoch 958/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7966 - acc: 0.8037 - val_loss: 0.8902 - val_acc: 0.7690\n",
      "Epoch 959/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7908 - acc: 0.8063 - val_loss: 0.8964 - val_acc: 0.7680\n",
      "Epoch 960/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7954 - acc: 0.8056 - val_loss: 0.8932 - val_acc: 0.7670\n",
      "Epoch 961/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7913 - acc: 0.8059 - val_loss: 0.8916 - val_acc: 0.7660\n",
      "Epoch 962/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7922 - acc: 0.8056 - val_loss: 0.9168 - val_acc: 0.7570\n",
      "Epoch 963/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7907 - acc: 0.8069 - val_loss: 0.8999 - val_acc: 0.7640\n",
      "Epoch 964/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7874 - acc: 0.8096 - val_loss: 0.9220 - val_acc: 0.7550\n",
      "Epoch 965/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7966 - acc: 0.8033 - val_loss: 0.8974 - val_acc: 0.7700\n",
      "Epoch 966/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7925 - acc: 0.8073 - val_loss: 0.9120 - val_acc: 0.7580\n",
      "Epoch 967/1000\n",
      "7800/7800 [==============================] - 0s 25us/step - loss: 0.7923 - acc: 0.8055 - val_loss: 0.9104 - val_acc: 0.7560\n",
      "Epoch 968/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7899 - acc: 0.8090 - val_loss: 0.8907 - val_acc: 0.7630\n",
      "Epoch 969/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7907 - acc: 0.8064 - val_loss: 0.9087 - val_acc: 0.7560\n",
      "Epoch 970/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7937 - acc: 0.8031 - val_loss: 0.9141 - val_acc: 0.7620\n",
      "Epoch 971/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7941 - acc: 0.8038 - val_loss: 0.9057 - val_acc: 0.7650\n",
      "Epoch 972/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7892 - acc: 0.8037 - val_loss: 0.8926 - val_acc: 0.7660\n",
      "Epoch 973/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7897 - acc: 0.8071 - val_loss: 0.9441 - val_acc: 0.7500\n",
      "Epoch 974/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8044 - acc: 0.8004 - val_loss: 0.8914 - val_acc: 0.7770\n",
      "Epoch 975/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7965 - acc: 0.8049 - val_loss: 0.8874 - val_acc: 0.7660\n",
      "Epoch 976/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7888 - acc: 0.8074 - val_loss: 0.8974 - val_acc: 0.7690\n",
      "Epoch 977/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8002 - acc: 0.8022 - val_loss: 0.8992 - val_acc: 0.7690\n",
      "Epoch 978/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7914 - acc: 0.8065 - val_loss: 0.8948 - val_acc: 0.7640\n",
      "Epoch 979/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7895 - acc: 0.8067 - val_loss: 0.9210 - val_acc: 0.7610\n",
      "Epoch 980/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7882 - acc: 0.8064 - val_loss: 0.8896 - val_acc: 0.7690\n",
      "Epoch 981/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7912 - acc: 0.8065 - val_loss: 0.8995 - val_acc: 0.7610\n",
      "Epoch 982/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.8059 - acc: 0.8008 - val_loss: 0.9037 - val_acc: 0.7680\n",
      "Epoch 983/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7914 - acc: 0.8041 - val_loss: 0.8904 - val_acc: 0.7680\n",
      "Epoch 984/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7865 - acc: 0.8083 - val_loss: 0.9007 - val_acc: 0.7620\n",
      "Epoch 985/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7869 - acc: 0.8118 - val_loss: 0.8912 - val_acc: 0.7690\n",
      "Epoch 986/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7872 - acc: 0.8121 - val_loss: 0.9017 - val_acc: 0.7640\n",
      "Epoch 987/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7940 - acc: 0.8058 - val_loss: 0.8942 - val_acc: 0.7660\n",
      "Epoch 988/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7884 - acc: 0.8068 - val_loss: 0.8958 - val_acc: 0.7670\n",
      "Epoch 989/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7903 - acc: 0.8071 - val_loss: 0.8993 - val_acc: 0.7680\n",
      "Epoch 990/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7979 - acc: 0.8024 - val_loss: 0.8978 - val_acc: 0.7640\n",
      "Epoch 991/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7894 - acc: 0.8074 - val_loss: 0.9454 - val_acc: 0.7460\n",
      "Epoch 992/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7936 - acc: 0.8078 - val_loss: 0.8925 - val_acc: 0.7630\n",
      "Epoch 993/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7888 - acc: 0.8051 - val_loss: 0.9096 - val_acc: 0.7600\n",
      "Epoch 994/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7864 - acc: 0.8073 - val_loss: 0.9054 - val_acc: 0.7640\n",
      "Epoch 995/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7912 - acc: 0.8087 - val_loss: 0.9220 - val_acc: 0.7540\n",
      "Epoch 996/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7904 - acc: 0.8060 - val_loss: 0.8996 - val_acc: 0.7650\n",
      "Epoch 997/1000\n",
      "7800/7800 [==============================] - 0s 27us/step - loss: 0.7892 - acc: 0.8071 - val_loss: 0.8871 - val_acc: 0.7690\n",
      "Epoch 998/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7869 - acc: 0.8106 - val_loss: 0.8921 - val_acc: 0.7740\n",
      "Epoch 999/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7872 - acc: 0.8069 - val_loss: 0.8909 - val_acc: 0.7740\n",
      "Epoch 1000/1000\n",
      "7800/7800 [==============================] - 0s 26us/step - loss: 0.7870 - acc: 0.8091 - val_loss: 0.8922 - val_acc: 0.7730\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 20us/step\n",
      "1200/1200 [==============================] - 0s 21us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7762257163341229, 0.8120512821124151]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9650393239657085, 0.7333333333333333]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 00:14:14.261950 4749919680 deprecation.py:506] From /Users/paulw/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7800 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7800/7800 [==============================] - 1s 96us/step - loss: 1.9729 - acc: 0.1413 - val_loss: 1.9431 - val_acc: 0.1530\n",
      "Epoch 2/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.9603 - acc: 0.1444 - val_loss: 1.9309 - val_acc: 0.1850\n",
      "Epoch 3/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.9459 - acc: 0.1553 - val_loss: 1.9221 - val_acc: 0.2160\n",
      "Epoch 4/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.9368 - acc: 0.1779 - val_loss: 1.9150 - val_acc: 0.2330\n",
      "Epoch 5/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.9331 - acc: 0.1686 - val_loss: 1.9081 - val_acc: 0.2460\n",
      "Epoch 6/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.9257 - acc: 0.1836 - val_loss: 1.9007 - val_acc: 0.2540\n",
      "Epoch 7/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.9205 - acc: 0.1903 - val_loss: 1.8936 - val_acc: 0.2670\n",
      "Epoch 8/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.9103 - acc: 0.2033 - val_loss: 1.8854 - val_acc: 0.2830\n",
      "Epoch 9/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.9015 - acc: 0.2038 - val_loss: 1.8762 - val_acc: 0.2930\n",
      "Epoch 10/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.8956 - acc: 0.2224 - val_loss: 1.8664 - val_acc: 0.3070\n",
      "Epoch 11/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.8864 - acc: 0.2269 - val_loss: 1.8554 - val_acc: 0.3180\n",
      "Epoch 12/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.8767 - acc: 0.2397 - val_loss: 1.8422 - val_acc: 0.3290\n",
      "Epoch 13/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.8704 - acc: 0.2385 - val_loss: 1.8286 - val_acc: 0.3360\n",
      "Epoch 14/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.8535 - acc: 0.2595 - val_loss: 1.8121 - val_acc: 0.3500\n",
      "Epoch 15/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.8492 - acc: 0.2592 - val_loss: 1.7953 - val_acc: 0.3640\n",
      "Epoch 16/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.8341 - acc: 0.2672 - val_loss: 1.7764 - val_acc: 0.3700\n",
      "Epoch 17/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.8174 - acc: 0.2754 - val_loss: 1.7565 - val_acc: 0.3940\n",
      "Epoch 18/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.8059 - acc: 0.2701 - val_loss: 1.7356 - val_acc: 0.4080\n",
      "Epoch 19/200\n",
      "7800/7800 [==============================] - 0s 33us/step - loss: 1.7866 - acc: 0.2917 - val_loss: 1.7126 - val_acc: 0.4200\n",
      "Epoch 20/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.7772 - acc: 0.2954 - val_loss: 1.6906 - val_acc: 0.4410\n",
      "Epoch 21/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.7623 - acc: 0.3056 - val_loss: 1.6663 - val_acc: 0.4460\n",
      "Epoch 22/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.7313 - acc: 0.3222 - val_loss: 1.6386 - val_acc: 0.4520\n",
      "Epoch 23/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.7176 - acc: 0.3201 - val_loss: 1.6139 - val_acc: 0.4540\n",
      "Epoch 24/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.6994 - acc: 0.3283 - val_loss: 1.5889 - val_acc: 0.4710\n",
      "Epoch 25/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.6869 - acc: 0.3401 - val_loss: 1.5642 - val_acc: 0.4770\n",
      "Epoch 26/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 1.6642 - acc: 0.3491 - val_loss: 1.5379 - val_acc: 0.4930\n",
      "Epoch 27/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 1.6439 - acc: 0.3603 - val_loss: 1.5116 - val_acc: 0.4960\n",
      "Epoch 28/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.6320 - acc: 0.3597 - val_loss: 1.4866 - val_acc: 0.5050\n",
      "Epoch 29/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.6191 - acc: 0.3664 - val_loss: 1.4647 - val_acc: 0.5140\n",
      "Epoch 30/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 1.5925 - acc: 0.3778 - val_loss: 1.4380 - val_acc: 0.5200\n",
      "Epoch 31/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.5820 - acc: 0.3886 - val_loss: 1.4148 - val_acc: 0.5250\n",
      "Epoch 32/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.5633 - acc: 0.3951 - val_loss: 1.3914 - val_acc: 0.5330\n",
      "Epoch 33/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.5444 - acc: 0.4024 - val_loss: 1.3686 - val_acc: 0.5410\n",
      "Epoch 34/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.5325 - acc: 0.4054 - val_loss: 1.3481 - val_acc: 0.5570\n",
      "Epoch 35/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.5095 - acc: 0.4140 - val_loss: 1.3257 - val_acc: 0.5600\n",
      "Epoch 36/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.4912 - acc: 0.4297 - val_loss: 1.3051 - val_acc: 0.5670\n",
      "Epoch 37/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.4779 - acc: 0.4338 - val_loss: 1.2834 - val_acc: 0.5860\n",
      "Epoch 38/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.4676 - acc: 0.4406 - val_loss: 1.2642 - val_acc: 0.5950\n",
      "Epoch 39/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.4550 - acc: 0.4429 - val_loss: 1.2456 - val_acc: 0.5990\n",
      "Epoch 40/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.4375 - acc: 0.4462 - val_loss: 1.2270 - val_acc: 0.6040\n",
      "Epoch 41/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.4196 - acc: 0.4500 - val_loss: 1.2106 - val_acc: 0.6090\n",
      "Epoch 42/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.4138 - acc: 0.4594 - val_loss: 1.1933 - val_acc: 0.6140\n",
      "Epoch 43/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.4000 - acc: 0.4705 - val_loss: 1.1774 - val_acc: 0.6180\n",
      "Epoch 44/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.3736 - acc: 0.4764 - val_loss: 1.1601 - val_acc: 0.6190\n",
      "Epoch 45/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.3729 - acc: 0.4738 - val_loss: 1.1453 - val_acc: 0.6300\n",
      "Epoch 46/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.3466 - acc: 0.4828 - val_loss: 1.1303 - val_acc: 0.6320\n",
      "Epoch 47/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.3487 - acc: 0.4877 - val_loss: 1.1174 - val_acc: 0.6360\n",
      "Epoch 48/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.3309 - acc: 0.4900 - val_loss: 1.1039 - val_acc: 0.6440\n",
      "Epoch 49/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.3261 - acc: 0.4919 - val_loss: 1.0897 - val_acc: 0.6480\n",
      "Epoch 50/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.3094 - acc: 0.5026 - val_loss: 1.0751 - val_acc: 0.6560\n",
      "Epoch 51/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.3079 - acc: 0.4999 - val_loss: 1.0649 - val_acc: 0.6600\n",
      "Epoch 52/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.2811 - acc: 0.5104 - val_loss: 1.0520 - val_acc: 0.6630\n",
      "Epoch 53/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.2852 - acc: 0.5122 - val_loss: 1.0398 - val_acc: 0.6650\n",
      "Epoch 54/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.2553 - acc: 0.5213 - val_loss: 1.0288 - val_acc: 0.6670\n",
      "Epoch 55/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.2602 - acc: 0.5237 - val_loss: 1.0197 - val_acc: 0.6670\n",
      "Epoch 56/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.2377 - acc: 0.5323 - val_loss: 1.0080 - val_acc: 0.6690\n",
      "Epoch 57/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.2562 - acc: 0.5185 - val_loss: 1.0006 - val_acc: 0.6700\n",
      "Epoch 58/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.2226 - acc: 0.5392 - val_loss: 0.9901 - val_acc: 0.6680\n",
      "Epoch 59/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.2058 - acc: 0.5426 - val_loss: 0.9782 - val_acc: 0.6730\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.2179 - acc: 0.5335 - val_loss: 0.9705 - val_acc: 0.6750\n",
      "Epoch 61/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.2109 - acc: 0.5444 - val_loss: 0.9627 - val_acc: 0.6770\n",
      "Epoch 62/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.1816 - acc: 0.5544 - val_loss: 0.9509 - val_acc: 0.6810\n",
      "Epoch 63/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.1949 - acc: 0.5483 - val_loss: 0.9457 - val_acc: 0.6810\n",
      "Epoch 64/200\n",
      "7800/7800 [==============================] - ETA: 0s - loss: 1.1967 - acc: 0.552 - 0s 34us/step - loss: 1.1942 - acc: 0.5527 - val_loss: 0.9409 - val_acc: 0.6770\n",
      "Epoch 65/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.1870 - acc: 0.5537 - val_loss: 0.9349 - val_acc: 0.6810\n",
      "Epoch 66/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.1724 - acc: 0.5537 - val_loss: 0.9265 - val_acc: 0.6860\n",
      "Epoch 67/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.1707 - acc: 0.5601 - val_loss: 0.9184 - val_acc: 0.6850\n",
      "Epoch 68/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.1577 - acc: 0.5586 - val_loss: 0.9133 - val_acc: 0.6900\n",
      "Epoch 69/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.1525 - acc: 0.5609 - val_loss: 0.9051 - val_acc: 0.6880\n",
      "Epoch 70/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.1483 - acc: 0.5669 - val_loss: 0.8983 - val_acc: 0.6910\n",
      "Epoch 71/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.1502 - acc: 0.5685 - val_loss: 0.8924 - val_acc: 0.6930\n",
      "Epoch 72/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.1394 - acc: 0.5633 - val_loss: 0.8873 - val_acc: 0.6920\n",
      "Epoch 73/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.1237 - acc: 0.5800 - val_loss: 0.8798 - val_acc: 0.6960\n",
      "Epoch 74/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.1134 - acc: 0.5818 - val_loss: 0.8738 - val_acc: 0.7000\n",
      "Epoch 75/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.1200 - acc: 0.5769 - val_loss: 0.8686 - val_acc: 0.7040\n",
      "Epoch 76/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.1025 - acc: 0.5858 - val_loss: 0.8626 - val_acc: 0.7080\n",
      "Epoch 77/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0966 - acc: 0.5855 - val_loss: 0.8585 - val_acc: 0.7130\n",
      "Epoch 78/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0951 - acc: 0.5942 - val_loss: 0.8521 - val_acc: 0.7100\n",
      "Epoch 79/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0943 - acc: 0.5873 - val_loss: 0.8476 - val_acc: 0.7160\n",
      "Epoch 80/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0925 - acc: 0.5876 - val_loss: 0.8424 - val_acc: 0.7190\n",
      "Epoch 81/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.0842 - acc: 0.5895 - val_loss: 0.8385 - val_acc: 0.7160\n",
      "Epoch 82/200\n",
      "7800/7800 [==============================] - 0s 38us/step - loss: 1.0714 - acc: 0.5994 - val_loss: 0.8328 - val_acc: 0.7230\n",
      "Epoch 83/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 1.0678 - acc: 0.5950 - val_loss: 0.8293 - val_acc: 0.7220\n",
      "Epoch 84/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.0599 - acc: 0.6078 - val_loss: 0.8220 - val_acc: 0.7240\n",
      "Epoch 85/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.0646 - acc: 0.6024 - val_loss: 0.8198 - val_acc: 0.7240\n",
      "Epoch 86/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0586 - acc: 0.6005 - val_loss: 0.8154 - val_acc: 0.7260\n",
      "Epoch 87/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0537 - acc: 0.6000 - val_loss: 0.8128 - val_acc: 0.7290\n",
      "Epoch 88/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0560 - acc: 0.6000 - val_loss: 0.8081 - val_acc: 0.7330\n",
      "Epoch 89/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0478 - acc: 0.6142 - val_loss: 0.8038 - val_acc: 0.7350\n",
      "Epoch 90/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.0563 - acc: 0.5996 - val_loss: 0.8005 - val_acc: 0.7320\n",
      "Epoch 91/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0395 - acc: 0.6123 - val_loss: 0.7971 - val_acc: 0.7360\n",
      "Epoch 92/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.0277 - acc: 0.6101 - val_loss: 0.7921 - val_acc: 0.7350\n",
      "Epoch 93/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0197 - acc: 0.6204 - val_loss: 0.7875 - val_acc: 0.7350\n",
      "Epoch 94/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0260 - acc: 0.6151 - val_loss: 0.7852 - val_acc: 0.7370\n",
      "Epoch 95/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.0139 - acc: 0.6215 - val_loss: 0.7811 - val_acc: 0.7360\n",
      "Epoch 96/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0177 - acc: 0.6206 - val_loss: 0.7776 - val_acc: 0.7400\n",
      "Epoch 97/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.0100 - acc: 0.6182 - val_loss: 0.7754 - val_acc: 0.7390\n",
      "Epoch 98/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0079 - acc: 0.6167 - val_loss: 0.7720 - val_acc: 0.7400\n",
      "Epoch 99/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 1.0088 - acc: 0.6299 - val_loss: 0.7706 - val_acc: 0.7420\n",
      "Epoch 100/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.9930 - acc: 0.6263 - val_loss: 0.7667 - val_acc: 0.7420\n",
      "Epoch 101/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.9960 - acc: 0.6294 - val_loss: 0.7640 - val_acc: 0.7450\n",
      "Epoch 102/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 1.0012 - acc: 0.6186 - val_loss: 0.7615 - val_acc: 0.7430\n",
      "Epoch 103/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9722 - acc: 0.6396 - val_loss: 0.7558 - val_acc: 0.7430\n",
      "Epoch 104/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9783 - acc: 0.6324 - val_loss: 0.7537 - val_acc: 0.7440\n",
      "Epoch 105/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9913 - acc: 0.6247 - val_loss: 0.7540 - val_acc: 0.7430\n",
      "Epoch 106/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9818 - acc: 0.6303 - val_loss: 0.7529 - val_acc: 0.7420\n",
      "Epoch 107/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9651 - acc: 0.6362 - val_loss: 0.7480 - val_acc: 0.7480\n",
      "Epoch 108/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9723 - acc: 0.6369 - val_loss: 0.7447 - val_acc: 0.7460\n",
      "Epoch 109/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9690 - acc: 0.6353 - val_loss: 0.7417 - val_acc: 0.7460\n",
      "Epoch 110/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9617 - acc: 0.6322 - val_loss: 0.7386 - val_acc: 0.7480\n",
      "Epoch 111/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.9708 - acc: 0.6323 - val_loss: 0.7383 - val_acc: 0.7450\n",
      "Epoch 112/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9463 - acc: 0.6494 - val_loss: 0.7333 - val_acc: 0.7460\n",
      "Epoch 113/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9606 - acc: 0.6431 - val_loss: 0.7324 - val_acc: 0.7480\n",
      "Epoch 114/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9585 - acc: 0.6383 - val_loss: 0.7298 - val_acc: 0.7490\n",
      "Epoch 115/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9414 - acc: 0.6501 - val_loss: 0.7261 - val_acc: 0.7480\n",
      "Epoch 116/200\n",
      "7800/7800 [==============================] - 0s 37us/step - loss: 0.9441 - acc: 0.6499 - val_loss: 0.7237 - val_acc: 0.7480\n",
      "Epoch 117/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.9487 - acc: 0.6491 - val_loss: 0.7224 - val_acc: 0.7520\n",
      "Epoch 118/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.9505 - acc: 0.6442 - val_loss: 0.7218 - val_acc: 0.7510\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 37us/step - loss: 0.9342 - acc: 0.6533 - val_loss: 0.7166 - val_acc: 0.7520\n",
      "Epoch 120/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9284 - acc: 0.6596 - val_loss: 0.7143 - val_acc: 0.7520\n",
      "Epoch 121/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.9264 - acc: 0.6509 - val_loss: 0.7122 - val_acc: 0.7540\n",
      "Epoch 122/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.9246 - acc: 0.6556 - val_loss: 0.7118 - val_acc: 0.7530\n",
      "Epoch 123/200\n",
      "7800/7800 [==============================] - 0s 37us/step - loss: 0.9313 - acc: 0.6513 - val_loss: 0.7085 - val_acc: 0.7560\n",
      "Epoch 124/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9160 - acc: 0.6638 - val_loss: 0.7071 - val_acc: 0.7600\n",
      "Epoch 125/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9207 - acc: 0.6556 - val_loss: 0.7054 - val_acc: 0.7550\n",
      "Epoch 126/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.9039 - acc: 0.6592 - val_loss: 0.7017 - val_acc: 0.7540\n",
      "Epoch 127/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9131 - acc: 0.6635 - val_loss: 0.7003 - val_acc: 0.7510\n",
      "Epoch 128/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.9223 - acc: 0.6585 - val_loss: 0.6978 - val_acc: 0.7560\n",
      "Epoch 129/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.9050 - acc: 0.6615 - val_loss: 0.6970 - val_acc: 0.7550\n",
      "Epoch 130/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.9000 - acc: 0.6613 - val_loss: 0.6946 - val_acc: 0.7590\n",
      "Epoch 131/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.8921 - acc: 0.6659 - val_loss: 0.6917 - val_acc: 0.7550\n",
      "Epoch 132/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.8892 - acc: 0.6705 - val_loss: 0.6893 - val_acc: 0.7560\n",
      "Epoch 133/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.8968 - acc: 0.6662 - val_loss: 0.6885 - val_acc: 0.7610\n",
      "Epoch 134/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.8891 - acc: 0.6678 - val_loss: 0.6865 - val_acc: 0.7590\n",
      "Epoch 135/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.8863 - acc: 0.6659 - val_loss: 0.6831 - val_acc: 0.7570\n",
      "Epoch 136/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.8921 - acc: 0.6696 - val_loss: 0.6819 - val_acc: 0.7580\n",
      "Epoch 137/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.8872 - acc: 0.6614 - val_loss: 0.6822 - val_acc: 0.7640\n",
      "Epoch 138/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.8915 - acc: 0.6737 - val_loss: 0.6811 - val_acc: 0.7590\n",
      "Epoch 139/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8804 - acc: 0.6696 - val_loss: 0.6779 - val_acc: 0.7620\n",
      "Epoch 140/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.8725 - acc: 0.6732 - val_loss: 0.6759 - val_acc: 0.7610\n",
      "Epoch 141/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8758 - acc: 0.6785 - val_loss: 0.6746 - val_acc: 0.7540\n",
      "Epoch 142/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8650 - acc: 0.6763 - val_loss: 0.6728 - val_acc: 0.7610\n",
      "Epoch 143/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8630 - acc: 0.6760 - val_loss: 0.6715 - val_acc: 0.7550\n",
      "Epoch 144/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8551 - acc: 0.6823 - val_loss: 0.6705 - val_acc: 0.7630\n",
      "Epoch 145/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.8489 - acc: 0.6822 - val_loss: 0.6667 - val_acc: 0.7650\n",
      "Epoch 146/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.8648 - acc: 0.6788 - val_loss: 0.6650 - val_acc: 0.7620\n",
      "Epoch 147/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8595 - acc: 0.6783 - val_loss: 0.6644 - val_acc: 0.7590\n",
      "Epoch 148/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8559 - acc: 0.6758 - val_loss: 0.6633 - val_acc: 0.7650\n",
      "Epoch 149/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8524 - acc: 0.6794 - val_loss: 0.6621 - val_acc: 0.7630\n",
      "Epoch 150/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8535 - acc: 0.6735 - val_loss: 0.6612 - val_acc: 0.7710\n",
      "Epoch 151/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8461 - acc: 0.6840 - val_loss: 0.6589 - val_acc: 0.7630\n",
      "Epoch 152/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8522 - acc: 0.6819 - val_loss: 0.6581 - val_acc: 0.7630\n",
      "Epoch 153/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8458 - acc: 0.6864 - val_loss: 0.6570 - val_acc: 0.7620\n",
      "Epoch 154/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8328 - acc: 0.6940 - val_loss: 0.6533 - val_acc: 0.7640\n",
      "Epoch 155/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8406 - acc: 0.6924 - val_loss: 0.6517 - val_acc: 0.7640\n",
      "Epoch 156/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8416 - acc: 0.6796 - val_loss: 0.6527 - val_acc: 0.7650\n",
      "Epoch 157/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8243 - acc: 0.6942 - val_loss: 0.6525 - val_acc: 0.7680\n",
      "Epoch 158/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8307 - acc: 0.6847 - val_loss: 0.6503 - val_acc: 0.7780\n",
      "Epoch 159/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8287 - acc: 0.6864 - val_loss: 0.6482 - val_acc: 0.7740\n",
      "Epoch 160/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8238 - acc: 0.6942 - val_loss: 0.6483 - val_acc: 0.7700\n",
      "Epoch 161/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8376 - acc: 0.6946 - val_loss: 0.6475 - val_acc: 0.7750\n",
      "Epoch 162/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8268 - acc: 0.6908 - val_loss: 0.6465 - val_acc: 0.7730\n",
      "Epoch 163/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8259 - acc: 0.6968 - val_loss: 0.6463 - val_acc: 0.7680\n",
      "Epoch 164/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8296 - acc: 0.6977 - val_loss: 0.6459 - val_acc: 0.7770\n",
      "Epoch 165/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8206 - acc: 0.6921 - val_loss: 0.6429 - val_acc: 0.7770\n",
      "Epoch 166/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8136 - acc: 0.6995 - val_loss: 0.6414 - val_acc: 0.7790\n",
      "Epoch 167/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8140 - acc: 0.6969 - val_loss: 0.6403 - val_acc: 0.7740\n",
      "Epoch 168/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.8313 - acc: 0.6846 - val_loss: 0.6409 - val_acc: 0.7770\n",
      "Epoch 169/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.8168 - acc: 0.6968 - val_loss: 0.6408 - val_acc: 0.7820\n",
      "Epoch 170/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.7905 - acc: 0.7042 - val_loss: 0.6368 - val_acc: 0.7790\n",
      "Epoch 171/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.8087 - acc: 0.6979 - val_loss: 0.6352 - val_acc: 0.7820\n",
      "Epoch 172/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.8110 - acc: 0.6991 - val_loss: 0.6349 - val_acc: 0.7780\n",
      "Epoch 173/200\n",
      "7800/7800 [==============================] - 0s 49us/step - loss: 0.8135 - acc: 0.7041 - val_loss: 0.6347 - val_acc: 0.7770\n",
      "Epoch 174/200\n",
      "7800/7800 [==============================] - 0s 47us/step - loss: 0.8121 - acc: 0.7005 - val_loss: 0.6346 - val_acc: 0.7770\n",
      "Epoch 175/200\n",
      "7800/7800 [==============================] - 0s 41us/step - loss: 0.7877 - acc: 0.7019 - val_loss: 0.6329 - val_acc: 0.7780\n",
      "Epoch 176/200\n",
      "7800/7800 [==============================] - 0s 45us/step - loss: 0.8039 - acc: 0.6963 - val_loss: 0.6308 - val_acc: 0.7770\n",
      "Epoch 177/200\n",
      "7800/7800 [==============================] - 0s 39us/step - loss: 0.8021 - acc: 0.6990 - val_loss: 0.6307 - val_acc: 0.7760\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 37us/step - loss: 0.7851 - acc: 0.7037 - val_loss: 0.6292 - val_acc: 0.7800\n",
      "Epoch 179/200\n",
      "7800/7800 [==============================] - 0s 37us/step - loss: 0.8096 - acc: 0.6963 - val_loss: 0.6282 - val_acc: 0.7810\n",
      "Epoch 180/200\n",
      "7800/7800 [==============================] - 0s 37us/step - loss: 0.7927 - acc: 0.7013 - val_loss: 0.6282 - val_acc: 0.7820\n",
      "Epoch 181/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.7898 - acc: 0.7062 - val_loss: 0.6275 - val_acc: 0.7830\n",
      "Epoch 182/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.7768 - acc: 0.7136 - val_loss: 0.6247 - val_acc: 0.7830\n",
      "Epoch 183/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.8000 - acc: 0.7008 - val_loss: 0.6241 - val_acc: 0.7780\n",
      "Epoch 184/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.7773 - acc: 0.7083 - val_loss: 0.6240 - val_acc: 0.7800\n",
      "Epoch 185/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.7757 - acc: 0.7129 - val_loss: 0.6224 - val_acc: 0.7840\n",
      "Epoch 186/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.7719 - acc: 0.7127 - val_loss: 0.6180 - val_acc: 0.7870\n",
      "Epoch 187/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.7738 - acc: 0.7071 - val_loss: 0.6175 - val_acc: 0.7860\n",
      "Epoch 188/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.7836 - acc: 0.7112 - val_loss: 0.6181 - val_acc: 0.7830\n",
      "Epoch 189/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.7869 - acc: 0.7109 - val_loss: 0.6171 - val_acc: 0.7770\n",
      "Epoch 190/200\n",
      "7800/7800 [==============================] - 0s 37us/step - loss: 0.7765 - acc: 0.7112 - val_loss: 0.6163 - val_acc: 0.7790\n",
      "Epoch 191/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.7652 - acc: 0.7133 - val_loss: 0.6156 - val_acc: 0.7770\n",
      "Epoch 192/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.7571 - acc: 0.7172 - val_loss: 0.6163 - val_acc: 0.7760\n",
      "Epoch 193/200\n",
      "7800/7800 [==============================] - 0s 36us/step - loss: 0.7661 - acc: 0.7132 - val_loss: 0.6146 - val_acc: 0.7790\n",
      "Epoch 194/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.7722 - acc: 0.7141 - val_loss: 0.6150 - val_acc: 0.7780\n",
      "Epoch 195/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.7655 - acc: 0.7138 - val_loss: 0.6139 - val_acc: 0.7810\n",
      "Epoch 196/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.7562 - acc: 0.7113 - val_loss: 0.6118 - val_acc: 0.7770\n",
      "Epoch 197/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.7619 - acc: 0.7169 - val_loss: 0.6125 - val_acc: 0.7800\n",
      "Epoch 198/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.7473 - acc: 0.7186 - val_loss: 0.6089 - val_acc: 0.7840\n",
      "Epoch 199/200\n",
      "7800/7800 [==============================] - 0s 35us/step - loss: 0.7594 - acc: 0.7186 - val_loss: 0.6102 - val_acc: 0.7900\n",
      "Epoch 200/200\n",
      "7800/7800 [==============================] - 0s 34us/step - loss: 0.7462 - acc: 0.7179 - val_loss: 0.6075 - val_acc: 0.7900\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800/7800 [==============================] - 0s 21us/step\n",
      "1200/1200 [==============================] - 0s 21us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46987954512620583, 0.8396153846765176]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6794314082463583, 0.7433333333333333]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 1.9429 - acc: 0.1707 - val_loss: 1.9129 - val_acc: 0.2203\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 1.8799 - acc: 0.2745 - val_loss: 1.8540 - val_acc: 0.3083\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 1.8003 - acc: 0.3620 - val_loss: 1.7520 - val_acc: 0.3923\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 1.6611 - acc: 0.4639 - val_loss: 1.5822 - val_acc: 0.5023\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 1.4615 - acc: 0.5552 - val_loss: 1.3739 - val_acc: 0.5913\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 1.2520 - acc: 0.6275 - val_loss: 1.1838 - val_acc: 0.6380\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 1.0798 - acc: 0.6703 - val_loss: 1.0385 - val_acc: 0.6743\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.9524 - acc: 0.6974 - val_loss: 0.9328 - val_acc: 0.6987\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.8619 - acc: 0.7159 - val_loss: 0.8573 - val_acc: 0.7133\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.7968 - acc: 0.7295 - val_loss: 0.8025 - val_acc: 0.7280\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.7490 - acc: 0.7423 - val_loss: 0.7621 - val_acc: 0.7363\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.7127 - acc: 0.7520 - val_loss: 0.7303 - val_acc: 0.7420\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.6838 - acc: 0.7590 - val_loss: 0.7058 - val_acc: 0.7530\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.6601 - acc: 0.7645 - val_loss: 0.6860 - val_acc: 0.7603\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.6404 - acc: 0.7719 - val_loss: 0.6694 - val_acc: 0.7587\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.6237 - acc: 0.7756 - val_loss: 0.6546 - val_acc: 0.7647\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.6089 - acc: 0.7809 - val_loss: 0.6432 - val_acc: 0.7683\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5956 - acc: 0.7841 - val_loss: 0.6334 - val_acc: 0.7723\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5839 - acc: 0.7891 - val_loss: 0.6234 - val_acc: 0.7747\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5731 - acc: 0.7927 - val_loss: 0.6148 - val_acc: 0.7767\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5634 - acc: 0.7957 - val_loss: 0.6084 - val_acc: 0.7827\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5543 - acc: 0.7988 - val_loss: 0.6010 - val_acc: 0.7840\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5457 - acc: 0.8027 - val_loss: 0.5952 - val_acc: 0.7857\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5379 - acc: 0.8058 - val_loss: 0.5902 - val_acc: 0.7953\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5306 - acc: 0.8087 - val_loss: 0.5871 - val_acc: 0.7923\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5235 - acc: 0.8111 - val_loss: 0.5806 - val_acc: 0.7963\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5167 - acc: 0.8132 - val_loss: 0.5761 - val_acc: 0.7967\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5106 - acc: 0.8159 - val_loss: 0.5723 - val_acc: 0.8027\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5048 - acc: 0.8182 - val_loss: 0.5703 - val_acc: 0.8020\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4987 - acc: 0.8205 - val_loss: 0.5674 - val_acc: 0.8010\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4938 - acc: 0.8223 - val_loss: 0.5632 - val_acc: 0.8040\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4884 - acc: 0.8239 - val_loss: 0.5594 - val_acc: 0.8040\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4837 - acc: 0.8254 - val_loss: 0.5619 - val_acc: 0.8043\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4788 - acc: 0.8280 - val_loss: 0.5556 - val_acc: 0.8050\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4744 - acc: 0.8298 - val_loss: 0.5543 - val_acc: 0.8083\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4698 - acc: 0.8314 - val_loss: 0.5512 - val_acc: 0.8070\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4653 - acc: 0.8338 - val_loss: 0.5507 - val_acc: 0.8083\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4616 - acc: 0.8348 - val_loss: 0.5469 - val_acc: 0.8120\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4575 - acc: 0.8369 - val_loss: 0.5470 - val_acc: 0.8103\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4535 - acc: 0.8392 - val_loss: 0.5434 - val_acc: 0.8137\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4498 - acc: 0.8395 - val_loss: 0.5427 - val_acc: 0.8110\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4463 - acc: 0.8408 - val_loss: 0.5434 - val_acc: 0.8120\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4427 - acc: 0.8417 - val_loss: 0.5412 - val_acc: 0.8117\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4394 - acc: 0.8431 - val_loss: 0.5428 - val_acc: 0.8117\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4363 - acc: 0.8453 - val_loss: 0.5391 - val_acc: 0.8107\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4326 - acc: 0.8463 - val_loss: 0.5358 - val_acc: 0.8130\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4296 - acc: 0.8466 - val_loss: 0.5362 - val_acc: 0.8133\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4267 - acc: 0.8494 - val_loss: 0.5356 - val_acc: 0.8107\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4239 - acc: 0.8503 - val_loss: 0.5391 - val_acc: 0.8107\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4211 - acc: 0.8506 - val_loss: 0.5342 - val_acc: 0.8123\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4179 - acc: 0.8525 - val_loss: 0.5335 - val_acc: 0.8120\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4151 - acc: 0.8537 - val_loss: 0.5340 - val_acc: 0.8100\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4126 - acc: 0.8551 - val_loss: 0.5333 - val_acc: 0.8133\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4101 - acc: 0.8564 - val_loss: 0.5331 - val_acc: 0.8123\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4076 - acc: 0.8575 - val_loss: 0.5347 - val_acc: 0.8127\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4050 - acc: 0.8578 - val_loss: 0.5316 - val_acc: 0.8123\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4022 - acc: 0.8583 - val_loss: 0.5334 - val_acc: 0.8113\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4002 - acc: 0.8589 - val_loss: 0.5355 - val_acc: 0.8113\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3977 - acc: 0.8610 - val_loss: 0.5349 - val_acc: 0.8130\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3958 - acc: 0.8609 - val_loss: 0.5311 - val_acc: 0.8110\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3932 - acc: 0.8622 - val_loss: 0.5319 - val_acc: 0.8150\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3911 - acc: 0.8631 - val_loss: 0.5330 - val_acc: 0.8113\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3888 - acc: 0.8646 - val_loss: 0.5312 - val_acc: 0.8117\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3865 - acc: 0.8650 - val_loss: 0.5313 - val_acc: 0.8147\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3841 - acc: 0.8668 - val_loss: 0.5305 - val_acc: 0.8117\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3825 - acc: 0.8673 - val_loss: 0.5329 - val_acc: 0.8143\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3803 - acc: 0.8676 - val_loss: 0.5355 - val_acc: 0.8080\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3784 - acc: 0.8685 - val_loss: 0.5339 - val_acc: 0.8110\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3768 - acc: 0.8693 - val_loss: 0.5333 - val_acc: 0.8113\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3747 - acc: 0.8695 - val_loss: 0.5317 - val_acc: 0.8157\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3725 - acc: 0.8701 - val_loss: 0.5342 - val_acc: 0.8110\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3710 - acc: 0.8710 - val_loss: 0.5336 - val_acc: 0.8107\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3692 - acc: 0.8710 - val_loss: 0.5339 - val_acc: 0.8100\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3674 - acc: 0.8723 - val_loss: 0.5343 - val_acc: 0.8107\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3655 - acc: 0.8726 - val_loss: 0.5349 - val_acc: 0.8140\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3639 - acc: 0.8739 - val_loss: 0.5331 - val_acc: 0.8130\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3619 - acc: 0.8748 - val_loss: 0.5338 - val_acc: 0.8150\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3607 - acc: 0.8743 - val_loss: 0.5346 - val_acc: 0.8120\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3582 - acc: 0.8759 - val_loss: 0.5342 - val_acc: 0.8077\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3570 - acc: 0.8763 - val_loss: 0.5362 - val_acc: 0.8133\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3554 - acc: 0.8774 - val_loss: 0.5358 - val_acc: 0.8097\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3539 - acc: 0.8787 - val_loss: 0.5367 - val_acc: 0.8150\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3520 - acc: 0.8782 - val_loss: 0.5359 - val_acc: 0.8097\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3507 - acc: 0.8785 - val_loss: 0.5394 - val_acc: 0.8087\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3491 - acc: 0.8792 - val_loss: 0.5351 - val_acc: 0.8133\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3473 - acc: 0.8790 - val_loss: 0.5357 - val_acc: 0.8137\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3456 - acc: 0.8804 - val_loss: 0.5381 - val_acc: 0.8130\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3442 - acc: 0.8809 - val_loss: 0.5372 - val_acc: 0.8130\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3424 - acc: 0.8814 - val_loss: 0.5407 - val_acc: 0.8117\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3414 - acc: 0.8823 - val_loss: 0.5399 - val_acc: 0.8150\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3399 - acc: 0.8835 - val_loss: 0.5391 - val_acc: 0.8117\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3381 - acc: 0.8836 - val_loss: 0.5411 - val_acc: 0.8103\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3370 - acc: 0.8826 - val_loss: 0.5429 - val_acc: 0.8093\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3355 - acc: 0.8847 - val_loss: 0.5412 - val_acc: 0.8143\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3342 - acc: 0.8851 - val_loss: 0.5399 - val_acc: 0.8107\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3329 - acc: 0.8848 - val_loss: 0.5482 - val_acc: 0.8110\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3311 - acc: 0.8862 - val_loss: 0.5416 - val_acc: 0.8100\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3299 - acc: 0.8860 - val_loss: 0.5452 - val_acc: 0.8120\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3288 - acc: 0.8870 - val_loss: 0.5432 - val_acc: 0.8103\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3272 - acc: 0.8874 - val_loss: 0.5486 - val_acc: 0.8090\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3264 - acc: 0.8888 - val_loss: 0.5464 - val_acc: 0.8103\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3246 - acc: 0.8875 - val_loss: 0.5448 - val_acc: 0.8110\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3236 - acc: 0.8875 - val_loss: 0.5450 - val_acc: 0.8117\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3220 - acc: 0.8896 - val_loss: 0.5471 - val_acc: 0.8093\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3209 - acc: 0.8893 - val_loss: 0.5480 - val_acc: 0.8140\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3199 - acc: 0.8902 - val_loss: 0.5513 - val_acc: 0.8100\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3181 - acc: 0.8908 - val_loss: 0.5476 - val_acc: 0.8137\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3171 - acc: 0.8909 - val_loss: 0.5510 - val_acc: 0.8130\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3160 - acc: 0.8923 - val_loss: 0.5502 - val_acc: 0.8140\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3147 - acc: 0.8916 - val_loss: 0.5531 - val_acc: 0.8097\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3130 - acc: 0.8924 - val_loss: 0.5570 - val_acc: 0.8093\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3119 - acc: 0.8932 - val_loss: 0.5522 - val_acc: 0.8100\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3110 - acc: 0.8938 - val_loss: 0.5572 - val_acc: 0.8070\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3094 - acc: 0.8940 - val_loss: 0.5553 - val_acc: 0.8120\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3084 - acc: 0.8936 - val_loss: 0.5557 - val_acc: 0.8113\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3077 - acc: 0.8941 - val_loss: 0.5568 - val_acc: 0.8103\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3061 - acc: 0.8948 - val_loss: 0.5568 - val_acc: 0.8120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.3045 - acc: 0.8958 - val_loss: 0.5606 - val_acc: 0.8060\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3041 - acc: 0.8964 - val_loss: 0.5610 - val_acc: 0.8110\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.3024 - acc: 0.8970 - val_loss: 0.5588 - val_acc: 0.8050\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 17us/step\n",
      "4000/4000 [==============================] - 0s 20us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29665969422008054, 0.8996060606060606]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5580034837722778, 0.8165]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
